"""
Reward Functions for RL4Rec
NDCG ê¸°ë°˜ ë¦¬ì›Œë“œ ê³„ì‚° ë° TRL í†µí•©
"""

import json
import torch
import numpy as np
from typing import List, Dict, Optional, Tuple
import ray
from pathlib import Path


def calculate_dcg(relevance_scores: torch.Tensor, k: Optional[int] = None) -> torch.Tensor:
    """
    DCG (Discounted Cumulative Gain) ê³„ì‚°
    
    Args:
        relevance_scores: [batch_size, num_items] ê´€ë ¨ì„± ì ìˆ˜
        k: top-k ê¹Œì§€ë§Œ ê³„ì‚° (Noneì´ë©´ ì „ì²´)
    
    Returns:
        DCG scores [batch_size]
    """
    if k is not None:
        relevance_scores = relevance_scores[:, :k]
    
    # DCG = sum(rel_i / log2(i + 2)) for i in range(k)
    positions = torch.arange(1, relevance_scores.shape[1] + 1, device=relevance_scores.device)
    discounts = torch.log2(positions + 1.0)
    dcg = (relevance_scores / discounts).sum(dim=1)
    
    return dcg


def calculate_ndcg_from_rank(ranks: torch.Tensor, k: int = 10) -> torch.Tensor:
    """
    Rank ê¸°ë°˜ NDCG ê³„ì‚°
    
    Args:
        ranks: [batch_size] GT ì•„ì´í…œì˜ rank (1-indexed)
        k: Top-K NDCG
    
    Returns:
        NDCG scores [batch_size]
    """
    # Rankê°€ kë³´ë‹¤ í¬ë©´ NDCG = 0
    ndcg_scores = torch.zeros_like(ranks, dtype=torch.float32)
    
    # Rankê°€ k ì´ë‚´ì¸ ê²½ìš°ë§Œ ê³„ì‚°
    valid_mask = ranks <= k
    valid_ranks = ranks[valid_mask]
    
    # DCG = 1 / log2(rank + 1)
    dcg = 1.0 / torch.log2(valid_ranks.float() + 1.0)
    
    # IDCG = 1 / log2(2) (ì´ìƒì ì¸ ê²½ìš°, rank=1)
    idcg = 1.0 / torch.log2(torch.tensor(2.0, device=ranks.device))
    
    # NDCG = DCG / IDCG
    ndcg_scores[valid_mask] = dcg / idcg
    
    return ndcg_scores


def calculate_hit_from_rank(ranks: torch.Tensor, k: int = 10) -> torch.Tensor:
    """
    Rank ê¸°ë°˜ Hit@K ê³„ì‚°
    
    Args:
        ranks: [batch_size] GT ì•„ì´í…œì˜ rank (1-indexed)
        k: Top-K
    
    Returns:
        Hit scores [batch_size]
    """
    # Rankê°€ k ì´ë‚´ë©´ 1, ì•„ë‹ˆë©´ 0
    hit_scores = (ranks <= k).float()
    return hit_scores


def calculate_mrr_from_rank(ranks: torch.Tensor, k: int = 10) -> torch.Tensor:
    """
    Rank ê¸°ë°˜ MRR (Mean Reciprocal Rank) ê³„ì‚°
    
    Args:
        ranks: [batch_size] GT ì•„ì´í…œì˜ rank (1-indexed)
        k: Top-K
    
    Returns:
        MRR scores [batch_size]
    """
    # Rankê°€ kë³´ë‹¤ í¬ë©´ MRR = 0
    mrr_scores = torch.zeros_like(ranks, dtype=torch.float32)
    
    # Rankê°€ k ì´ë‚´ì¸ ê²½ìš°ë§Œ ê³„ì‚°
    valid_mask = ranks <= k
    valid_ranks = ranks[valid_mask]
    
    # MRR = 1 / rank
    mrr_scores[valid_mask] = 1.0 / valid_ranks.float()
    
    return mrr_scores


def calculate_novelty_ndcg(
    ranks: torch.Tensor, 
    item_weights: torch.Tensor,
    k: int = 10, 
) -> torch.Tensor:
    """
    Popularity-based novelty score calculation
    
    NDCGì— ì•„ì´í…œ ì¸ê¸°ë„ ì—­ìˆ˜(item_weights)ë¥¼ ê³±í•˜ì—¬ novelty ê³„ì‚°
    ì¸ê¸° ì—†ëŠ” ì•„ì´í…œì„ ë†’ì€ rankë¡œ ì˜ˆì¸¡í• ìˆ˜ë¡ ë†’ì€ novelty
    
    Args:
        ranks: [batch_size] GT ì•„ì´í…œì˜ rank (1-indexed)
        item_weights: [batch_size] ê° íƒ€ê²Ÿ ì•„ì´í…œì˜ popularity weight (ì—­ìˆ˜)
        k: Top-K
        target_rank: (ì‚¬ìš© ì•ˆí•¨, backward compatibility)
    
    Returns:
        novelty scores [batch_size] = NDCG Ã— item_weights
    """
    # 1. Ranksë¡œë¶€í„° NDCG ê³„ì‚°
    ndcg_scores = calculate_ndcg_from_rank(ranks, k=k)
    
    # 2. NDCGì— popularity weight ê³±í•˜ê¸°
    # ì¸ê¸° ì—†ëŠ” ì•„ì´í…œ(ë†’ì€ weight)ì„ ì˜ ì˜ˆì¸¡í•˜ë©´ ë†’ì€ novelty
    novelty = ndcg_scores * item_weights
    
    return novelty


def calculate_ndcg(
    predicted_scores: torch.Tensor,
    target_items: List[int],
    history_items: List[List[int]],
    k: int = 10,
    use_negatives_only: bool = False,
) -> torch.Tensor:
    """
    NDCG (Normalized Discounted Cumulative Gain) ê³„ì‚°
    
    Args:
        predicted_scores: [batch_size, num_items] ë˜ëŠ” [batch_size, 1+num_negs] ì˜ˆì¸¡ ì ìˆ˜
        target_items: [batch_size] ì‹¤ì œ íƒ€ê²Ÿ ì•„ì´í…œ ID ë¦¬ìŠ¤íŠ¸
        history_items: [batch_size, *] ì‚¬ìš©ìë³„ íˆìŠ¤í† ë¦¬ ì•„ì´í…œ ID ë¦¬ìŠ¤íŠ¸
        k: Top-K NDCG (default: 10)
        use_negatives_only: Trueì´ë©´ target+negativesë§Œ ì‚¬ìš© (scores shape [batch_size, 1+num_negs])
    
    Returns:
        NDCG scores [batch_size]
    """
    batch_size = predicted_scores.shape[0]
    ndcg_scores = torch.zeros(batch_size, device=predicted_scores.device)
    
    if use_negatives_only:
        # Target + negativesë§Œ ê³ ë ¤í•˜ëŠ” ê²½ìš°
        # scores shape: [batch_size, 1 + num_negs]
        # targetì€ í•­ìƒ index 0
        for i in range(batch_size):
            scores = predicted_scores[i]  # [1 + num_negs]
            k_actual = min(k, len(scores))
            
            # Top-K ì¶”ì¶œ
            top_k_scores, top_k_indices = torch.topk(scores, k=k_actual)
            
            # Target (index 0)ì´ top-kì— ìˆëŠ”ì§€ í™•ì¸
            relevance = torch.zeros(k_actual, device=predicted_scores.device)
            target_positions = (top_k_indices == 0).nonzero(as_tuple=True)[0]
            
            if len(target_positions) > 0:
                position = target_positions[0].item()
                relevance[position] = 1.0
                
                # DCG ê³„ì‚°
                dcg = calculate_dcg(relevance.unsqueeze(0), k=k_actual)[0]
                
                # IDCG ê³„ì‚°
                ideal_relevance = torch.zeros(k_actual, device=predicted_scores.device)
                ideal_relevance[0] = 1.0
                idcg = calculate_dcg(ideal_relevance.unsqueeze(0), k=k_actual)[0]
                
                ndcg_scores[i] = dcg / (idcg + 1e-10)
            else:
                ndcg_scores[i] = 0.0
    else:
        # ì „ì²´ ì•„ì´í…œ ê³ ë ¤í•˜ëŠ” ê²½ìš° (ê¸°ì¡´ ë¡œì§)
        for i in range(batch_size):
            # 1. íˆìŠ¤í† ë¦¬ ì•„ì´í…œ ì œì™¸ (masking)
            scores = predicted_scores[i].clone()
            if history_items[i]:
                history_mask = torch.zeros_like(scores, dtype=torch.bool, device=scores.device)
                history_mask[history_items[i]] = True
                history_mask[target_items[i]] = False
                scores[history_mask] = -float('inf')
            
            # 2. Top-K ì•„ì´í…œ ì¶”ì¶œ
            top_k_scores, top_k_indices = torch.topk(scores, k=min(k, len(scores)))
            
            # 3. Relevance ê³„ì‚° (target itemì´ top-kì— ìˆìœ¼ë©´ í•´ë‹¹ ìœ„ì¹˜ì— 1, ì—†ìœ¼ë©´ 0)
            relevance = torch.zeros(k, device=predicted_scores.device)
            target_item = target_items[i]
            
            # Top-kì—ì„œ target itemì˜ ìœ„ì¹˜ ì°¾ê¸°
            target_positions = (top_k_indices == target_item).nonzero(as_tuple=True)[0]
            if len(target_positions) > 0:
                position = target_positions[0].item()
                relevance[position] = 1.0
            
            # 4. DCG ê³„ì‚°
            if relevance.sum() > 0:
                dcg = calculate_dcg(relevance.unsqueeze(0), k=k)[0]
                
                # 5. IDCG (Ideal DCG) ê³„ì‚° - ì´ìƒì ì¸ ê²½ìš° (targetì´ 1ìœ„)
                ideal_relevance = torch.zeros(k, device=predicted_scores.device)
                ideal_relevance[0] = 1.0
                idcg = calculate_dcg(ideal_relevance.unsqueeze(0), k=k)[0]
                
                # 6. NDCG = DCG / IDCG
                ndcg_scores[i] = dcg / (idcg + 1e-10)
            else:
                # Targetì´ top-kì— ì—†ìœ¼ë©´ NDCG = 0
                ndcg_scores[i] = 0.0
    
    return ndcg_scores


def calculate_hit_rate(
    predicted_scores: torch.Tensor,
    target_items: List[int],
    history_items: List[List[int]],
    k: int = 10,
    use_negatives_only: bool = False,
) -> torch.Tensor:
    """
    Hit@K ê³„ì‚° (targetì´ top-kì— ìˆìœ¼ë©´ 1, ì—†ìœ¼ë©´ 0)
    
    Args:
        predicted_scores: [batch_size, num_items] ë˜ëŠ” [batch_size, 1+num_negs]
        target_items: [batch_size]
        history_items: [batch_size, *]
        k: Top-K
        use_negatives_only: Trueì´ë©´ target+negativesë§Œ ì‚¬ìš©
    
    Returns:
        Hit scores [batch_size]
    """
    batch_size = predicted_scores.shape[0]
    hit_scores = torch.zeros(batch_size, device=predicted_scores.device)
    
    if use_negatives_only:
        # Target + negativesë§Œ ê³ ë ¤
        for i in range(batch_size):
            scores = predicted_scores[i]
            k_actual = min(k, len(scores))
            
            # Top-K ì¶”ì¶œ
            _, top_k_indices = torch.topk(scores, k=k_actual)
            
            # Target (index 0)ì´ top-kì— ìˆëŠ”ì§€ í™•ì¸
            if 0 in top_k_indices:
                hit_scores[i] = 1.0
    else:
        # ì „ì²´ ì•„ì´í…œ ê³ ë ¤ (ê¸°ì¡´ ë¡œì§)
        for i in range(batch_size):
            # íˆìŠ¤í† ë¦¬ ì•„ì´í…œ ì œì™¸
            scores = predicted_scores[i].clone()
            if history_items[i]:
                history_mask = torch.zeros_like(scores, dtype=torch.bool, device=scores.device)
                history_mask[history_items[i]] = True
                history_mask[target_items[i]] = False
                scores[history_mask] = -float('inf')
            
            # Top-K ì¶”ì¶œ
            _, top_k_indices = torch.topk(scores, k=min(k, len(scores)))
            
            # Targetì´ top-kì— ìˆëŠ”ì§€ í™•ì¸
            if target_items[i] in top_k_indices:
                hit_scores[i] = 1.0
    
    return hit_scores


def calculate_mrr(
    predicted_scores: torch.Tensor,
    target_items: List[int],
    history_items: List[List[int]],
    k: int = 10,
    use_negatives_only: bool = False,
) -> torch.Tensor:
    """
    MRR (Mean Reciprocal Rank) ê³„ì‚°
    
    Args:
        predicted_scores: [batch_size, num_items] ë˜ëŠ” [batch_size, 1+num_negs]
        target_items: [batch_size]
        history_items: [batch_size, *]
        k: Top-K
        use_negatives_only: Trueì´ë©´ target+negativesë§Œ ì‚¬ìš©
    
    Returns:
        MRR scores [batch_size]
    """
    batch_size = predicted_scores.shape[0]
    mrr_scores = torch.zeros(batch_size, device=predicted_scores.device)
    
    if use_negatives_only:
        # Target + negativesë§Œ ê³ ë ¤
        for i in range(batch_size):
            scores = predicted_scores[i]
            k_actual = min(k, len(scores))
            
            # Top-K ì¶”ì¶œ
            _, top_k_indices = torch.topk(scores, k=k_actual)
            
            # Target (index 0)ì˜ rank ì°¾ê¸°
            target_positions = (top_k_indices == 0).nonzero(as_tuple=True)[0]
            if len(target_positions) > 0:
                rank = target_positions[0].item() + 1  # 1-indexed rank
                mrr_scores[i] = 1.0 / rank
    else:
        # ì „ì²´ ì•„ì´í…œ ê³ ë ¤ (ê¸°ì¡´ ë¡œì§)
        for i in range(batch_size):
            # íˆìŠ¤í† ë¦¬ ì•„ì´í…œ ì œì™¸
            scores = predicted_scores[i].clone()
            if history_items[i]:
                history_mask = torch.zeros_like(scores, dtype=torch.bool)
                history_mask[history_items[i]] = True
                scores[history_mask] = -float('inf')
            
            # Top-K ì¶”ì¶œ
            _, top_k_indices = torch.topk(scores, k=min(k, len(scores)))
            
            # Targetì˜ rank ì°¾ê¸°
            target_positions = (top_k_indices == target_items[i]).nonzero(as_tuple=True)[0]
            if len(target_positions) > 0:
                rank = target_positions[0].item() + 1  # 1-indexed rank
                mrr_scores[i] = 1.0 / rank
    
    return mrr_scores


class RecRewardFrunction:
    """
    TRLê³¼ í˜¸í™˜ë˜ëŠ” ë¦¬ì›Œë“œ í•¨ìˆ˜ í´ë˜ìŠ¤
    Ray RetrievalServiceì™€ í†µí•©
    """
    
    def __init__(
        self,
        retrieval_service_name: str = "RetrievalService",
        namespace: str = "rl4rec",
        data_name: str = "beauty",
        reward_type: str = "ndcg",
        k: int = 10,
        normalize: bool = True,
        test_target: bool = False,
    ):
        """
        Args:
            retrieval_service_name: Ray actor ì´ë¦„
            namespace: Ray namespace
            data_name: ë°ì´í„°ì…‹ ì´ë¦„
            reward_type: ë¦¬ì›Œë“œ íƒ€ì… ('ndcg', 'hit', 'mrr', 'mixed')
            k: Top-K ê°’
            normalize: ë¦¬ì›Œë“œ ì •ê·œí™” ì—¬ë¶€
        """
        self.__name__ = "RecRewardFrunction"
        self.retrieval_service_name = retrieval_service_name
        self.namespace = namespace
        self.data_name = data_name
        self.reward_type = reward_type
        self.k = k
        self.normalize = normalize
        self.test_target = test_target

        # RetrievalService ì—°ê²°
        try:
            self.retrieval_service = ray.get_actor(
                retrieval_service_name,
                namespace=namespace
            )
            print(f"âœ“ Connected to {retrieval_service_name}")
        except ValueError as e:
            raise RuntimeError(
                f"Failed to connect to {retrieval_service_name}. "
                f"Make sure retrieval service is running."
            ) from e

        #load item metadata
        with open(f"data/{data_name}/meta_text_fix.json", "r") as f:
            self.item_metadata = json.load(f)
        self.item_metadata = {int(k): v["title"] + "\n" + v["brand"] + "\n" + v["category"] for k, v in self.item_metadata.items()}
    
    def __call__(
        self,
        generated_texts: List[str],
        targets: List[int],
        histories: List[List[int]],
        neg_items: Optional[List[List[int]]] = None,
        **kwargs
    ) -> torch.Tensor:
        """
        TRL í˜¸í™˜ ë¦¬ì›Œë“œ í•¨ìˆ˜
        
        Args:
            generated_texts: [batch_size] ìƒì„±ëœ í…ìŠ¤íŠ¸
            targets: [batch_size] íƒ€ê²Ÿ ì•„ì´í…œ ID
            histories: [batch_size, *] íˆìŠ¤í† ë¦¬ ì•„ì´í…œ ID
            neg_items: [batch_size, num_negs] Negative ì•„ì´í…œ ID (optional)
        
        Returns:
            rewards: [batch_size] ë¦¬ì›Œë“œ ê°’
        """
        # add target text to generated_texts
        if self.test_target:
            generated_texts = [self.item_metadata[target] + "\n" + generated_text for generated_text, target in zip(generated_texts, targets)]  

        # 1. RetrievalServiceë¥¼ í†µí•´ ìœ ì‚¬ë„ ì ìˆ˜ ê³„ì‚°
        use_negatives_only = neg_items is not None
        
        scores_ref = self.retrieval_service.calculate_reward.remote(
            generated_texts,
            data_name=self.data_name,
            targets=targets if use_negatives_only else None,
            neg_items=neg_items,
        )
        scores = ray.get(scores_ref)  # [batch_size, num_items] or [batch_size, 1+num_negs]
        
        # 2. ë¦¬ì›Œë“œ íƒ€ì…ì— ë”°ë¼ ê³„ì‚°
        if self.reward_type == "ndcg":
            rewards = calculate_ndcg(scores, targets, histories, k=self.k, use_negatives_only=use_negatives_only)
        elif self.reward_type == "hit":
            rewards = calculate_hit_rate(scores, targets, histories, k=self.k, use_negatives_only=use_negatives_only)
        elif self.reward_type == "mrr":
            rewards = calculate_mrr(scores, targets, histories, k=self.k, use_negatives_only=use_negatives_only)
        elif self.reward_type == "mixed":
            # NDCG + Hit@Kì˜ ê°€ì¤‘ í‰ê· 
            ndcg = calculate_ndcg(scores, targets, histories, k=self.k, use_negatives_only=use_negatives_only)
            hit = calculate_hit_rate(scores, targets, histories, k=self.k, use_negatives_only=use_negatives_only)
            rewards = 0.7 * ndcg + 0.3 * hit
        else:
            raise ValueError(f"Unknown reward_type: {self.reward_type}")
        
        # 3. ì •ê·œí™” (optional)
        if self.normalize and rewards.std() > 0:
            rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-8)
        
        return rewards
    
    def compute_metrics(
        self,
        prompts: List[str],
        completions: List[str],
        completion_ids: List[List[int]],
        targets: List[int],
        histories: List[List[int]],
        neg_items: Optional[List[List[int]]] = None,
    ) -> Dict[str, float]:
        """
        í‰ê°€ ë©”íŠ¸ë¦­ ê³„ì‚° (NDCG, Hit@K, MRR ëª¨ë‘)
        
        Args:
            neg_items: [batch_size, num_negs] Negative ì•„ì´í…œ ID (optional)
        
        Returns:
            ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬
        """
        # RetrievalServiceë¥¼ í†µí•´ ìœ ì‚¬ë„ ì ìˆ˜ ê³„ì‚°
        use_negatives_only = neg_items is not None
        
        scores_ref = self.retrieval_service.calculate_reward.remote(
            completions,
            data_name=self.data_name,
            targets=targets if use_negatives_only else None,
            neg_items=neg_items,
        )
        scores = ray.get(scores_ref)
        
        # ëª¨ë“  ë©”íŠ¸ë¦­ ê³„ì‚°
        ndcg = calculate_ndcg(scores, targets, histories, k=self.k, use_negatives_only=use_negatives_only)
        hit = calculate_hit_rate(scores, targets, histories, k=self.k, use_negatives_only=use_negatives_only)
        mrr = calculate_mrr(scores, targets, histories, k=self.k, use_negatives_only=use_negatives_only)
        
        return {
            f"ndcg@{self.k}": ndcg.mean().item(),
            f"hit@{self.k}": hit.mean().item(),
            f"mrr@{self.k}": mrr.mean().item(),
        }


# TRL PPOTrainerì™€ í˜¸í™˜ë˜ëŠ” ë˜í¼ í•¨ìˆ˜
def create_reward_function(
    retrieval_service_name: str = "RetrievalService",
    namespace: str = "rl4rec",
    data_name: str = "beauty",
    reward_type: str = "ndcg",
    k: int = 10,
) -> RecRewardFrunction:
    """
    TRL PPOTrainerì—ì„œ ì‚¬ìš©í•  ë¦¬ì›Œë“œ í•¨ìˆ˜ ìƒì„±
    
    Usage:
        reward_fn = create_reward_function(reward_type="ndcg", k=10)
        rewards = reward_fn(generated_texts, targets, histories)
    """
    return RecRewardFrunction(
        retrieval_service_name=retrieval_service_name,
        namespace=namespace,
        data_name=data_name,
        reward_type=reward_type,
        k=k,
    )


def load_negative_pool(data_name: str, data_dir: str = "data", k: int = 10) -> Dict[int, List[int]]:
    """
    negative.txt íŒŒì¼ì—ì„œ negative pool ë¡œë“œ
    
    íŒŒì¼ í˜•ì‹: ê° ë¼ì¸ì€ "user_id neg_item1 neg_item2 ... neg_itemN"
    
    Args:
        data_name: ë°ì´í„°ì…‹ ì´ë¦„ (e.g., "beauty")
        data_dir: ë°ì´í„° ë””ë ‰í† ë¦¬
    
    Returns:
        user_idë¥¼ í‚¤ë¡œ í•˜ëŠ” negative items ë¦¬ìŠ¤íŠ¸ ë”•ì…”ë„ˆë¦¬
    """
    negative_file = Path(data_dir) / data_name / "negative.txt"
    
    if not negative_file.exists():
        raise FileNotFoundError(f"Negative pool file not found: {negative_file}")
    
    print(f"ğŸ“¦ Loading negative pool from: {negative_file}")
    negative_pool = {}
    
    with open(negative_file, 'r') as f:
        for line in f:
            parts = line.strip().split()
            user_id = int(parts[0])
            neg_items = [int(item_id) for item_id in parts[1:]]
            #random sample k items
            neg_items = neg_items[:k-1]
            # neg_items = np.random.choice(neg_items, size=k-1, replace=False).tolist()
            negative_pool[user_id] = neg_items
    
    print(f"âœ“ Loaded negative pool for {len(negative_pool)} users")
    if len(negative_pool) > 0:
        sample_user = next(iter(negative_pool))
        print(f"  Example: User {sample_user} has {len(negative_pool[sample_user])} negative items")
    
    return negative_pool


class LocalEmbeddingRewardFunction:
    """
    ë¡œì»¬ ì„ë² ë”© ê¸°ë°˜ ë¦¬ì›Œë“œ í•¨ìˆ˜
    DB ëŒ€ì‹  ìì²´ì ìœ¼ë¡œ negative item ì„ë² ë”©ì„ ê³„ì‚°í•˜ì—¬ NDCGë¥¼ rewardë¡œ í™œìš©
    """
    
    def __init__(
        self,
        uid_2_target: Dict[int, int],
        data_name: str,
        k: int = 10,
        reward_type: str = "ndcg",
        emb_model_name: str = "mixedbread-ai/mxbai-embed-large-v1",
        emb_type: str = "review_description",
        device: str = "cuda",
        emb_batch_size: int = 128,
        data_dir: str = "data",
        normalize: bool = True,
        novelty_reward: bool = False,
        novelty_target_rank: int = 3,
        novelty_mode: str = "gaussian",
        popularity_coef: float = 0.0,
    ):
        """
        Args:
            uid_2_target: ì‚¬ìš©ì ID to íƒ€ê²Ÿ ì•„ì´í…œ ID ë§¤í•‘
            data_name: ë°ì´í„°ì…‹ ì´ë¦„
            k: Top-K ê°’
            reward_type: ë¦¬ì›Œë“œ íƒ€ì… ('ndcg', 'hit', 'mrr', 'mixed')
            emb_model_name: ì„ë² ë”© ëª¨ë¸ ì´ë¦„
            emb_type: ì„ë² ë”© íƒ€ì… (review_description, title, etc.)
            device: ë””ë°”ì´ìŠ¤
            emb_batch_size: ì„ë² ë”© ê³„ì‚° ë°°ì¹˜ í¬ê¸°
            data_dir: ë°ì´í„° ë””ë ‰í† ë¦¬
            normalize: ë¦¬ì›Œë“œ ì •ê·œí™” ì—¬ë¶€
            novelty_reward: Novelty ë¦¬ì›Œë“œ ì‚¬ìš© ì—¬ë¶€ (True/False)
                           Novelty = NDCG Ã— popularity_weight
            novelty_target_rank: (ì‚¬ìš© ì•ˆí•¨, backward compatibility)
            novelty_mode: (ì‚¬ìš© ì•ˆí•¨, backward compatibility)
            popularity_coef: Popularity ë¦¬ì›Œë“œ ê³„ìˆ˜ (0.0 = ì‚¬ìš© ì•ˆí•¨)
                            ì •ë‹µì¸ ê²½ìš°ì—ë§Œ popularity bonus ì¶”ê°€
        """
        self.__name__ = "LocalEmbeddingRewardFunction"
        self.data_name = data_name
        self.reward_type = reward_type
        self.k = k
        self.normalize = normalize
        self.device = device
        self.emb_batch_size = emb_batch_size
        
        # Novelty ê´€ë ¨ íŒŒë¼ë¯¸í„°
        self.novelty_reward = novelty_reward
        self.novelty_target_rank = novelty_target_rank
        self.novelty_mode = novelty_mode
        
        # Popularity ê´€ë ¨ íŒŒë¼ë¯¸í„°
        self.popularity_coef = popularity_coef
        
        print(f"ğŸ’° Reward configuration:")
        print(f"  - Reward type: {reward_type}")
        print(f"  - Top-K: {k}")
        print(f"  - Normalize: {normalize}")
        if novelty_reward:
            print(f"  - Novelty reward: ENABLED")
            print(f"  - Novelty = NDCG Ã— popularity_weight (ì¸ê¸° ì—†ëŠ” ì•„ì´í…œ ì¥ë ¤)")
        if popularity_coef > 0:
            print(f"  - Popularity coefficient: {popularity_coef}")
            print(f"  - Popularity bonus for unpopular items (when correct)")
        
        # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
        print(f"ğŸ¤– Loading embedding model: {emb_model_name}")
        from sentence_transformers import SentenceTransformer
        self.emb_model = SentenceTransformer(emb_model_name, device=device)
        print(f"âœ“ Embedding model loaded on {device}")
        
        # Negative pool ë¡œë“œ
        self.negative_pool = load_negative_pool(data_name, data_dir, k)

        # prepare candidate set, target comes first
        self.candidate_tensor = self._prepare_candidate_tensor(uid_2_target, self.negative_pool)
        
        # ì‚¬ì „ ê³„ì‚°ëœ ì•„ì´í…œ ì„ë² ë”© ë¡œë“œ
        emb_model_name_dir = emb_model_name.split("/")[-1]
        item_embedding_file_path = f"data_emb/{data_name}_{emb_type}_{emb_model_name_dir}.pt"
        print(f"ğŸ“¦ Loading pre-computed item embeddings from: {item_embedding_file_path}")
        self.item_embeddings = torch.load(item_embedding_file_path, map_location=device)
        print(f"âœ“ Loaded embeddings for {len(self.item_embeddings)} items")
        
        # ì•„ì´í…œ ì¸ê¸°ë„ ê³„ì‚° (train setì—ì„œ)
        # Novelty ë˜ëŠ” Popularity reward ì‚¬ìš© ì‹œ í•„ìš”
        if self.novelty_reward or self.popularity_coef > 0:
            self.item_popularity_weights = self._compute_item_popularity(
                uid_2_target, self.negative_pool, data_name, data_dir
            )
        else:
            self.item_popularity_weights = None

    def _prepare_candidate_tensor(self, uid_2_target: Dict[int, int], neg_pool: Dict[int, List[int]]) -> torch.Tensor:
        candidate_tensor = torch.zeros(len(uid_2_target)+1, self.k, dtype=torch.long)
        for uid, target_id in uid_2_target.items():
            candidate_tensor[uid] = torch.tensor([target_id] + neg_pool[uid], dtype=torch.long)
        return candidate_tensor
    
    def _compute_item_popularity(
        self, 
        uid_2_target: Dict[int, int], 
        neg_pool: Dict[int, List[int]],
        data_name: str,
        data_dir: str
    ) -> torch.Tensor:
        """
        Train setì—ì„œ ì•„ì´í…œ ì¸ê¸°ë„ ê³„ì‚° (log ì—­ìˆ˜ ì‚¬ìš©)
        
        Args:
            uid_2_target: ì‚¬ìš©ì ID to íƒ€ê²Ÿ ì•„ì´í…œ ID ë§¤í•‘
            neg_pool: Negative pool
            data_name: ë°ì´í„°ì…‹ ì´ë¦„
            data_dir: ë°ì´í„° ë””ë ‰í† ë¦¬
        
        Returns:
            item_popularity_weights: [num_items] ê° ì•„ì´í…œì˜ ì¸ê¸°ë„ ê°€ì¤‘ì¹˜
                                     ì¸ê¸° ì—†ëŠ” ì•„ì´í…œì¼ìˆ˜ë¡ ë†’ì€ ê°’
        """
        print(f"ğŸ“Š Computing item popularity weights from train set...")
        
        # sequential_data.txtì—ì„œ train set ë¡œë“œ (target_index=-3)
        sequential_file = f"{data_dir}/{data_name}/sequential_data.txt"
        item_counts = {}
        
        with open(sequential_file, 'r') as f:
            for line in f:
                parts = [int(p) for p in line.strip().split()]
                user_id = parts[0]
                history = parts[1:-3]  # Train setì˜ history
                target = parts[-3]  # Train setì˜ target
                
                # Historyì˜ ëª¨ë“  ì•„ì´í…œ ì¹´ìš´íŠ¸
                for item_id in history:
                    item_counts[item_id] = item_counts.get(item_id, 0) + 1
                
                # Target ì•„ì´í…œë„ ì¹´ìš´íŠ¸
                item_counts[target] = item_counts.get(target, 0) + 1
        
        # ì „ì²´ ì•„ì´í…œ ìˆ˜ íŒŒì•…
        all_item_ids = set(item_counts.keys())
        max_item_id = max(all_item_ids) if all_item_ids else 0
        
        print(f"  Total unique items in train set: {len(all_item_ids)}")
        print(f"  Max item ID: {max_item_id}")
        
        # ì¸ê¸°ë„ ê°€ì¤‘ì¹˜ ê³„ì‚°: log(count + 1)ì˜ ì—­ìˆ˜
        # ì¸ê¸° ë§ì€ ì•„ì´í…œ -> ë‚®ì€ ê°€ì¤‘ì¹˜
        # ì¸ê¸° ì—†ëŠ” ì•„ì´í…œ -> ë†’ì€ ê°€ì¤‘ì¹˜
        item_weights = torch.ones(max_item_id + 1, device=self.device)
        
        for item_id, count in item_counts.items():
            # log(count + 1)ì˜ ì—­ìˆ˜
            item_weights[item_id] = 1.0 / np.log(count + 1)
        
        # ì •ê·œí™” (í‰ê· ì´ 1ì´ ë˜ë„ë¡)
        # ë‚˜íƒ€ë‚˜ì§€ ì•Šì€ ì•„ì´í…œì€ 1.0 ìœ ì§€
        appeared_mask = torch.zeros(max_item_id + 1, dtype=torch.bool, device=self.device)
        for item_id in all_item_ids:
            appeared_mask[item_id] = True
        
        if appeared_mask.sum() > 0:
            mean_weight = item_weights[appeared_mask].mean()
            item_weights[appeared_mask] = item_weights[appeared_mask] / mean_weight
        
        print(f"  Item popularity weight statistics:")
        print(f"    Min: {item_weights[appeared_mask].min().item():.4f}")
        print(f"    Max: {item_weights[appeared_mask].max().item():.4f}")
        print(f"    Mean: {item_weights[appeared_mask].mean().item():.4f}")
        print(f"    Std: {item_weights[appeared_mask].std().item():.4f}")
        
        # ì˜ˆì‹œ ì¶œë ¥
        sorted_counts = sorted(item_counts.items(), key=lambda x: x[1], reverse=True)
        print(f"  Top-5 popular items:")
        for item_id, count in sorted_counts[:5]:
            weight = item_weights[item_id].item()
            print(f"    Item {item_id}: count={count}, weight={weight:.4f}")
        
        print(f"  Bottom-5 popular items:")
        for item_id, count in sorted_counts[-5:]:
            weight = item_weights[item_id].item()
            print(f"    Item {item_id}: count={count}, weight={weight:.4f}")
        
        print(f"âœ“ Item popularity weights computed")
        
        return item_weights
    
    def _compute_similarity_scores(
        self,
        generated_texts: List[str],
        user_ids: torch.Tensor,
    ) -> torch.Tensor:
        """
        Compute similarity scores between generated texts and candidate set
        Args:
            generated_texts: [batch_size] generated texts
            user_ids: [batch_size] user ids
        Returns:
            ranks: [batch_size] ranks of target items
        """
        batch_size = len(generated_texts)
        
        # 1. ìƒì„±ëœ í…ìŠ¤íŠ¸ ì„ë² ë”© ê³„ì‚°
        query_embeddings = self.emb_model.encode(
            generated_texts,
            convert_to_tensor=True,
            show_progress_bar=False,
            device=self.device,
            batch_size=self.emb_batch_size,
        )  # [batch_size, emb_dim]
        
        batch_candidate_tensor = self.candidate_tensor[user_ids]
        scores = torch.bmm(query_embeddings.unsqueeze(1), self.item_embeddings[batch_candidate_tensor].transpose(1, 2)).squeeze(1)
        target_scores = scores[:, 0].unsqueeze(1)
        ranks = (scores > target_scores).sum(dim=1) + 1
        return ranks
    
    def __call__(
        self,
        generated_texts: List[str],
        user_ids: torch.Tensor,
        **kwargs
    ) -> torch.Tensor:
        """
        TRL í˜¸í™˜ ë¦¬ì›Œë“œ í•¨ìˆ˜
        
        Args:
            generated_texts: [batch_size] ìƒì„±ëœ í…ìŠ¤íŠ¸
            user_ids: [batch_size] ì‚¬ìš©ì ID (required)
            **kwargs: ì¶”ê°€ íŒŒë¼ë¯¸í„° (targets, histories ë“±ì€ ë¬´ì‹œë¨)
        
        Returns:
            rewards: [batch_size] ë¦¬ì›Œë“œ ê°’ 
            
            If novelty_reward=True:
                rewards = NDCG Ã— popularity_weight
                (ì¸ê¸° ì—†ëŠ” ì•„ì´í…œì„ ë†’ì€ rankë¡œ ì˜ˆì¸¡í• ìˆ˜ë¡ ë†’ì€ ë³´ìƒ)
            
            Else:
                rewards = base_reward (NDCG/Hit/MRR ë“±)
        """
        
        # rank ê³„ì‚° (target + negatives)
        ranks = self._compute_similarity_scores(generated_texts, user_ids)
        
        # ê¸°ë³¸ ë¦¬ì›Œë“œ íƒ€ì…ì— ë”°ë¼ ê³„ì‚°
        if self.reward_type == "ndcg":
            base_rewards = calculate_ndcg_from_rank(ranks, k=self.k)
        elif self.reward_type == "hit":
            base_rewards = calculate_hit_from_rank(ranks, k=self.k)
        elif self.reward_type == "mrr":
            base_rewards = calculate_mrr_from_rank(ranks, k=self.k)
        elif self.reward_type == "mixed":
            ndcg = calculate_ndcg_from_rank(ranks, k=self.k)
            hit = calculate_hit_from_rank(ranks, k=self.k)
            base_rewards = 0.7 * ndcg + 0.3 * hit
        else:
            raise ValueError(f"Unknown reward_type: {self.reward_type}")
        
        # Novelty reward ì‚¬ìš© ì—¬ë¶€ì— ë”°ë¼ ë¶„ê¸°
        if self.novelty_reward and self.item_popularity_weights is not None:
            # Novelty reward ì‚¬ìš©: rewards = NDCG Ã— item_popularity_weights
            batch_candidate_tensor = self.candidate_tensor[user_ids]  # [batch_size, k]
            target_item_ids = batch_candidate_tensor[:, 0]  # [batch_size] - targetì€ í•­ìƒ ì²« ë²ˆì§¸
            
            # Target itemì˜ popularity weight
            item_weights = self.item_popularity_weights[target_item_ids]  # [batch_size]
            
            # Novelty = NDCG Ã— popularity_weight
            rewards = calculate_novelty_ndcg(
                ranks, 
                item_weights=item_weights,
                k=self.k,
            )
        else:
            # ê¸°ë³¸ ë¦¬ì›Œë“œ ì‚¬ìš©
            rewards = base_rewards
        
        # ì •ê·œí™” (optional)
        if self.normalize and rewards.std() > 0:
            rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-8)
        
        return rewards
