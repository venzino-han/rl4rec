"""
Reward Functions for RL4Rec
NDCG ê¸°ë°˜ ë¦¬ì›Œë“œ ê³„ì‚° ë° TRL í†µí•©
"""

import json
import torch
import numpy as np
from typing import List, Dict, Optional, Tuple
import ray
from pathlib import Path
import argparse


def calculate_dcg(relevance_scores: torch.Tensor, k: Optional[int] = None) -> torch.Tensor:
    """
    DCG (Discounted Cumulative Gain) ê³„ì‚°
    
    Args:
        relevance_scores: [batch_size, num_items] ê´€ë ¨ì„± ì ìˆ˜
        k: top-k ê¹Œì§€ë§Œ ê³„ì‚° (Noneì´ë©´ ì „ì²´)
    
    Returns:
        DCG scores [batch_size]
    """
    if k is not None:
        relevance_scores = relevance_scores[:, :k]
    
    # DCG = sum(rel_i / log2(i + 2)) for i in range(k)
    positions = torch.arange(1, relevance_scores.shape[1] + 1, device=relevance_scores.device)
    discounts = torch.log2(positions + 1.0)
    dcg = (relevance_scores / discounts).sum(dim=1)
    
    return dcg


def calculate_ndcg_from_rank(ranks: torch.Tensor, k: int = 10) -> torch.Tensor:
    """
    Rank ê¸°ë°˜ NDCG ê³„ì‚°
    
    Args:
        ranks: [batch_size] GT ì•„ì´í…œì˜ rank (1-indexed)
        k: Top-K NDCG
    
    Returns:
        NDCG scores [batch_size]
    """
    # Rankê°€ kë³´ë‹¤ í¬ë©´ NDCG = 0
    ndcg_scores = torch.zeros_like(ranks, dtype=torch.float32)
    
    # Rankê°€ k ì´ë‚´ì¸ ê²½ìš°ë§Œ ê³„ì‚°
    valid_mask = ranks <= k
    valid_ranks = ranks[valid_mask]
    
    # DCG = 1 / log2(rank + 1)
    dcg = 1.0 / torch.log2(valid_ranks.float() + 1.0)
    
    # IDCG = 1 / log2(2) (ì´ìƒì ì¸ ê²½ìš°, rank=1)
    idcg = 1.0 / torch.log2(torch.tensor(2.0, device=ranks.device))
    
    # NDCG = DCG / IDCG
    ndcg_scores[valid_mask] = dcg / idcg
    
    return ndcg_scores


def calculate_hit_from_rank(ranks: torch.Tensor, k: int = 10) -> torch.Tensor:
    """
    Rank ê¸°ë°˜ Hit@K ê³„ì‚°
    
    Args:
        ranks: [batch_size] GT ì•„ì´í…œì˜ rank (1-indexed)
        k: Top-K
    
    Returns:
        Hit scores [batch_size]
    """
    # Rankê°€ k ì´ë‚´ë©´ 1, ì•„ë‹ˆë©´ 0
    hit_scores = (ranks <= k).float()
    return hit_scores


def calculate_mrr_from_rank(ranks: torch.Tensor, k: int = 10) -> torch.Tensor:
    """
    Rank ê¸°ë°˜ MRR (Mean Reciprocal Rank) ê³„ì‚°
    
    Args:
        ranks: [batch_size] GT ì•„ì´í…œì˜ rank (1-indexed)
        k: Top-K
    
    Returns:
        MRR scores [batch_size]
    """
    # Rankê°€ kë³´ë‹¤ í¬ë©´ MRR = 0
    mrr_scores = torch.zeros_like(ranks, dtype=torch.float32)
    
    # Rankê°€ k ì´ë‚´ì¸ ê²½ìš°ë§Œ ê³„ì‚°
    valid_mask = ranks <= k
    valid_ranks = ranks[valid_mask]
    
    # MRR = 1 / rank
    mrr_scores[valid_mask] = 1.0 / valid_ranks.float()
    
    return mrr_scores


def calculate_novelty_ndcg(
    ranks: torch.Tensor, 
    item_weights: torch.Tensor,
    k: int = 10, 
) -> torch.Tensor:
    """
    Popularity-based novelty score calculation
    
    NDCGì— ì•„ì´í…œ ì¸ê¸°ë„ ì—­ìˆ˜(item_weights)ë¥¼ ê³±í•˜ì—¬ novelty ê³„ì‚°
    ì¸ê¸° ì—†ëŠ” ì•„ì´í…œì„ ë†’ì€ rankë¡œ ì˜ˆì¸¡í• ìˆ˜ë¡ ë†’ì€ novelty
    
    Args:
        ranks: [batch_size] GT ì•„ì´í…œì˜ rank (1-indexed)
        item_weights: [batch_size] ê° íƒ€ê²Ÿ ì•„ì´í…œì˜ popularity weight (ì—­ìˆ˜)
        k: Top-K
        target_rank: (ì‚¬ìš© ì•ˆí•¨, backward compatibility)
    
    Returns:
        novelty scores [batch_size] = NDCG Ã— item_weights
    """
    # 1. Ranksë¡œë¶€í„° NDCG ê³„ì‚°
    ndcg_scores = calculate_ndcg_from_rank(ranks, k=k)
    
    # 2. NDCGì— popularity weight ê³±í•˜ê¸°
    # ì¸ê¸° ì—†ëŠ” ì•„ì´í…œ(ë†’ì€ weight)ì„ ì˜ ì˜ˆì¸¡í•˜ë©´ ë†’ì€ novelty
    novelty = ndcg_scores * item_weights
    
    return novelty


def calculate_ndcg(
    predicted_scores: torch.Tensor,
    target_items: List[int],
    history_items: List[List[int]],
    k: int = 10,
    use_negatives_only: bool = False,
) -> torch.Tensor:
    """
    NDCG (Normalized Discounted Cumulative Gain) ê³„ì‚°
    
    Args:
        predicted_scores: [batch_size, num_items] ë˜ëŠ” [batch_size, 1+num_negs] ì˜ˆì¸¡ ì ìˆ˜
        target_items: [batch_size] ì‹¤ì œ íƒ€ê²Ÿ ì•„ì´í…œ ID ë¦¬ìŠ¤íŠ¸
        history_items: [batch_size, *] ì‚¬ìš©ìë³„ íˆìŠ¤í† ë¦¬ ì•„ì´í…œ ID ë¦¬ìŠ¤íŠ¸
        k: Top-K NDCG (default: 10)
        use_negatives_only: Trueì´ë©´ target+negativesë§Œ ì‚¬ìš© (scores shape [batch_size, 1+num_negs])
    
    Returns:
        NDCG scores [batch_size]
    """
    batch_size = predicted_scores.shape[0]
    ndcg_scores = torch.zeros(batch_size, device=predicted_scores.device)
    
    if use_negatives_only:
        # Target + negativesë§Œ ê³ ë ¤í•˜ëŠ” ê²½ìš°
        # scores shape: [batch_size, 1 + num_negs]
        # targetì€ í•­ìƒ index 0
        for i in range(batch_size):
            scores = predicted_scores[i]  # [1 + num_negs]
            k_actual = min(k, len(scores))
            
            # Top-K ì¶”ì¶œ
            top_k_scores, top_k_indices = torch.topk(scores, k=k_actual)
            
            # Target (index 0)ì´ top-kì— ìˆëŠ”ì§€ í™•ì¸
            relevance = torch.zeros(k_actual, device=predicted_scores.device)
            target_positions = (top_k_indices == 0).nonzero(as_tuple=True)[0]
            
            if len(target_positions) > 0:
                position = target_positions[0].item()
                relevance[position] = 1.0
                
                # DCG ê³„ì‚°
                dcg = calculate_dcg(relevance.unsqueeze(0), k=k_actual)[0]
                
                # IDCG ê³„ì‚°
                ideal_relevance = torch.zeros(k_actual, device=predicted_scores.device)
                ideal_relevance[0] = 1.0
                idcg = calculate_dcg(ideal_relevance.unsqueeze(0), k=k_actual)[0]
                
                ndcg_scores[i] = dcg / (idcg + 1e-10)
            else:
                ndcg_scores[i] = 0.0
    else:
        # ì „ì²´ ì•„ì´í…œ ê³ ë ¤í•˜ëŠ” ê²½ìš° (ê¸°ì¡´ ë¡œì§)
        for i in range(batch_size):
            # 1. íˆìŠ¤í† ë¦¬ ì•„ì´í…œ ì œì™¸ (masking)
            scores = predicted_scores[i].clone()
            if history_items[i]:
                history_mask = torch.zeros_like(scores, dtype=torch.bool, device=scores.device)
                history_mask[history_items[i]] = True
                history_mask[target_items[i]] = False
                scores[history_mask] = -float('inf')
            
            # 2. Top-K ì•„ì´í…œ ì¶”ì¶œ
            top_k_scores, top_k_indices = torch.topk(scores, k=min(k, len(scores)))
            
            # 3. Relevance ê³„ì‚° (target itemì´ top-kì— ìˆìœ¼ë©´ í•´ë‹¹ ìœ„ì¹˜ì— 1, ì—†ìœ¼ë©´ 0)
            relevance = torch.zeros(k, device=predicted_scores.device)
            target_item = target_items[i]
            
            # Top-kì—ì„œ target itemì˜ ìœ„ì¹˜ ì°¾ê¸°
            target_positions = (top_k_indices == target_item).nonzero(as_tuple=True)[0]
            if len(target_positions) > 0:
                position = target_positions[0].item()
                relevance[position] = 1.0
            
            # 4. DCG ê³„ì‚°
            if relevance.sum() > 0:
                dcg = calculate_dcg(relevance.unsqueeze(0), k=k)[0]
                
                # 5. IDCG (Ideal DCG) ê³„ì‚° - ì´ìƒì ì¸ ê²½ìš° (targetì´ 1ìœ„)
                ideal_relevance = torch.zeros(k, device=predicted_scores.device)
                ideal_relevance[0] = 1.0
                idcg = calculate_dcg(ideal_relevance.unsqueeze(0), k=k)[0]
                
                # 6. NDCG = DCG / IDCG
                ndcg_scores[i] = dcg / (idcg + 1e-10)
            else:
                # Targetì´ top-kì— ì—†ìœ¼ë©´ NDCG = 0
                ndcg_scores[i] = 0.0
    
    return ndcg_scores


def calculate_hit_rate(
    predicted_scores: torch.Tensor,
    target_items: List[int],
    history_items: List[List[int]],
    k: int = 10,
    use_negatives_only: bool = False,
) -> torch.Tensor:
    """
    Hit@K ê³„ì‚° (targetì´ top-kì— ìˆìœ¼ë©´ 1, ì—†ìœ¼ë©´ 0)
    
    Args:
        predicted_scores: [batch_size, num_items] ë˜ëŠ” [batch_size, 1+num_negs]
        target_items: [batch_size]
        history_items: [batch_size, *]
        k: Top-K
        use_negatives_only: Trueì´ë©´ target+negativesë§Œ ì‚¬ìš©
    
    Returns:
        Hit scores [batch_size]
    """
    batch_size = predicted_scores.shape[0]
    hit_scores = torch.zeros(batch_size, device=predicted_scores.device)
    
    if use_negatives_only:
        # Target + negativesë§Œ ê³ ë ¤
        for i in range(batch_size):
            scores = predicted_scores[i]
            k_actual = min(k, len(scores))
            
            # Top-K ì¶”ì¶œ
            _, top_k_indices = torch.topk(scores, k=k_actual)
            
            # Target (index 0)ì´ top-kì— ìˆëŠ”ì§€ í™•ì¸
            if 0 in top_k_indices:
                hit_scores[i] = 1.0
    else:
        # ì „ì²´ ì•„ì´í…œ ê³ ë ¤ (ê¸°ì¡´ ë¡œì§)
        for i in range(batch_size):
            # íˆìŠ¤í† ë¦¬ ì•„ì´í…œ ì œì™¸
            scores = predicted_scores[i].clone()
            if history_items[i]:
                history_mask = torch.zeros_like(scores, dtype=torch.bool, device=scores.device)
                history_mask[history_items[i]] = True
                history_mask[target_items[i]] = False
                scores[history_mask] = -float('inf')
            
            # Top-K ì¶”ì¶œ
            _, top_k_indices = torch.topk(scores, k=min(k, len(scores)))
            
            # Targetì´ top-kì— ìˆëŠ”ì§€ í™•ì¸
            if target_items[i] in top_k_indices:
                hit_scores[i] = 1.0
    
    return hit_scores


def calculate_mrr(
    predicted_scores: torch.Tensor,
    target_items: List[int],
    history_items: List[List[int]],
    k: int = 10,
    use_negatives_only: bool = False,
) -> torch.Tensor:
    """
    MRR (Mean Reciprocal Rank) ê³„ì‚°
    
    Args:
        predicted_scores: [batch_size, num_items] ë˜ëŠ” [batch_size, 1+num_negs]
        target_items: [batch_size]
        history_items: [batch_size, *]
        k: Top-K
        use_negatives_only: Trueì´ë©´ target+negativesë§Œ ì‚¬ìš©
    
    Returns:
        MRR scores [batch_size]
    """
    batch_size = predicted_scores.shape[0]
    mrr_scores = torch.zeros(batch_size, device=predicted_scores.device)
    
    if use_negatives_only:
        # Target + negativesë§Œ ê³ ë ¤
        for i in range(batch_size):
            scores = predicted_scores[i]
            k_actual = min(k, len(scores))
            
            # Top-K ì¶”ì¶œ
            _, top_k_indices = torch.topk(scores, k=k_actual)
            
            # Target (index 0)ì˜ rank ì°¾ê¸°
            target_positions = (top_k_indices == 0).nonzero(as_tuple=True)[0]
            if len(target_positions) > 0:
                rank = target_positions[0].item() + 1  # 1-indexed rank
                mrr_scores[i] = 1.0 / rank
    else:
        # ì „ì²´ ì•„ì´í…œ ê³ ë ¤ (ê¸°ì¡´ ë¡œì§)
        for i in range(batch_size):
            # íˆìŠ¤í† ë¦¬ ì•„ì´í…œ ì œì™¸
            scores = predicted_scores[i].clone()
            if history_items[i]:
                history_mask = torch.zeros_like(scores, dtype=torch.bool)
                history_mask[history_items[i]] = True
                scores[history_mask] = -float('inf')
            
            # Top-K ì¶”ì¶œ
            _, top_k_indices = torch.topk(scores, k=min(k, len(scores)))
            
            # Targetì˜ rank ì°¾ê¸°
            target_positions = (top_k_indices == target_items[i]).nonzero(as_tuple=True)[0]
            if len(target_positions) > 0:
                rank = target_positions[0].item() + 1  # 1-indexed rank
                mrr_scores[i] = 1.0 / rank
    
    return mrr_scores


class RecRewardFrunction:
    """
    TRLê³¼ í˜¸í™˜ë˜ëŠ” ë¦¬ì›Œë“œ í•¨ìˆ˜ í´ë˜ìŠ¤
    Ray RetrievalServiceì™€ í†µí•©
    """
    
    def __init__(
        self,
        retrieval_service_name: str = "RetrievalService",
        namespace: str = "rl4rec",
        data_name: str = "beauty",
        reward_type: str = "ndcg",
        k: int = 10,
        normalize: bool = True,
        test_target: bool = False,
    ):
        """
        Args:
            retrieval_service_name: Ray actor ì´ë¦„
            namespace: Ray namespace
            data_name: ë°ì´í„°ì…‹ ì´ë¦„
            reward_type: ë¦¬ì›Œë“œ íƒ€ì… ('ndcg', 'hit', 'mrr', 'mixed')
            k: Top-K ê°’
            normalize: ë¦¬ì›Œë“œ ì •ê·œí™” ì—¬ë¶€
        """
        self.__name__ = "RecRewardFrunction"
        self.retrieval_service_name = retrieval_service_name
        self.namespace = namespace
        self.data_name = data_name
        self.reward_type = reward_type
        self.k = k
        self.normalize = normalize
        self.test_target = test_target

        # RetrievalService ì—°ê²°
        try:
            self.retrieval_service = ray.get_actor(
                retrieval_service_name,
                namespace=namespace
            )
            print(f"âœ“ Connected to {retrieval_service_name}")
        except ValueError as e:
            raise RuntimeError(
                f"Failed to connect to {retrieval_service_name}. "
                f"Make sure retrieval service is running."
            ) from e

        #load item metadata
        with open(f"data/{data_name}/meta_text_fix.json", "r") as f:
            self.item_metadata = json.load(f)
        self.item_metadata = {int(k): v["title"] + "\n" + v["brand"] + "\n" + v["category"] for k, v in self.item_metadata.items()}
    
    def __call__(
        self,
        generated_texts: List[str],
        targets: List[int],
        histories: List[List[int]],
        neg_items: Optional[List[List[int]]] = None,
        **kwargs
    ) -> torch.Tensor:
        """
        TRL í˜¸í™˜ ë¦¬ì›Œë“œ í•¨ìˆ˜
        
        Args:
            generated_texts: [batch_size] ìƒì„±ëœ í…ìŠ¤íŠ¸
            targets: [batch_size] íƒ€ê²Ÿ ì•„ì´í…œ ID
            histories: [batch_size, *] íˆìŠ¤í† ë¦¬ ì•„ì´í…œ ID
            neg_items: [batch_size, num_negs] Negative ì•„ì´í…œ ID (optional)
        
        Returns:
            rewards: [batch_size] ë¦¬ì›Œë“œ ê°’
        """
        # add target text to generated_texts
        if self.test_target:
            generated_texts = [self.item_metadata[target] + "\n" + generated_text for generated_text, target in zip(generated_texts, targets)]  

        # 1. RetrievalServiceë¥¼ í†µí•´ ìœ ì‚¬ë„ ì ìˆ˜ ê³„ì‚°
        use_negatives_only = neg_items is not None
        
        scores_ref = self.retrieval_service.calculate_reward.remote(
            generated_texts,
            data_name=self.data_name,
            targets=targets if use_negatives_only else None,
            neg_items=neg_items,
        )
        scores = ray.get(scores_ref)  # [batch_size, num_items] or [batch_size, 1+num_negs]
        
        # 2. ë¦¬ì›Œë“œ íƒ€ì…ì— ë”°ë¼ ê³„ì‚°
        if self.reward_type == "ndcg":
            rewards = calculate_ndcg(scores, targets, histories, k=self.k, use_negatives_only=use_negatives_only)
        elif self.reward_type == "hit":
            rewards = calculate_hit_rate(scores, targets, histories, k=self.k, use_negatives_only=use_negatives_only)
        elif self.reward_type == "mrr":
            rewards = calculate_mrr(scores, targets, histories, k=self.k, use_negatives_only=use_negatives_only)
        elif self.reward_type == "mixed":
            # NDCG + Hit@Kì˜ ê°€ì¤‘ í‰ê· 
            ndcg = calculate_ndcg(scores, targets, histories, k=self.k, use_negatives_only=use_negatives_only)
            hit = calculate_hit_rate(scores, targets, histories, k=self.k, use_negatives_only=use_negatives_only)
            rewards = 0.7 * ndcg + 0.3 * hit
        else:
            raise ValueError(f"Unknown reward_type: {self.reward_type}")
        
        # 3. ì •ê·œí™” (optional)
        if self.normalize and rewards.std() > 0:
            rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-8)
        
        return rewards
    
    def compute_metrics(
        self,
        prompts: List[str],
        completions: List[str],
        completion_ids: List[List[int]],
        targets: List[int],
        histories: List[List[int]],
        neg_items: Optional[List[List[int]]] = None,
    ) -> Dict[str, float]:
        """
        í‰ê°€ ë©”íŠ¸ë¦­ ê³„ì‚° (NDCG, Hit@K, MRR ëª¨ë‘)
        
        Args:
            neg_items: [batch_size, num_negs] Negative ì•„ì´í…œ ID (optional)
        
        Returns:
            ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬
        """
        # RetrievalServiceë¥¼ í†µí•´ ìœ ì‚¬ë„ ì ìˆ˜ ê³„ì‚°
        use_negatives_only = neg_items is not None
        
        scores_ref = self.retrieval_service.calculate_reward.remote(
            completions,
            data_name=self.data_name,
            targets=targets if use_negatives_only else None,
            neg_items=neg_items,
        )
        scores = ray.get(scores_ref)
        
        # ëª¨ë“  ë©”íŠ¸ë¦­ ê³„ì‚°
        ndcg = calculate_ndcg(scores, targets, histories, k=self.k, use_negatives_only=use_negatives_only)
        hit = calculate_hit_rate(scores, targets, histories, k=self.k, use_negatives_only=use_negatives_only)
        mrr = calculate_mrr(scores, targets, histories, k=self.k, use_negatives_only=use_negatives_only)
        
        return {
            f"ndcg@{self.k}": ndcg.mean().item(),
            f"hit@{self.k}": hit.mean().item(),
            f"mrr@{self.k}": mrr.mean().item(),
        }


# TRL PPOTrainerì™€ í˜¸í™˜ë˜ëŠ” ë˜í¼ í•¨ìˆ˜
def create_reward_function(
    retrieval_service_name: str = "RetrievalService",
    namespace: str = "rl4rec",
    data_name: str = "beauty",
    reward_type: str = "ndcg",
    k: int = 10,
) -> RecRewardFrunction:
    """
    TRL PPOTrainerì—ì„œ ì‚¬ìš©í•  ë¦¬ì›Œë“œ í•¨ìˆ˜ ìƒì„±
    
    Usage:
        reward_fn = create_reward_function(reward_type="ndcg", k=10)
        rewards = reward_fn(generated_texts, targets, histories)
    """
    return RecRewardFrunction(
        retrieval_service_name=retrieval_service_name,
        namespace=namespace,
        data_name=data_name,
        reward_type=reward_type,
        k=k,
    )


def load_negative_pool(data_name: str, data_dir: str = "data", k: int = 10) -> Dict[int, List[int]]:
    """
    negative.txt íŒŒì¼ì—ì„œ negative pool ë¡œë“œ
    
    íŒŒì¼ í˜•ì‹: ê° ë¼ì¸ì€ "user_id neg_item1 neg_item2 ... neg_itemN"
    
    Args:
        data_name: ë°ì´í„°ì…‹ ì´ë¦„ (e.g., "beauty")
        data_dir: ë°ì´í„° ë””ë ‰í† ë¦¬
    
    Returns:
        user_idë¥¼ í‚¤ë¡œ í•˜ëŠ” negative items ë¦¬ìŠ¤íŠ¸ ë”•ì…”ë„ˆë¦¬
    """
    negative_file = Path(data_dir) / data_name / "negative.txt"
    
    if not negative_file.exists():
        raise FileNotFoundError(f"Negative pool file not found: {negative_file}")
    
    print(f"ğŸ“¦ Loading negative pool from: {negative_file}")
    negative_pool = {}
    
    with open(negative_file, 'r') as f:
        for line in f:
            parts = line.strip().split()
            user_id = int(parts[0])
            neg_items = [int(item_id) for item_id in parts[1:]]
            #random sample k items
            # neg_items = neg_items[:k-1]
            # neg_items = np.random.choice(neg_items, size=k-1, replace=False).tolist()
            negative_pool[user_id] = neg_items
    
    print(f"âœ“ Loaded negative pool for {len(negative_pool)} users")
    if len(negative_pool) > 0:
        sample_user = next(iter(negative_pool))
        print(f"  Example: User {sample_user} has {len(negative_pool[sample_user])} negative items")
    
    return negative_pool


class SimilarHistoryItemMentionReward:
    """
    ìœ ì € êµ¬ë§¤ê¸°ë¡ ì¤‘ íƒ€ê²Ÿê³¼ ê°€ì¥ ìœ ì‚¬ë„ê°€ ë†’ì€ ì•„ì´í…œì˜ titleì„ ì–¸ê¸‰í•  ê²½ìš° ë³´ìƒ
    ì„ë² ë”© ìœ ì‚¬ë„ ê¸°ë°˜ìœ¼ë¡œ ê°€ì¥ ìœ ì‚¬í•œ ì•„ì´í…œì„ ìºì‹±í•˜ê³ , titleì˜ ì²« 3ë‹¨ì–´ë¥¼ ì–¸ê¸‰í•˜ë©´ reward 1ì  ë¶€ì—¬
    """
    
    def __init__(
        self,
        data_name: str,
        item_embeddings: torch.Tensor,
        uid_2_target: Dict[int, int],
        device: str = "cuda",
        data_dir: str = "data",
        use_position_weight: bool = False,
        position_decay: float = 1.0,
        similarity_threshold: float = 0.7,
    ):
        """
        Args:
            data_name: ë°ì´í„°ì…‹ ì´ë¦„
            item_embeddings: ì‚¬ì „ ê³„ì‚°ëœ ì•„ì´í…œ ì„ë² ë”© [num_items+1, emb_dim]
            uid_2_target: ì‚¬ìš©ì ID to íƒ€ê²Ÿ ì•„ì´í…œ ID ë§¤í•‘
            device: ë””ë°”ì´ìŠ¤
            data_dir: ë°ì´í„° ë””ë ‰í† ë¦¬
            use_position_weight: ìœ„ì¹˜ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ì‚¬ìš© ì—¬ë¶€
                                Trueì´ë©´ í…ìŠ¤íŠ¸ ì•ìª½ì— ì–¸ê¸‰ë ìˆ˜ë¡ ë” ë†’ì€ ë³´ìƒ
            position_decay: ìœ„ì¹˜ ê¸°ë°˜ ê°ì†Œìœ¨ (0.0 ~ 1.0)
                          0.0 = ìœ„ì¹˜ ë¬´ê´€í•˜ê²Œ ë™ì¼ ë³´ìƒ
                          1.0 = í…ìŠ¤íŠ¸ ëì—ì„œëŠ” ë³´ìƒ 0
                          ì˜ˆ: 0.5ì´ë©´ í…ìŠ¤íŠ¸ ëì—ì„œ ë³´ìƒì´ ì ˆë°˜ìœ¼ë¡œ ê°ì†Œ
            similarity_threshold: ìœ ì‚¬ë„ ì—­ì¹˜ (0.0 ~ 1.0)
                                ì´ ê°’ ì´í•˜ì´ë©´ ë§ˆì§€ë§‰ ìƒí˜¸ì‘ìš© ì•„ì´í…œì„ ì„ íƒ
        """
        self.__name__ = "SimilarHistoryItemMentionReward"
        self.data_name = data_name
        self.item_embeddings = item_embeddings
        self.device = device
        self.use_position_weight = use_position_weight
        self.position_decay = position_decay
        self.similarity_threshold = similarity_threshold
        
        # ì•„ì´í…œ ë©”íƒ€ë°ì´í„° ë¡œë“œ (title, brand, category)
        with open(f"{data_dir}/{data_name}/meta_text_fix.json", "r") as f:
            self.item_metadata = json.load(f)
        
        print(f"âœ“ SimilarHistoryItemMentionReward initialization started")
        print(f"  - Loaded metadata for {len(self.item_metadata)} items")
        print(f"  - Similarity threshold: {self.similarity_threshold}")
        print(f"    â†’ If max similarity < threshold, use last interacted item")
        if self.use_position_weight:
            print(f"  - Position-based weighting: ENABLED (decay={self.position_decay})")
            print(f"    â†’ Earlier mentions get higher rewards")
        else:
            print(f"  - Position-based weighting: DISABLED")
        
        # ìºì‹œ: user_id -> (most_similar_history_item_id, max_similarity)
        self.similarity_cache = {}
        
        # ì „ì²´ ë°ì´í„°ì— ëŒ€í•´ ë¯¸ë¦¬ ìœ ì‚¬í•œ ì•„ì´í…œ ê³„ì‚°
        print(f"  - Pre-computing most similar history items for all users...")
        self._precompute_similar_items(uid_2_target, data_name, data_dir)
        print(f"âœ“ Pre-computed similar items for {len(self.similarity_cache)} user-target pairs")
    
    def _precompute_similar_items(
        self,
        uid_2_target: Dict[int, int],
        data_name: str,
        data_dir: str
    ):
        """
        ì „ì²´ ë°ì´í„°ì— ëŒ€í•´ íƒ€ê²Ÿê³¼ ê°€ì¥ ìœ ì‚¬í•œ íˆìŠ¤í† ë¦¬ ì•„ì´í…œì„ ë¯¸ë¦¬ ê³„ì‚°
        ìœ ì‚¬ë„ê°€ ì—­ì¹˜ ì´í•˜ì´ë©´ ë§ˆì§€ë§‰ ìƒí˜¸ì‘ìš© ì•„ì´í…œì„ ì„ íƒ
        
        Args:
            uid_2_target: ì‚¬ìš©ì ID to íƒ€ê²Ÿ ì•„ì´í…œ ID ë§¤í•‘
            data_name: ë°ì´í„°ì…‹ ì´ë¦„
            data_dir: ë°ì´í„° ë””ë ‰í† ë¦¬
        """
        # sequential_data.txtì—ì„œ íˆìŠ¤í† ë¦¬ ì •ë³´ ë¡œë“œ
        sequential_file = f"{data_dir}/{data_name}/sequential_data.txt"
        
        # ì •ê·œí™”ëœ ì„ë² ë”© ë¯¸ë¦¬ ê³„ì‚° (ì „ì²´ ì•„ì´í…œ)
        normalized_embeddings = torch.nn.functional.normalize(self.item_embeddings, p=2, dim=1)
        
        fallback_count = 0  # ì—­ì¹˜ ë¯¸ë§Œìœ¼ë¡œ ë§ˆì§€ë§‰ ì•„ì´í…œ ì‚¬ìš©í•œ íšŸìˆ˜
        
        with open(sequential_file, 'r') as f:
            for line in f:
                parts = [int(p) for p in line.strip().split()]
                user_id = parts[0]
                history = parts[1:-3]  # Train setì˜ history
                target_id = parts[-3]   # Train setì˜ target
                
                # uid_2_targetì— í•´ë‹¹í•˜ëŠ” ì‚¬ìš©ìë§Œ ì²˜ë¦¬
                if user_id not in uid_2_target:
                    continue
                
                # íˆìŠ¤í† ë¦¬ê°€ ë¹„ì–´ìˆìœ¼ë©´ ìŠ¤í‚µ
                if len(history) == 0:
                    continue
                
                # íƒ€ê²Ÿ ì„ë² ë”© (ì •ê·œí™”ë¨)
                target_emb = normalized_embeddings[target_id]  # [emb_dim]
                
                # íˆìŠ¤í† ë¦¬ ì„ë² ë”© (ì •ê·œí™”ë¨)
                history_ids = torch.tensor(history, dtype=torch.long, device=self.device)
                history_embs = normalized_embeddings[history_ids]  # [history_len, emb_dim]
                
                # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
                similarities = torch.mm(target_emb.unsqueeze(0), history_embs.T).squeeze(0)  # [history_len]
                
                # ê°€ì¥ ìœ ì‚¬í•œ ì•„ì´í…œ ì°¾ê¸°
                max_similarity = similarities.max().item()
                most_similar_idx = similarities.argmax().item()
                
                # ìœ ì‚¬ë„ê°€ ì—­ì¹˜ ì´í•˜ì´ë©´ ë§ˆì§€ë§‰ ìƒí˜¸ì‘ìš© ì•„ì´í…œ ì„ íƒ
                if max_similarity < self.similarity_threshold:
                    selected_item_id = history[-1]  # ë§ˆì§€ë§‰ ì•„ì´í…œ
                    fallback_count += 1
                else:
                    selected_item_id = history[most_similar_idx]
                
                # ìºì‹œì— ì €ì¥ (ì•„ì´í…œ IDì™€ ìµœëŒ€ ìœ ì‚¬ë„)
                self.similarity_cache[user_id] = (selected_item_id, max_similarity)
        
        # í†µê³„ ì¶œë ¥
        total_users = len(self.similarity_cache)
        if total_users > 0:
            fallback_ratio = (fallback_count / total_users) * 100
            print(f"  - Fallback to last item: {fallback_count}/{total_users} ({fallback_ratio:.1f}%)")

    
    def _get_most_similar_history_item(
        self,
        user_id: int,
    ) -> int:
        """
        íˆìŠ¤í† ë¦¬ ì¤‘ íƒ€ê²Ÿê³¼ ê°€ì¥ ìœ ì‚¬í•œ ì•„ì´í…œ ì°¾ê¸° (ìºì‹œì—ì„œ ê°€ì ¸ì˜¤ê±°ë‚˜ ì‹¤ì‹œê°„ ê³„ì‚°)
        ìœ ì‚¬ë„ê°€ ì—­ì¹˜ ì´í•˜ì´ë©´ ë§ˆì§€ë§‰ ìƒí˜¸ì‘ìš© ì•„ì´í…œ ë°˜í™˜
        
        Args:
            user_id: ì‚¬ìš©ì ID
            
        Returns:
            selected_item_id: ì„ íƒëœ íˆìŠ¤í† ë¦¬ ì•„ì´í…œ ID
                            (ìœ ì‚¬ë„ ì—­ì¹˜ ì´ìƒ: ê°€ì¥ ìœ ì‚¬í•œ ì•„ì´í…œ,
                             ìœ ì‚¬ë„ ì—­ì¹˜ ë¯¸ë§Œ: ë§ˆì§€ë§‰ ìƒí˜¸ì‘ìš© ì•„ì´í…œ)
        """        
        selected_item_id, _ = self.similarity_cache[user_id]
        return selected_item_id
    
    def _get_first_three_words(self, title: str) -> str:
        """
        Titleì˜ ì²« 3ë‹¨ì–´ ì¶”ì¶œ
        
        Args:
            title: ì•„ì´í…œ title
            
        Returns:
            first_three_words: ì²« 3ë‹¨ì–´ë¥¼ ê³µë°±ìœ¼ë¡œ ì—°ê²°í•œ ë¬¸ìì—´ (ì†Œë¬¸ì)
        """
        words = title.strip().split()
        first_three = " ".join(words[:3])
        return first_three.lower()
    
    def _calculate_position_weight(self, position: int, text_length: int) -> float:
        """
        ìœ„ì¹˜ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ê³„ì‚°
        
        Args:
            position: ì–¸ê¸‰ëœ ìœ„ì¹˜ (ë¬¸ì ì¸ë±ìŠ¤)
            text_length: ì „ì²´ í…ìŠ¤íŠ¸ ê¸¸ì´
            
        Returns:
            weight: ìœ„ì¹˜ ê¸°ë°˜ ê°€ì¤‘ì¹˜ (0.0 ~ 1.0)
                   ì•ìª½ì¼ìˆ˜ë¡ 1.0ì— ê°€ê¹ê³ , ë’¤ìª½ì¼ìˆ˜ë¡ ê°ì†Œ
        """
        if text_length == 0:
            return 1.0
        
        # ìƒëŒ€ì  ìœ„ì¹˜ ê³„ì‚° (0.0 = ë§¨ ì•, 1.0 = ë§¨ ë’¤)
        position_ratio = position / text_length
        
        # ê°€ì¤‘ì¹˜ ê³„ì‚°: 1.0 - (position_ratio * decay)
        # decay=0.0 â†’ ìœ„ì¹˜ ë¬´ê´€í•˜ê²Œ 1.0
        # decay=1.0 â†’ ë§¨ ë’¤ì—ì„œëŠ” 0.0
        # decay=0.5 â†’ ë§¨ ë’¤ì—ì„œëŠ” 0.5
        weight = 1.0 - (position_ratio * self.position_decay)
        
        return max(0.0, weight)  # ìµœì†Œê°’ 0.0 ë³´ì¥
    
    def __call__(
        self,
        generated_texts: List[str],
        targets: List[int],
        histories: List[List[int]],
        user_ids: List[int],
        **kwargs
    ) -> List[float]:
        """
        ìƒì„±ëœ í…ìŠ¤íŠ¸ì—ì„œ ìœ ì‚¬í•œ íˆìŠ¤í† ë¦¬ ì•„ì´í…œì˜ title ì–¸ê¸‰ ì—¬ë¶€ë¥¼ í™•ì¸í•˜ì—¬ ë³´ìƒ
        
        Args:
            generated_texts: [batch_size] ìƒì„±ëœ í…ìŠ¤íŠ¸
            targets: [batch_size] íƒ€ê²Ÿ ì•„ì´í…œ ID
            histories: [batch_size, *] íˆìŠ¤í† ë¦¬ ì•„ì´í…œ ID ë¦¬ìŠ¤íŠ¸
            user_ids: [batch_size] ì‚¬ìš©ì ID
            
        Returns:
            rewards: [batch_size] ë³´ìƒ ê°’
                    - use_position_weight=False: 0 ë˜ëŠ” 1.0
                    - use_position_weight=True: 0 ~ 1.0 (ìœ„ì¹˜ì— ë”°ë¼ ê°€ì¤‘)
        """
        rewards = []
        
        for gen_text, target_id, history_ids, user_id in zip(generated_texts, targets, histories, user_ids):
            reward = 0.0
            
            # ê°€ì¥ ìœ ì‚¬í•œ íˆìŠ¤í† ë¦¬ ì•„ì´í…œ ì°¾ê¸°
            most_similar_item_id = self._get_most_similar_history_item(user_id)
            
            # í•´ë‹¹ ì•„ì´í…œì˜ title ê°€ì ¸ì˜¤ê¸°
            if str(most_similar_item_id) in self.item_metadata:
                item_title = self.item_metadata[str(most_similar_item_id)]["title"]
                first_three_words = self._get_first_three_words(item_title)
                
                # ìƒì„±ëœ í…ìŠ¤íŠ¸ì— ì²« 3ë‹¨ì–´ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸ (ëŒ€ì†Œë¬¸ì ë¬´ì‹œ)
                gen_text_lower = gen_text.lower()
                if first_three_words in gen_text_lower:
                    if self.use_position_weight:
                        # ìœ„ì¹˜ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ì ìš©
                        position = gen_text_lower.find(first_three_words)
                        text_length = len(gen_text_lower)
                        weight = self._calculate_position_weight(position, text_length)
                        reward = 1.0 * weight
                    else:
                        # ìœ„ì¹˜ ë¬´ê´€í•˜ê²Œ 1.0ì 
                        reward = 1.0
            
            rewards.append(reward)
        
        return rewards


class BrandMentionReward:
    """
    íƒ€ê²Ÿ ì•„ì´í…œì˜ ë¸Œëœë“œë¥¼ ì–¸ê¸‰í•  ê²½ìš° ë³´ìƒ (0.5ì )
    """
    
    def __init__(
        self,
        data_name: str,
        device: str = "cuda",
        data_dir: str = "data",
    ):
        """
        Args:
            data_name: ë°ì´í„°ì…‹ ì´ë¦„
            device: ë””ë°”ì´ìŠ¤
            data_dir: ë°ì´í„° ë””ë ‰í† ë¦¬
        """
        self.__name__ = "BrandMentionReward"
        self.data_name = data_name
        self.device = device
        
        # ì•„ì´í…œ ë©”íƒ€ë°ì´í„° ë¡œë“œ
        with open(f"{data_dir}/{data_name}/meta_text_fix.json", "r") as f:
            item_metadata = json.load(f)
            item_metadata = {int(k): v for k, v in item_metadata.items()}
        self.item_brands = {item_id: str(item_metadata[item_id]["brand"]) for item_id in item_metadata}
        
        print(f"âœ“ BrandMentionReward initialized")
        print(f"  - Loaded brands for {len(self.item_brands)} items")
    
    def __call__(
        self,
        generated_texts: List[str],
        targets: List[int],
        **kwargs
    ) -> List[float]:
        """
        ìƒì„±ëœ í…ìŠ¤íŠ¸ì—ì„œ íƒ€ê²Ÿ ì•„ì´í…œì˜ ë¸Œëœë“œ ì–¸ê¸‰ ì—¬ë¶€ë¥¼ í™•ì¸í•˜ì—¬ ë³´ìƒ
        
        Args:
            generated_texts: [batch_size] ìƒì„±ëœ í…ìŠ¤íŠ¸
            targets: [batch_size] íƒ€ê²Ÿ ì•„ì´í…œ ID
            
        Returns:
            rewards: [batch_size] ë³´ìƒ ê°’ (0 ë˜ëŠ” 0.5)
        """
        rewards = []
        
        for gen_text, target_id in zip(generated_texts, targets):
            reward = 0.0
            if self.item_brands[target_id].lower() in gen_text.lower():
                reward = 0.5
            rewards.append(reward)
        return rewards


class CategoryMentionReward:
    """
    íƒ€ê²Ÿ ì•„ì´í…œì˜ ì¹´í…Œê³ ë¦¬ë¥¼ ì–¸ê¸‰í•  ê²½ìš° ë³´ìƒ (0.5ì )
    """
    
    def __init__(
        self,
        data_name: str,
        device: str = "cuda",
        data_dir: str = "data",
    ):
        """
        Args:
            data_name: ë°ì´í„°ì…‹ ì´ë¦„
            device: ë””ë°”ì´ìŠ¤
            data_dir: ë°ì´í„° ë””ë ‰í† ë¦¬
        """
        self.__name__ = "CategoryMentionReward"
        self.data_name = data_name
        self.device = device
        
        # ì•„ì´í…œ ë©”íƒ€ë°ì´í„° ë¡œë“œ
        with open(f"{data_dir}/{data_name}/meta_text_fix.json", "r") as f:
            item_metadata = json.load(f)
            item_metadata = {int(k): v for k, v in item_metadata.items()}
        self.item_categories = {item_id: str(item_metadata[item_id]["category"]) for item_id in item_metadata}
        print(f"âœ“ CategoryMentionReward initialized")
        print(f"  - Loaded categories for {len(self.item_categories)} items")
    
    def __call__(
        self,
        generated_texts: List[str],
        targets: List[int],
        **kwargs
    ) -> List[float]:
        """
        ìƒì„±ëœ í…ìŠ¤íŠ¸ì—ì„œ íƒ€ê²Ÿ ì•„ì´í…œì˜ ì¹´í…Œê³ ë¦¬ ì–¸ê¸‰ ì—¬ë¶€ë¥¼ í™•ì¸í•˜ì—¬ ë³´ìƒ
        
        Args:
            generated_texts: [batch_size] ìƒì„±ëœ í…ìŠ¤íŠ¸
            targets: [batch_size] íƒ€ê²Ÿ ì•„ì´í…œ ID
            
        Returns:
            rewards: [batch_size] ë³´ìƒ ê°’ (0 ë˜ëŠ” 0.5)
        """
        rewards = []
        
        for gen_text, target_id in zip(generated_texts, targets):
            reward = 0.0
            if self.item_categories[target_id].lower() in gen_text.lower():
                reward = 0.5
            rewards.append(reward)
        
        return rewards


class LocalEmbeddingRewardFunction:
    """
    ë¡œì»¬ ì„ë² ë”© ê¸°ë°˜ ë¦¬ì›Œë“œ í•¨ìˆ˜
    DB ëŒ€ì‹  ìì²´ì ìœ¼ë¡œ negative item ì„ë² ë”©ì„ ê³„ì‚°í•˜ì—¬ NDCGë¥¼ rewardë¡œ í™œìš©
    """
    
    def __init__(
        self,
        args: argparse.Namespace,
        uid_2_target: Dict[int, int],
    ):
        """
        Args:
            uid_2_target: ì‚¬ìš©ì ID to íƒ€ê²Ÿ ì•„ì´í…œ ID ë§¤í•‘
            data_name: ë°ì´í„°ì…‹ ì´ë¦„
            k: Top-K ê°’
            reward_type: ë¦¬ì›Œë“œ íƒ€ì… ('ndcg', 'hit', 'mrr', 'mixed')
            emb_model_name: ì„ë² ë”© ëª¨ë¸ ì´ë¦„
            emb_type: ì„ë² ë”© íƒ€ì… (review_description, title, etc.)
            device: ë””ë°”ì´ìŠ¤
            emb_batch_size: ì„ë² ë”© ê³„ì‚° ë°°ì¹˜ í¬ê¸°
            data_dir: ë°ì´í„° ë””ë ‰í† ë¦¬
            normalize: ë¦¬ì›Œë“œ ì •ê·œí™” ì—¬ë¶€
            novelty_reward: Novelty ë¦¬ì›Œë“œ ì‚¬ìš© ì—¬ë¶€ (True/False)
                           Novelty = NDCG Ã— popularity_weight
            novelty_coef: Novelty ë¦¬ì›Œë“œ ê³„ìˆ˜ (default: 1.0)
            novelty_target_rank: (ì‚¬ìš© ì•ˆí•¨, backward compatibility)
            novelty_mode: (ì‚¬ìš© ì•ˆí•¨, backward compatibility)
            novelty_annealing: Novelty annealing ì‚¬ìš© ì—¬ë¶€
                              Trueì´ë©´ í•™ìŠµ ì§„í–‰ë„ì— ë”°ë¼ novelty ë¹„ìœ¨ì„ 0â†’1ë¡œ ì„ í˜• ì¦ê°€
                              Final reward = (1-ratio)*base + ratio*novelty_coef*novelty
            popularity_coef: Popularity ë¦¬ì›Œë“œ ê³„ìˆ˜ (0.0 = ì‚¬ìš© ì•ˆí•¨)
                            ì •ë‹µì¸ ê²½ìš°ì—ë§Œ popularity bonus ì¶”ê°€
            target_emb_reward: íƒ€ê²Ÿ ì„ë² ë”© ìœ ì‚¬ë„ ë¦¬ì›Œë“œ ì‚¬ìš© ì—¬ë¶€
            target_emb_coef: íƒ€ê²Ÿ ì„ë² ë”© ë¦¬ì›Œë“œ ê³„ìˆ˜
            infonce_reward: InfoNCE (ëŒ€ì¡° í•™ìŠµ) ë¦¬ì›Œë“œ ì‚¬ìš© ì—¬ë¶€
            infonce_coef: InfoNCE ë¦¬ì›Œë“œ ê³„ìˆ˜
            infonce_temperature: InfoNCE temperature íŒŒë¼ë¯¸í„° (default: 0.07)
            infonce_emb_type: InfoNCEìš© ì„ë² ë”© íƒ€ì… (Noneì´ë©´ emb_typeê³¼ ë™ì¼)
            proxy_label_reward: Proxy label ë¦¬ì›Œë“œ ì‚¬ìš© ì—¬ë¶€
                               Trueì´ë©´ íƒ€ê²Ÿê³¼ ìœ ì‚¬í•œ ìƒìœ„ proxy_kê°œ ì•„ì´í…œë„ ë¶€ë¶„ì ìœ¼ë¡œ ì •ë‹µìœ¼ë¡œ ì·¨ê¸‰
                               ê¸°ì¡´ base_rewardì— ì¶”ê°€ë¡œ ë”í•´ì§
            proxy_k: Proxy labelë¡œ ì‚¬ìš©í•  ìœ ì‚¬í•œ ì•„ì´í…œ ê°œìˆ˜
            proxy_label_coef: Proxy label ë¦¬ì›Œë“œ ê³„ìˆ˜
            max_steps: ìµœëŒ€ í•™ìŠµ ìŠ¤í… ìˆ˜ (novelty annealing ê³„ì‚°ì— ì‚¬ìš©)
        """
        self.__name__ = "LocalEmbeddingRewardFunction"
        self.args = args
        self.data_name = args.data_name
        self.reward_type = args.reward_type
        self.k = args.k
        self.normalize = args.normalize_rewards
        self.device = args.device
        self.emb_batch_size = args.emb_batch_size
        self.uid_2_target = uid_2_target  # Store for full item pool ranking
        
        # Novelty ê´€ë ¨ íŒŒë¼ë¯¸í„°
        self.novelty_reward = args.novelty_reward
        self.novelty_coef = args.novelty_coef
        self.novelty_target_rank = args.novelty_target_rank
        self.novelty_mode = args.novelty_mode
        self.novelty_annealing = args.novelty_annealing
        
        # Popularity ê´€ë ¨ íŒŒë¼ë¯¸í„°
        self.popularity_coef = args.popularity_coef
        
        # Target embedding ìœ ì‚¬ë„ ë¦¬ì›Œë“œ íŒŒë¼ë¯¸í„°
        self.target_emb_reward = args.target_emb_reward
        self.target_emb_file = args.target_emb_file
        self.target_emb_coef = args.target_emb_coef
        
        # InfoNCE ë¦¬ì›Œë“œ íŒŒë¼ë¯¸í„°
        self.infonce_reward = args.infonce_reward
        self.infonce_coef = args.infonce_coef
        self.infonce_temperature = args.infonce_temperature
        self.infonce_emb_type = args.infonce_emb_type if args.infonce_emb_type is not None else args.emb_type
        
        # Proxy label ë¦¬ì›Œë“œ íŒŒë¼ë¯¸í„°
        if hasattr(args, "proxy_label_reward"):
            self.proxy_label_reward = args.proxy_label_reward
            self.proxy_k = args.proxy_k
            self.proxy_label_coef = args.proxy_label_coef
        else:
            self.proxy_label_reward = False
            self.proxy_k = 0
            self.proxy_label_coef = 0
        
        # Training ê´€ë ¨ íŒŒë¼ë¯¸í„°
        self.max_steps = args.max_steps
        
        print(f"ğŸ’° Reward configuration:")
        print(f"  - Reward type: {self.reward_type}")
        print(f"  - Top-K: {self.k}")
        print(f"  - Normalize: {self.normalize}")
        if self.novelty_reward:
            print(f"  - Novelty reward: ENABLED")
            print(f"  - Novelty coefficient: {self.novelty_coef}")
            print(f"  - Novelty = NDCG Ã— popularity_weight (ì¸ê¸° ì—†ëŠ” ì•„ì´í…œ ì¥ë ¤)")
            if self.novelty_annealing:
                print(f"  - Novelty annealing: ENABLED")
                print(f"  - Novelty ratio will increase linearly from 0 to 1 over {self.max_steps} steps")
                print(f"  - Final reward = (1-ratio)*base + ratio*novelty")
        if self.popularity_coef > 0:
            print(f"  - Popularity coefficient: {self.popularity_coef}")
            print(f"  - Popularity bonus for unpopular items (when correct)")
        if self.target_emb_reward:
            print(f"  - Target embedding reward: ENABLED")
            print(f"  - Target embedding file: {self.target_emb_file}")
            print(f"  - Target embedding coefficient: {self.target_emb_coef}")
            print(f"  - Reward based on cosine similarity with target embedding")
        if self.infonce_reward:
            print(f"  - InfoNCE reward: ENABLED")
            print(f"  - InfoNCE coefficient: {self.infonce_coef}")
            print(f"  - InfoNCE temperature: {self.infonce_temperature}")
            print(f"  - InfoNCE embedding type: {self.infonce_emb_type}")
            print(f"  - Contrastive learning: maximize target similarity, minimize negative similarity")
        if self.proxy_label_reward:
            print(f"  - Proxy label reward: ENABLED")
            print(f"  - Proxy K: {self.proxy_k}")
            print(f"  - Proxy label coefficient: {self.proxy_label_coef}")
            print(f"  - Use top-{self.proxy_k} similar items as soft labels with similarity-weighted NDCG")
            print(f"  - Final reward = base_reward + proxy_label_coef * proxy_label_ndcg")
        
        # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
        print(f"ğŸ¤– Loading embedding model: {args.emb_model_name}")
        from sentence_transformers import SentenceTransformer
        self.emb_model = SentenceTransformer(args.emb_model_name, device=self.device)
        print(f"âœ“ Embedding model loaded on {self.device}")

        total_user_count = 0
        sequential_file = f"data/{self.data_name}/sequential_data.txt"        
        with open(sequential_file, 'r') as f:
            for line in f:
                total_user_count += 1
        
        # k > 100ì´ë©´ ì „ì²´ ì•„ì´í…œ í’€ ì‚¬ìš©, ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ negative pool ì‚¬ìš©
        self.use_full_item_pool = (self.k > 100)
        if self.use_full_item_pool:
            print(f"âš ï¸ k={self.k} > 100: Using full item pool for ranking (no negative sampling)")
            self.negative_pool = None
            self.candidate_tensor = None  # Will use full item embeddings
        else:
            # Negative pool ë¡œë“œ
            self.negative_pool = load_negative_pool(self.data_name, args.data_dir, self.k)
            # prepare candidate set, target comes first
            self.candidate_tensor = self._prepare_candidate_tensor(total_user_count, uid_2_target, self.negative_pool)
        
        # ì‚¬ì „ ê³„ì‚°ëœ ì•„ì´í…œ ì„ë² ë”© ë¡œë“œ
        emb_model_name_dir = args.emb_model_name.split("/")[-1]
        item_embedding_file_path = f"data_emb/{self.data_name}_{args.emb_type}_{emb_model_name_dir}_emb.pt"
        print(f"ğŸ“¦ Loading pre-computed item embeddings from: {item_embedding_file_path}")
        self.item_embeddings = torch.load(item_embedding_file_path, map_location=self.device)
        print(f"âœ“ Loaded embeddings for {len(self.item_embeddings)} items")
        
        # InfoNCEìš© ì¶”ê°€ ì„ë² ë”© ë¡œë“œ (í•„ìš” ì‹œ)
        if self.infonce_reward and self.infonce_emb_type != args.emb_type:
            infonce_embedding_file_path = f"data_emb/{self.data_name}_{self.infonce_emb_type}_{emb_model_name_dir}_emb.pt"
            print(f"ğŸ“¦ Loading InfoNCE embeddings from: {infonce_embedding_file_path}")
            self.infonce_item_embeddings = torch.load(infonce_embedding_file_path, map_location=self.device)
            print(f"âœ“ Loaded InfoNCE embeddings for {len(self.infonce_item_embeddings)} items")
        else:
            # ê°™ì€ ì„ë² ë”© ì‚¬ìš©
            self.infonce_item_embeddings = self.item_embeddings if self.infonce_reward else None
        
        # Proxy labelì„ ìœ„í•œ ì•„ì´í…œ ê°„ ìœ ì‚¬ë„ ë¡œë“œ ë˜ëŠ” ê³„ì‚°
        if self.proxy_label_reward:
            # ì €ì¥ëœ proxy labels íŒŒì¼ í™•ì¸
            proxy_labels_file = f"data_emb/{self.data_name}_proxy_labels_k100_{args.emb_type}_{emb_model_name_dir}.json"
            proxy_labels_path = Path(proxy_labels_file)
            
            if proxy_labels_path.exists():
                print(f"ğŸ“¦ Loading pre-computed proxy labels from: {proxy_labels_file}")
                self.item_proxy_labels = self._load_proxy_labels(proxy_labels_path)
                print(f"âœ“ Loaded proxy labels for {len(self.item_proxy_labels)} items")
            else:
                print(f"âš ï¸  Pre-computed proxy labels not found: {proxy_labels_file}")
                print(f"   Computing proxy labels on-the-fly (this may take time)...")
                exit()

        else:
            self.item_proxy_labels = None
        
        # Target embeddings ì¤€ë¹„ (target_emb_reward ì‚¬ìš© ì‹œ)
        if self.target_emb_reward:
            self.target_embeddings = self._prepare_target_embeddings(uid_2_target)
            print(f"âœ“ Prepared target embeddings for {len(uid_2_target)} users")
        else:
            self.target_embeddings = None
        
        # ì•„ì´í…œ ì¸ê¸°ë„ ê³„ì‚° (train setì—ì„œ)
        # Novelty ë˜ëŠ” Popularity reward ì‚¬ìš© ì‹œ í•„ìš”
        if self.novelty_reward or self.popularity_coef > 0:
            self.item_popularity_weights = self._compute_item_popularity(
                uid_2_target, self.negative_pool, self.data_name, args.data_dir
            )
        else:
            self.item_popularity_weights = None

    def _prepare_candidate_tensor(self, total_user_count: int, uid_2_target: Dict[int, int], neg_pool: Dict[int, List[int]]) -> torch.Tensor:
        candidate_tensor = torch.zeros(total_user_count+1, len(list(neg_pool.values())[0])+1, dtype=torch.long)
        for uid, target_id in uid_2_target.items():
            candidate_tensor[uid] = torch.tensor([target_id] + neg_pool[uid], dtype=torch.long)
        return candidate_tensor
    
    def _load_proxy_labels(self, file_path: Path) -> Dict[int, Tuple[torch.Tensor, torch.Tensor]]:
        """
        ì €ì¥ëœ proxy labelsë¥¼ ë¡œë“œ
        
        Args:
            file_path: proxy labels JSON íŒŒì¼ ê²½ë¡œ
            
        Returns:
            item_proxy_labels: Dict[item_id, (proxy_item_ids, proxy_similarities)]
        """
        with open(file_path, 'r') as f:
            proxy_labels_json = json.load(f)
        
        # JSONì—ì„œ ë¡œë“œí•œ ë°ì´í„°ë¥¼ í…ì„œë¡œ ë³€í™˜
        item_proxy_labels = {}
        for item_id_str, proxy_list in proxy_labels_json.items():
            proxy_list = proxy_list[:self.proxy_k]
            item_id = int(item_id_str)
            
            # List[Tuple[item_id, similarity]]ë¥¼ ë‘ ê°œì˜ í…ì„œë¡œ ë¶„ë¦¬
            proxy_ids = torch.tensor([p[0] for p in proxy_list], dtype=torch.long, device=self.device)
            proxy_sims = torch.tensor([p[1] for p in proxy_list], dtype=torch.float32, device=self.device)
            
            item_proxy_labels[item_id] = (proxy_ids, proxy_sims)
        
        return item_proxy_labels
    
    def _prepare_target_embeddings(self, uid_2_target: Dict[int, int]) -> torch.Tensor:
        """
        ê° ì‚¬ìš©ìì˜ íƒ€ê²Ÿ ì•„ì´í…œ ì„ë² ë”©ì„ ì¤€ë¹„
        
        Args:
            uid_2_target: ì‚¬ìš©ì ID to íƒ€ê²Ÿ ì•„ì´í…œ ID ë§¤í•‘
            
        Returns:
            target_embeddings: [max_uid+1, emb_dim] ê° ì‚¬ìš©ìì˜ íƒ€ê²Ÿ ì„ë² ë”©
        """
        if self.target_emb_file is not None:
            target_embeddings = torch.load(f"data_emb/{self.args.target_emb_file}", map_location=self.device)
            return target_embeddings
        
        max_uid = max(uid_2_target.keys())
        emb_dim = self.item_embeddings.shape[1]
        
        # ì‚¬ìš©ìë³„ íƒ€ê²Ÿ ì„ë² ë”© í…ì„œ ì´ˆê¸°í™”
        target_embeddings = torch.zeros(max_uid + 1, emb_dim, device=self.device)
        
        for uid, target_id in uid_2_target.items():
            target_embeddings[uid] = self.item_embeddings[target_id]
        
        return target_embeddings
    
    def _compute_item_popularity(
        self, 
        uid_2_target: Dict[int, int], 
        neg_pool: Dict[int, List[int]],
        data_name: str,
        data_dir: str
    ) -> torch.Tensor:
        """
        Train setì—ì„œ ì•„ì´í…œ ì¸ê¸°ë„ ê³„ì‚° (log ì—­ìˆ˜ ì‚¬ìš©)
        
        Args:
            uid_2_target: ì‚¬ìš©ì ID to íƒ€ê²Ÿ ì•„ì´í…œ ID ë§¤í•‘
            neg_pool: Negative pool
            data_name: ë°ì´í„°ì…‹ ì´ë¦„
            data_dir: ë°ì´í„° ë””ë ‰í† ë¦¬
        
        Returns:
            item_popularity_weights: [num_items] ê° ì•„ì´í…œì˜ ì¸ê¸°ë„ ê°€ì¤‘ì¹˜
                                     ì¸ê¸° ì—†ëŠ” ì•„ì´í…œì¼ìˆ˜ë¡ ë†’ì€ ê°’
        """
        print(f"ğŸ“Š Computing item popularity weights from train set...")
        
        # sequential_data.txtì—ì„œ train set ë¡œë“œ (target_index=-3)
        sequential_file = f"{data_dir}/{data_name}/sequential_data.txt"
        item_counts = {}
        
        with open(sequential_file, 'r') as f:
            for line in f:
                parts = [int(p) for p in line.strip().split()]
                user_id = parts[0]
                history = parts[1:-3]  # Train setì˜ history
                target = parts[-3]  # Train setì˜ target
                
                # Historyì˜ ëª¨ë“  ì•„ì´í…œ ì¹´ìš´íŠ¸
                for item_id in history:
                    item_counts[item_id] = item_counts.get(item_id, 0) + 1
                
                # Target ì•„ì´í…œë„ ì¹´ìš´íŠ¸
                item_counts[target] = item_counts.get(target, 0) + 1
        
        # ì „ì²´ ì•„ì´í…œ ìˆ˜ íŒŒì•…
        all_item_ids = set(item_counts.keys())
        max_item_id = max(all_item_ids) if all_item_ids else 0
        
        print(f"  Total unique items in train set: {len(all_item_ids)}")
        print(f"  Max item ID: {max_item_id}")
        
        # ì¸ê¸°ë„ ê°€ì¤‘ì¹˜ ê³„ì‚°: log(count + 1)ì˜ ì—­ìˆ˜
        # ì¸ê¸° ë§ì€ ì•„ì´í…œ -> ë‚®ì€ ê°€ì¤‘ì¹˜
        # ì¸ê¸° ì—†ëŠ” ì•„ì´í…œ -> ë†’ì€ ê°€ì¤‘ì¹˜
        item_weights = torch.ones(max_item_id + 1, device=self.device)
        
        for item_id, count in item_counts.items():
            # log(count + 1)ì˜ ì—­ìˆ˜
            item_weights[item_id] = 1.0 / np.log(count + 1)
        
        # ì •ê·œí™” (í‰ê· ì´ 1ì´ ë˜ë„ë¡)
        # ë‚˜íƒ€ë‚˜ì§€ ì•Šì€ ì•„ì´í…œì€ 1.0 ìœ ì§€
        appeared_mask = torch.zeros(max_item_id + 1, dtype=torch.bool, device=self.device)
        for item_id in all_item_ids:
            appeared_mask[item_id] = True
        
        if appeared_mask.sum() > 0:
            mean_weight = item_weights[appeared_mask].mean()
            item_weights[appeared_mask] = item_weights[appeared_mask] / mean_weight
        
        print(f"  Item popularity weight statistics:")
        print(f"    Min: {item_weights[appeared_mask].min().item():.4f}")
        print(f"    Max: {item_weights[appeared_mask].max().item():.4f}")
        print(f"    Mean: {item_weights[appeared_mask].mean().item():.4f}")
        print(f"    Std: {item_weights[appeared_mask].std().item():.4f}")
        
        # ì˜ˆì‹œ ì¶œë ¥
        sorted_counts = sorted(item_counts.items(), key=lambda x: x[1], reverse=True)
        print(f"  Top-5 popular items:")
        for item_id, count in sorted_counts[:5]:
            weight = item_weights[item_id].item()
            print(f"    Item {item_id}: count={count}, weight={weight:.4f}")
        
        print(f"  Bottom-5 popular items:")
        for item_id, count in sorted_counts[-5:]:
            weight = item_weights[item_id].item()
            print(f"    Item {item_id}: count={count}, weight={weight:.4f}")
        
        print(f"âœ“ Item popularity weights computed")
        
        return item_weights
    
    def _encode_texts(self, generated_texts: List[str]) -> torch.Tensor:
        """
        ìƒì„±ëœ í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
        
        Args:
            generated_texts: [batch_size] ìƒì„±ëœ í…ìŠ¤íŠ¸
            
        Returns:
            embeddings: [batch_size, emb_dim] ì„ë² ë”©
        """
        embeddings = self.emb_model.encode(
            generated_texts,
            convert_to_tensor=True,
            show_progress_bar=False,
            device=self.device,
            batch_size=self.emb_batch_size,
        )
        return embeddings
    
    def _compute_similarity_scores(
        self,
        query_embeddings: torch.Tensor,
        user_ids: torch.Tensor,
        return_scores: bool = False,
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
        """
        Compute similarity scores between query embeddings and candidate set
        Args:
            query_embeddings: [batch_size, emb_dim] query embeddings
            user_ids: [batch_size] user ids
            return_scores: if True, return (ranks, scores), otherwise return (ranks, None)
        Returns:
            ranks: [batch_size] ranks of target items
            scores: [batch_size, num_candidates] similarity scores (only if return_scores=True)
        """
        if self.use_full_item_pool:
            # ì „ì²´ ì•„ì´í…œ í’€ì— ëŒ€í•´ ìœ ì‚¬ë„ ê³„ì‚°
            # query_embeddings: [batch_size, emb_dim]
            # item_embeddings: [num_items, emb_dim]
            scores = torch.mm(query_embeddings, self.item_embeddings.T)  # [batch_size, num_items]
            
            # ê° ì‚¬ìš©ìì˜ target item ê°€ì ¸ì˜¤ê¸°
            target_item_ids = torch.tensor(
                [self.uid_2_target[uid] for uid in user_ids],
                device=self.device
            )  # [batch_size]
            
            # Target itemì˜ ì ìˆ˜
            target_scores = scores[torch.arange(scores.size(0), device=self.device), target_item_ids]  # [batch_size]
            
            # Rank ê³„ì‚°: targetë³´ë‹¤ ë†’ì€ ì ìˆ˜ë¥¼ ê°€ì§„ ì•„ì´í…œì˜ ê°œìˆ˜ + 1
            ranks = (scores > target_scores.unsqueeze(1)).sum(dim=1) + 1
        else:
            # Negative pool ê¸°ë°˜ ê³„ì‚° (ê¸°ì¡´ ë¡œì§)
            batch_candidate_tensor = self.candidate_tensor[user_ids]
            scores = torch.bmm(query_embeddings.unsqueeze(1), self.item_embeddings[batch_candidate_tensor].transpose(1, 2)).squeeze(1)
            target_scores = scores[:, 0].unsqueeze(1)
            ranks = (scores > target_scores).sum(dim=1) + 1
        
        if return_scores:
            return ranks, scores
        else:
            return ranks, None
    
    def _compute_proxy_label_ndcg(
        self,
        query_embeddings: torch.Tensor,
        user_ids: torch.Tensor,
        predicted_scores: torch.Tensor,
    ) -> torch.Tensor:
        """
        Proxy label ê¸°ë°˜ NDCG ê³„ì‚°
        ë¯¸ë¦¬ ê³„ì‚°ëœ ì•„ì´í…œ ê°„ ìœ ì‚¬ë„ë¥¼ í™œìš©í•˜ì—¬ íƒ€ê²Ÿì˜ proxy labelsì„ soft labelë¡œ ì‚¬ìš©
        
        Args:
            query_embeddings: [batch_size, emb_dim] ì¿¼ë¦¬ ì„ë² ë”©
            user_ids: [batch_size] ì‚¬ìš©ì ID
            predicted_scores: [batch_size, num_items] ë˜ëŠ” [batch_size, num_candidates] ì˜ˆì¸¡ ì ìˆ˜
            
        Returns:
            ndcg_rewards: [batch_size] Proxy label ê¸°ë°˜ NDCG ë¦¬ì›Œë“œ
        """
        batch_size = len(user_ids)
        ndcg_rewards = torch.zeros(batch_size, device=self.device)
        
        # íƒ€ê²Ÿ ì•„ì´í…œ ID ê°€ì ¸ì˜¤ê¸°
        target_item_ids = torch.tensor(
            [self.uid_2_target[uid] for uid in user_ids],
            device=self.device
        )  # [batch_size]
        
        if self.use_full_item_pool:
            # ì „ì²´ ì•„ì´í…œ í’€ ì‚¬ìš©
            num_items = len(self.item_embeddings)
            
            for i in range(batch_size):
                target_id = target_item_ids[i].item()
                
                # # íƒ€ê²Ÿ ì•„ì´í…œì˜ ë¯¸ë¦¬ ê³„ì‚°ëœ proxy labels ê°€ì ¸ì˜¤ê¸°
                # if target_id not in self.item_proxy_labels:
                #     # Proxy labelsê°€ ì—†ìœ¼ë©´ íƒ€ê²Ÿë§Œ 1.0
                #     proxy_ids = torch.tensor([], dtype=torch.long, device=self.device)
                #     proxy_sims = torch.tensor([], dtype=torch.float32, device=self.device)
                # else:
                proxy_ids, proxy_sims = self.item_proxy_labels[target_id]
                
                # Relevance scores ìƒì„±: íƒ€ê²Ÿ ìì‹ ì€ 1.0, proxyëŠ” ìœ ì‚¬ë„ ë¹„ë¡€
                relevance_scores = torch.zeros(num_items, device=self.device)
                relevance_scores[target_id] = 1.0  # íƒ€ê²Ÿ ìì‹ 
                relevance_scores[proxy_ids] = proxy_sims  # Proxy labels
                
                # ì˜ˆì¸¡ ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ Top-K ì¶”ì¶œ
                pred_scores = predicted_scores[i]  # [num_items]
                top_k_pred_scores, top_k_pred_indices = torch.topk(pred_scores, k=min(self.k, len(pred_scores)))
                
                # Top-K ì˜ˆì¸¡ ê²°ê³¼ì—ì„œ relevance ì¶”ì¶œ
                predicted_relevance = relevance_scores[top_k_pred_indices]  # [k]
                
                # DCG ê³„ì‚°
                dcg = calculate_dcg(predicted_relevance.unsqueeze(0), k=self.k)[0]
                
                # IDCG ê³„ì‚° (ì´ìƒì ì¸ ê²½ìš°: relevanceê°€ ë†’ì€ ìˆœì„œëŒ€ë¡œ ì •ë ¬)
                ideal_relevance, _ = torch.sort(relevance_scores, descending=True)
                ideal_relevance = ideal_relevance[:self.k]
                idcg = calculate_dcg(ideal_relevance.unsqueeze(0), k=self.k)[0]
                
                # NDCG ê³„ì‚°
                if idcg > 0:
                    ndcg_rewards[i] = dcg / (idcg + 1e-10)
                else:
                    ndcg_rewards[i] = 0.0
        else:
            # Candidate set ê¸°ë°˜ ê³„ì‚°
            for i in range(batch_size):
                target_id = target_item_ids[i].item()
                
                # Candidate set ê°€ì ¸ì˜¤ê¸°
                batch_candidate_tensor = self.candidate_tensor[user_ids[i]]  # [num_candidates]
                num_candidates = len(batch_candidate_tensor)
                
                # íƒ€ê²Ÿ ì•„ì´í…œì˜ ë¯¸ë¦¬ ê³„ì‚°ëœ proxy labels ê°€ì ¸ì˜¤ê¸°
                if target_id not in self.item_proxy_labels:
                    # Proxy labelsê°€ ì—†ìœ¼ë©´ íƒ€ê²Ÿë§Œ 1.0
                    proxy_ids = torch.tensor([], dtype=torch.long, device=self.device)
                    proxy_sims = torch.tensor([], dtype=torch.float32, device=self.device)
                else:
                    proxy_ids, proxy_sims = self.item_proxy_labels[target_id]
                
                # Candidate set ë‚´ì—ì„œ relevance scores ìƒì„±
                relevance_scores = torch.zeros(num_candidates, device=self.device)
                
                # íƒ€ê²Ÿ ì•„ì´í…œì´ candidate setì— ìˆëŠ” ìœ„ì¹˜ ì°¾ê¸° (ë³´í†µ index 0)
                target_mask = batch_candidate_tensor == target_id
                if target_mask.any():
                    target_idx_in_candidates = target_mask.nonzero(as_tuple=True)[0][0]
                    relevance_scores[target_idx_in_candidates] = 1.0
                
                # Proxy labelsë„ candidate setì— ìˆëŠ”ì§€ í™•ì¸í•˜ê³  relevance í• ë‹¹
                if len(proxy_ids) > 0:
                    for proxy_id, proxy_sim in zip(proxy_ids, proxy_sims):
                        proxy_mask = batch_candidate_tensor == proxy_id.item()
                        if proxy_mask.any():
                            proxy_idx_in_candidates = proxy_mask.nonzero(as_tuple=True)[0][0]
                            relevance_scores[proxy_idx_in_candidates] = proxy_sim.item()
                
                # ì˜ˆì¸¡ ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ Top-K ì¶”ì¶œ
                pred_scores = predicted_scores[i]  # [num_candidates]
                top_k_pred_scores, top_k_pred_indices = torch.topk(pred_scores, k=min(self.k, len(pred_scores)))
                
                # Top-K ì˜ˆì¸¡ ê²°ê³¼ì—ì„œ relevance ì¶”ì¶œ
                predicted_relevance = relevance_scores[top_k_pred_indices]  # [k]
                
                # DCG ê³„ì‚°
                dcg = calculate_dcg(predicted_relevance.unsqueeze(0), k=self.k)[0]
                
                # IDCG ê³„ì‚°
                ideal_relevance, _ = torch.sort(relevance_scores, descending=True)
                ideal_relevance = ideal_relevance[:self.k]
                idcg = calculate_dcg(ideal_relevance.unsqueeze(0), k=self.k)[0]
                
                # NDCG ê³„ì‚°
                if idcg > 0:
                    ndcg_rewards[i] = dcg / (idcg + 1e-10)
                else:
                    ndcg_rewards[i] = 0.0
        
        return ndcg_rewards
    
    def _compute_target_embedding_reward(
        self,
        query_embeddings: torch.Tensor,
        user_ids: torch.Tensor,
    ) -> torch.Tensor:
        """
        íƒ€ê²Ÿ ì„ë² ë”©ê³¼ì˜ ìœ ì‚¬ë„ ê¸°ë°˜ ë¦¬ì›Œë“œ ê³„ì‚°
        
        ìƒì„±ëœ í…ìŠ¤íŠ¸ê°€ íƒ€ê²Ÿ ì„ë² ë”©ê³¼ ìœ ì‚¬í• ìˆ˜ë¡, 
        ê·¸ë¦¬ê³  negative ì„ë² ë”©ë“¤ë³´ë‹¤ íƒ€ê²Ÿê³¼ ë” ìœ ì‚¬í• ìˆ˜ë¡ ë†’ì€ ë¦¬ì›Œë“œ
        
        Args:
            query_embeddings: [batch_size, emb_dim] ì¿¼ë¦¬ ì„ë² ë”©
            user_ids: [batch_size] ì‚¬ìš©ì ID
            
        Returns:
            rewards: [batch_size] íƒ€ê²Ÿ ì„ë² ë”© ìœ ì‚¬ë„ ë¦¬ì›Œë“œ
                    (íƒ€ê²Ÿ ìœ ì‚¬ë„ - negative í‰ê·  ìœ ì‚¬ë„)
        """
        # L2 ì •ê·œí™” (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ìœ„í•´)
        query_embeddings = torch.nn.functional.normalize(query_embeddings, p=2, dim=1)
        
        # 2. íƒ€ê²Ÿ ì„ë² ë”©ê³¼ì˜ ìœ ì‚¬ë„ ê³„ì‚°
        target_embs = self.target_embeddings[user_ids]  # [batch_size, emb_dim]
        target_embs = torch.nn.functional.normalize(target_embs, p=2, dim=1)
        
        target_similarities = (query_embeddings * target_embs).sum(dim=1)  # [batch_size]
        
        if self.use_full_item_pool:
            # ì „ì²´ ì•„ì´í…œ í’€ ì‚¬ìš©: íƒ€ê²Ÿì„ ì œì™¸í•œ ëª¨ë“  ì•„ì´í…œì„ negativeë¡œ ì‚¬ìš©
            # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„ ìœ„í•´ ìƒ˜í”Œë§í•˜ê±°ë‚˜, ì „ì²´ ì•„ì´í…œì˜ í‰ê·  ìœ ì‚¬ë„ë¥¼ ê³„ì‚°
            # ì—¬ê¸°ì„œëŠ” ì „ì²´ ì•„ì´í…œ ì„ë² ë”©ì˜ í‰ê·  ìœ ì‚¬ë„ë¥¼ ì‚¬ìš©
            all_item_embs = torch.nn.functional.normalize(self.item_embeddings, p=2, dim=1)  # [num_items, emb_dim]
            
            # íƒ€ê²Ÿ ì•„ì´í…œ ID
            target_item_ids = torch.tensor(
                [self.uid_2_target[uid] for uid in user_ids],
                device=self.device
            )  # [batch_size]
            
            # ì „ì²´ ì•„ì´í…œê³¼ì˜ ìœ ì‚¬ë„ ê³„ì‚°
            all_similarities = torch.mm(query_embeddings, all_item_embs.T)  # [batch_size, num_items]
            
            # íƒ€ê²Ÿì„ ì œì™¸í•œ í‰ê·  ìœ ì‚¬ë„ ê³„ì‚°
            batch_size = all_similarities.size(0)
            num_items = all_similarities.size(1)
            
            # íƒ€ê²Ÿ ë§ˆìŠ¤í¬ ìƒì„±
            mask = torch.ones(batch_size, num_items, device=self.device, dtype=torch.bool)
            mask[torch.arange(batch_size, device=self.device), target_item_ids] = False
            
            # íƒ€ê²Ÿì„ ì œì™¸í•œ negativeë“¤ì˜ í‰ê·  ìœ ì‚¬ë„
            negative_mean_similarities = all_similarities[mask].view(batch_size, -1).mean(dim=1)
            rewards = target_similarities - torch.clamp(negative_mean_similarities, min=0.0)
        else:
            # # 3. Negative ì„ë² ë”©ë“¤ê³¼ì˜ í‰ê·  ìœ ì‚¬ë„ ê³„ì‚° (ê¸°ì¡´ ë¡œì§)
            # batch_candidate_tensor = self.candidate_tensor[user_ids]  # [batch_size, k]
            # negative_ids = batch_candidate_tensor[:, 1:]  # [batch_size, k-1] (ì²« ë²ˆì§¸ëŠ” target ì œì™¸)
            
            # # ì „ì²´ íƒ€ê²Ÿ ì„ë² ë”©ê³¼ì˜ ìœ ì‚¬ë„ ê³„ì‚°
            # all_similarities = torch.mm(query_embeddings, self.target_embeddings.T)  # [batch_size, num_users]
            
            # # í‰ê·  ìœ ì‚¬ë„ ê³„ì‚°
            # negative_mean_similarities = all_similarities.mean(dim=1)  # [batch_size]
            rewards = target_similarities
                
        return rewards
    
    def _compute_infonce_reward(
        self,
        query_embeddings: torch.Tensor,
        user_ids: torch.Tensor,
    ) -> torch.Tensor:
        """
        InfoNCE (ëŒ€ì¡° í•™ìŠµ) ë¦¬ì›Œë“œ ê³„ì‚°
        
        InfoNCE lossë¥¼ rewardë¡œ ë³€í™˜:
        reward = log(exp(sim(q, pos)/tau) / (exp(sim(q, pos)/tau) + sum(exp(sim(q, neg_i)/tau))))
        
        íƒ€ê²Ÿê³¼ì˜ ìœ ì‚¬ë„ëŠ” ë†’ì´ê³ , negativeë“¤ê³¼ì˜ ìœ ì‚¬ë„ëŠ” ë‚®ì¶”ë„ë¡ ì¥ë ¤
        
        Args:
            query_embeddings: [batch_size, emb_dim] ì¿¼ë¦¬ ì„ë² ë”©
            user_ids: [batch_size] ì‚¬ìš©ì ID
            
        Returns:
            rewards: [batch_size] InfoNCE ë¦¬ì›Œë“œ (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)
        """
        # L2 ì •ê·œí™” (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ìœ„í•´)
        query_embeddings = torch.nn.functional.normalize(query_embeddings, p=2, dim=1)
        
        if self.use_full_item_pool:
            # ì „ì²´ ì•„ì´í…œ í’€ ì‚¬ìš©
            # íƒ€ê²Ÿ ì•„ì´í…œ ID
            target_ids = torch.tensor(
                [self.uid_2_target[uid] for uid in user_ids],
                device=self.device
            )  # [batch_size]
            
            # InfoNCEìš© ì„ë² ë”© ì‚¬ìš©
            target_embs = self.infonce_item_embeddings[target_ids]  # [batch_size, emb_dim]
            target_embs = torch.nn.functional.normalize(target_embs, p=2, dim=1)
            
            all_item_embs = torch.nn.functional.normalize(self.infonce_item_embeddings, p=2, dim=1)  # [num_items, emb_dim]
            
            # 1. íƒ€ê²Ÿê³¼ì˜ ìœ ì‚¬ë„ ê³„ì‚°
            pos_sim = (query_embeddings * target_embs).sum(dim=1)  # [batch_size]
            pos_sim = pos_sim / self.infonce_temperature
            
            # 2. ì „ì²´ ì•„ì´í…œë“¤ê³¼ì˜ ìœ ì‚¬ë„ ê³„ì‚° (íƒ€ê²Ÿ ì œì™¸)
            all_sims = torch.mm(query_embeddings, all_item_embs.T)  # [batch_size, num_items]
            all_sims = all_sims / self.infonce_temperature
            
            # 3. InfoNCE ê³„ì‚°
            # log(exp(pos_sim) / sum(exp(all_sims)))
            log_sum_exp = torch.logsumexp(all_sims, dim=1)  # [batch_size]
            infonce_rewards = pos_sim - log_sum_exp  # [batch_size]
        else:
            # Candidate tensor ê°€ì ¸ì˜¤ê¸° (ê¸°ì¡´ ë¡œì§)
            batch_candidate_tensor = self.candidate_tensor[user_ids]  # [batch_size, k]
            target_ids = batch_candidate_tensor[:, 0]  # [batch_size] - target
            negative_ids = batch_candidate_tensor[:, 1:]  # [batch_size, k-1] - negatives
            
            # InfoNCEìš© ì„ë² ë”© ì‚¬ìš©
            target_embs = self.infonce_item_embeddings[target_ids]  # [batch_size, emb_dim]
            target_embs = torch.nn.functional.normalize(target_embs, p=2, dim=1)
            
            negative_embs = self.infonce_item_embeddings[negative_ids]  # [batch_size, k-1, emb_dim]
            negative_embs = torch.nn.functional.normalize(negative_embs, p=2, dim=2)
            
            # 1. íƒ€ê²Ÿê³¼ì˜ ìœ ì‚¬ë„ ê³„ì‚°
            pos_sim = (query_embeddings * target_embs).sum(dim=1)  # [batch_size]
            pos_sim = pos_sim / self.infonce_temperature
            
            # 2. Negativeë“¤ê³¼ì˜ ìœ ì‚¬ë„ ê³„ì‚°
            neg_sims = torch.bmm(
                query_embeddings.unsqueeze(1),  # [batch_size, 1, emb_dim]
                negative_embs.transpose(1, 2)   # [batch_size, emb_dim, k-1]
            ).squeeze(1)  # [batch_size, k-1]
            neg_sims = neg_sims / self.infonce_temperature
            
            # 3. InfoNCE ê³„ì‚°
            # log(exp(pos_sim) / (exp(pos_sim) + sum(exp(neg_sims))))
            # = pos_sim - log(exp(pos_sim) + sum(exp(neg_sims)))
            # = pos_sim - logsumexp([pos_sim, neg_sims])
            
            all_sims = torch.cat([pos_sim.unsqueeze(1), neg_sims], dim=1)  # [batch_size, k]
            log_sum_exp = torch.logsumexp(all_sims, dim=1)  # [batch_size]
            
            infonce_rewards = pos_sim - log_sum_exp  # [batch_size]
        
        return infonce_rewards
    
    def __call__(
        self,
        generated_texts: List[str],
        user_ids: torch.Tensor,
        **kwargs
    ) -> torch.Tensor:
        """
        TRL í˜¸í™˜ ë¦¬ì›Œë“œ í•¨ìˆ˜
        
        Args:
            generated_texts: [batch_size] ìƒì„±ëœ í…ìŠ¤íŠ¸
            user_ids: [batch_size] ì‚¬ìš©ì ID (required)
            **kwargs: ì¶”ê°€ íŒŒë¼ë¯¸í„° (targets, histories, trainer_state ë“±)
        
        Returns:
            rewards: [batch_size] ë¦¬ì›Œë“œ ê°’ 
            
            If proxy_label_reward=True:
                rewards = base_reward + proxy_label_coef Ã— proxy_label_ndcg
                (íƒ€ê²Ÿê³¼ ìœ ì‚¬í•œ ì•„ì´í…œë“¤ë„ ë¶€ë¶„ì ìœ¼ë¡œ ì •ë‹µìœ¼ë¡œ ì·¨ê¸‰)
            
            If novelty_reward=True and novelty_annealing=False:
                rewards = novelty_coef Ã— (NDCG Ã— popularity_weight)
                (ì¸ê¸° ì—†ëŠ” ì•„ì´í…œì„ ë†’ì€ rankë¡œ ì˜ˆì¸¡í• ìˆ˜ë¡ ë†’ì€ ë³´ìƒ)
            
            If novelty_reward=True and novelty_annealing=True:
                novelty_ratio = current_step / max_steps (0 â†’ 1 ì„ í˜• ì¦ê°€)
                rewards = (1 - novelty_ratio) * base_reward + novelty_ratio * novelty_coef * novelty_reward
            
            Else:
                rewards = base_reward (NDCG/Hit/MRR ë“±)
        """
        
        # ìƒì„±ëœ í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜ (í•œ ë²ˆë§Œ ìˆ˜í–‰)
        query_embeddings = self._encode_texts(generated_texts)
        
        # ê¸°ì¡´ rank ê¸°ë°˜ ë¦¬ì›Œë“œ ê³„ì‚°
        # rank ê³„ì‚° (target + negatives)
        ranks, _ = self._compute_similarity_scores(query_embeddings, user_ids, return_scores=False)
        
        # ê¸°ë³¸ ë¦¬ì›Œë“œ íƒ€ì…ì— ë”°ë¼ ê³„ì‚°
        if self.reward_type == "ndcg":
            base_rewards = calculate_ndcg_from_rank(ranks, k=self.k)
        elif self.reward_type == "hit":
            base_rewards = calculate_hit_from_rank(ranks, k=self.k)
        elif self.reward_type == "mrr":
            base_rewards = calculate_mrr_from_rank(ranks, k=self.k)
        elif self.reward_type == "mixed":
            ndcg = calculate_ndcg_from_rank(ranks, k=self.k)
            hit = calculate_hit_from_rank(ranks, k=self.k)
            base_rewards = 0.7 * ndcg + 0.3 * hit
        else:
            raise ValueError(f"Unknown reward_type: {self.reward_type}")
        # Proxy label reward ì‚¬ìš© ì—¬ë¶€ì— ë”°ë¼ ë¶„ê¸°
        if self.proxy_label_reward:
            # Proxy label ë¦¬ì›Œë“œ ì‚¬ìš© ì‹œ: ê¸°ì¡´ base_reward + proxy_label_reward
            # ì˜ˆì¸¡ ì ìˆ˜ë„ í•¨ê»˜ ê³„ì‚° í•„ìš”
            _, predicted_scores = self._compute_similarity_scores(query_embeddings, user_ids, return_scores=True)
            # 2. Proxy label NDCG ê³„ì‚°
            proxy_label_rewards = self._compute_proxy_label_ndcg(query_embeddings, user_ids, predicted_scores)
            
            # 3. ë‘ ë¦¬ì›Œë“œë¥¼ í•©ì‚°
            base_rewards = base_rewards + self.proxy_label_coef * proxy_label_rewards
        
        # Novelty reward ì‚¬ìš© ì—¬ë¶€ì— ë”°ë¼ ë¶„ê¸°
        if self.novelty_reward and self.item_popularity_weights is not None:
            # Novelty reward ê³„ì‚°
            if self.use_full_item_pool:
                # ì „ì²´ ì•„ì´í…œ í’€ ì‚¬ìš© ì‹œ: uid_2_targetì—ì„œ ì§ì ‘ ê°€ì ¸ì˜¤ê¸°
                target_item_ids = torch.tensor(
                    [self.uid_2_target[uid] for uid in user_ids],
                    device=self.device
                )  # [batch_size]
            else:
                # Negative pool ì‚¬ìš© ì‹œ: candidate_tensorì—ì„œ ê°€ì ¸ì˜¤ê¸°
                batch_candidate_tensor = self.candidate_tensor[user_ids]  # [batch_size, k]
                target_item_ids = batch_candidate_tensor[:, 0]  # [batch_size] - targetì€ í•­ìƒ ì²« ë²ˆì§¸
            
            # Target itemì˜ popularity weight
            item_weights = self.item_popularity_weights[target_item_ids]  # [batch_size]
            
            # Novelty = NDCG Ã— popularity_weight
            novelty_rewards = calculate_novelty_ndcg(
                ranks, 
                item_weights=item_weights,
                k=self.k,
            )
            
            # Novelty annealing ì ìš© ì—¬ë¶€ì— ë”°ë¼ ë¶„ê¸°
            if self.novelty_annealing:
                # trainer_stateì—ì„œ í˜„ì¬ step ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                trainer_state = kwargs.get("trainer_state", None)
                
                if trainer_state is not None and hasattr(trainer_state, "global_step"):
                    current_step = trainer_state.global_step
                    # Novelty ratio: 0 (ì´ˆë°˜) â†’ 1 (í›„ë°˜) ì„ í˜• ì¦ê°€
                    novelty_ratio = min(1.0, current_step / max(1, self.max_steps))
                else:
                    # trainer_stateê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ 0.5 ì‚¬ìš© (ì¤‘ê°„ê°’)
                    novelty_ratio = 0.5
                
                # ì„ í˜• ë³´ê°„: (1-ratio)*base + ratio*novelty, novelty_coef ì ìš©
                rewards = (1.0 - novelty_ratio) * base_rewards + novelty_ratio * self.novelty_coef * novelty_rewards
            else:
                # Annealing ì—†ì´ novelty rewardë§Œ ì‚¬ìš©, novelty_coef ì ìš©
                rewards = self.novelty_coef * novelty_rewards
        else:
            # ê¸°ë³¸ ë¦¬ì›Œë“œ ì‚¬ìš©
            rewards = base_rewards
        
        # Target embedding ìœ ì‚¬ë„ ë¦¬ì›Œë“œ ì¶”ê°€
        if self.target_emb_reward and self.target_embeddings is not None:
            target_emb_rewards = self._compute_target_embedding_reward(query_embeddings, user_ids)
            rewards = rewards + self.target_emb_coef * target_emb_rewards
        
        # InfoNCE ë¦¬ì›Œë“œ ì¶”ê°€
        if self.infonce_reward and self.infonce_item_embeddings is not None:
            infonce_rewards = self._compute_infonce_reward(query_embeddings, user_ids)
            rewards = rewards + self.infonce_coef * infonce_rewards
        
        # ì •ê·œí™” (optional)
        if self.normalize and rewards.std() > 0:
            rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-8)
        
        return rewards
