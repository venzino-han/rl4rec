"""
Reward Functions for RL4Rec
NDCG ê¸°ë°˜ ë¦¬ì›Œë“œ ê³„ì‚° ë° TRL í†µí•©
"""

import json
import torch
import numpy as np
from typing import List, Dict, Optional, Tuple
import ray
from pathlib import Path
import argparse
import nltk
from nltk.corpus import stopwords
import re


def extract_query_from_tags(text: str, tag: str = "query") -> str:
    """
    í…ìŠ¤íŠ¸ì—ì„œ íŠ¹ì • XML-like íƒœê·¸ ë‚´ë¶€ì˜ ë‚´ìš©ì„ ì¶”ì¶œ
    íƒœê·¸ê°€ ì—†ìœ¼ë©´ ì›ë³¸ í…ìŠ¤íŠ¸ ë°˜í™˜
    
    Args:
        text: ì…ë ¥ í…ìŠ¤íŠ¸
        tag: ì¶”ì¶œí•  íƒœê·¸ ì´ë¦„ (default: "query")
        
    Returns:
        íƒœê·¸ ë‚´ë¶€ì˜ í…ìŠ¤íŠ¸, íƒœê·¸ê°€ ì—†ìœ¼ë©´ ì›ë³¸ í…ìŠ¤íŠ¸
        
    Example:
        >>> text = "<thinking>...</thinking><query>camping gear</query>"
        >>> extract_query_from_tags(text)
        "camping gear"
    """
    # ì •ê·œì‹ìœ¼ë¡œ íƒœê·¸ ë‚´ìš© ì¶”ì¶œ (ëŒ€ì†Œë¬¸ì ë¬´ì‹œ, ì¤„ë°”ê¿ˆ í¬í•¨)
    pattern = f"<{tag}>(.*?)</{tag}>"
    match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)
    
    if match:
        # íƒœê·¸ ë‚´ìš© ì¶”ì¶œ ë° ì•ë’¤ ê³µë°± ì œê±°
        return match.group(1).strip()
    else:
        # íƒœê·¸ê°€ ì—†ìœ¼ë©´ ì›ë³¸ í…ìŠ¤íŠ¸ ë°˜í™˜
        return text


def calculate_dcg(relevance_scores: torch.Tensor, k: Optional[int] = None) -> torch.Tensor:
    """
    DCG (Discounted Cumulative Gain) ê³„ì‚°
    
    Args:
        relevance_scores: [batch_size, num_items] ê´€ë ¨ì„± ì ìˆ˜
        k: top-k ê¹Œì§€ë§Œ ê³„ì‚° (Noneì´ë©´ ì „ì²´)
    
    Returns:
        DCG scores [batch_size]
    """
    if k is not None:
        relevance_scores = relevance_scores[:, :k]
    
    # DCG = sum(rel_i / log2(i + 2)) for i in range(k)
    positions = torch.arange(1, relevance_scores.shape[1] + 1, device=relevance_scores.device)
    discounts = torch.log2(positions + 1.0)
    dcg = (relevance_scores / discounts).sum(dim=1)
    
    return dcg


def calculate_ndcg_from_rank(ranks: torch.Tensor, k: int = 10) -> torch.Tensor:
    """
    Rank ê¸°ë°˜ NDCG ê³„ì‚°
    
    Args:
        ranks: [batch_size] GT ì•„ì´í…œì˜ rank (1-indexed)
        k: Top-K NDCG
    
    Returns:
        NDCG scores [batch_size]
    """
    # Rankê°€ kë³´ë‹¤ í¬ë©´ NDCG = 0
    ndcg_scores = torch.zeros_like(ranks, dtype=torch.float32)
    
    # Rankê°€ k ì´ë‚´ì¸ ê²½ìš°ë§Œ ê³„ì‚°
    valid_mask = ranks <= k
    valid_ranks = ranks[valid_mask]
    
    # DCG = 1 / log2(rank + 1)
    dcg = 1.0 / torch.log2(valid_ranks.float() + 1.0)
    
    # IDCG = 1 / log2(2) (ì´ìƒì ì¸ ê²½ìš°, rank=1)
    idcg = 1.0 / torch.log2(torch.tensor(2.0, device=ranks.device))
    
    # NDCG = DCG / IDCG
    ndcg_scores[valid_mask] = dcg / idcg
    
    return ndcg_scores


def calculate_hit_from_rank(ranks: torch.Tensor, k: int = 10) -> torch.Tensor:
    """
    Rank ê¸°ë°˜ Hit@K ê³„ì‚°
    
    Args:
        ranks: [batch_size] GT ì•„ì´í…œì˜ rank (1-indexed)
        k: Top-K
    
    Returns:
        Hit scores [batch_size]
    """
    # Rankê°€ k ì´ë‚´ë©´ 1, ì•„ë‹ˆë©´ 0
    hit_scores = (ranks <= k).float()
    return hit_scores


def calculate_mrr_from_rank(ranks: torch.Tensor, k: int = 10) -> torch.Tensor:
    """
    Rank ê¸°ë°˜ MRR (Mean Reciprocal Rank) ê³„ì‚°
    
    Args:
        ranks: [batch_size] GT ì•„ì´í…œì˜ rank (1-indexed)
        k: Top-K
    
    Returns:
        MRR scores [batch_size]
    """
    # Rankê°€ kë³´ë‹¤ í¬ë©´ MRR = 0
    mrr_scores = torch.zeros_like(ranks, dtype=torch.float32)
    
    # Rankê°€ k ì´ë‚´ì¸ ê²½ìš°ë§Œ ê³„ì‚°
    valid_mask = ranks <= k
    valid_ranks = ranks[valid_mask]
    
    # MRR = 1 / rank
    mrr_scores[valid_mask] = 1.0 / valid_ranks.float()
    
    return mrr_scores


def calculate_novelty_ndcg(
    ranks: torch.Tensor, 
    item_weights: torch.Tensor,
    k: int = 10, 
) -> torch.Tensor:
    """
    Popularity-based novelty score calculation
    
    NDCGì— ì•„ì´í…œ ì¸ê¸°ë„ ì—­ìˆ˜(item_weights)ë¥¼ ê³±í•˜ì—¬ novelty ê³„ì‚°
    ì¸ê¸° ì—†ëŠ” ì•„ì´í…œì„ ë†’ì€ rankë¡œ ì˜ˆì¸¡í• ìˆ˜ë¡ ë†’ì€ novelty
    
    Args:
        ranks: [batch_size] GT ì•„ì´í…œì˜ rank (1-indexed)
        item_weights: [batch_size] ê° íƒ€ê²Ÿ ì•„ì´í…œì˜ popularity weight (ì—­ìˆ˜)
        k: Top-K
        target_rank: (ì‚¬ìš© ì•ˆí•¨, backward compatibility)
    
    Returns:
        novelty scores [batch_size] = NDCG Ã— item_weights
    """
    # 1. Ranksë¡œë¶€í„° NDCG ê³„ì‚°
    ndcg_scores = calculate_ndcg_from_rank(ranks, k=k)
    
    # 2. NDCGì— popularity weight ê³±í•˜ê¸°
    # ì¸ê¸° ì—†ëŠ” ì•„ì´í…œ(ë†’ì€ weight)ì„ ì˜ ì˜ˆì¸¡í•˜ë©´ ë†’ì€ novelty
    novelty = ndcg_scores * item_weights
    
    return novelty


def calculate_ndcg(
    predicted_scores: torch.Tensor,
    target_items: List[int],
    history_items: List[List[int]],
    k: int = 10,
    use_negatives_only: bool = False,
) -> torch.Tensor:
    """
    NDCG (Normalized Discounted Cumulative Gain) ê³„ì‚°
    
    Args:
        predicted_scores: [batch_size, num_items] ë˜ëŠ” [batch_size, 1+num_negs] ì˜ˆì¸¡ ì ìˆ˜
        target_items: [batch_size] ì‹¤ì œ íƒ€ê²Ÿ ì•„ì´í…œ ID ë¦¬ìŠ¤íŠ¸
        history_items: [batch_size, *] ì‚¬ìš©ìë³„ íˆìŠ¤í† ë¦¬ ì•„ì´í…œ ID ë¦¬ìŠ¤íŠ¸
        k: Top-K NDCG (default: 10)
        use_negatives_only: Trueì´ë©´ target+negativesë§Œ ì‚¬ìš© (scores shape [batch_size, 1+num_negs])
    
    Returns:
        NDCG scores [batch_size]
    """
    batch_size = predicted_scores.shape[0]
    ndcg_scores = torch.zeros(batch_size, device=predicted_scores.device)
    
    if use_negatives_only:
        # Target + negativesë§Œ ê³ ë ¤í•˜ëŠ” ê²½ìš°
        # scores shape: [batch_size, 1 + num_negs]
        # targetì€ í•­ìƒ index 0
        for i in range(batch_size):
            scores = predicted_scores[i]  # [1 + num_negs]
            k_actual = min(k, len(scores))
            
            # Top-K ì¶”ì¶œ
            top_k_scores, top_k_indices = torch.topk(scores, k=k_actual)
            
            # Target (index 0)ì´ top-kì— ìˆëŠ”ì§€ í™•ì¸
            relevance = torch.zeros(k_actual, device=predicted_scores.device)
            target_positions = (top_k_indices == 0).nonzero(as_tuple=True)[0]
            
            if len(target_positions) > 0:
                position = target_positions[0].item()
                relevance[position] = 1.0
                
                # DCG ê³„ì‚°
                dcg = calculate_dcg(relevance.unsqueeze(0), k=k_actual)[0]
                
                # IDCG ê³„ì‚°
                ideal_relevance = torch.zeros(k_actual, device=predicted_scores.device)
                ideal_relevance[0] = 1.0
                idcg = calculate_dcg(ideal_relevance.unsqueeze(0), k=k_actual)[0]
                
                ndcg_scores[i] = dcg / (idcg + 1e-10)
            else:
                ndcg_scores[i] = 0.0
    else:
        # ì „ì²´ ì•„ì´í…œ ê³ ë ¤í•˜ëŠ” ê²½ìš° (ê¸°ì¡´ ë¡œì§)
        for i in range(batch_size):
            # 1. íˆìŠ¤í† ë¦¬ ì•„ì´í…œ ì œì™¸ (masking)
            scores = predicted_scores[i].clone()
            if history_items[i]:
                history_mask = torch.zeros_like(scores, dtype=torch.bool, device=scores.device)
                history_mask[history_items[i]] = True
                history_mask[target_items[i]] = False
                scores[history_mask] = -float('inf')
            
            # 2. Top-K ì•„ì´í…œ ì¶”ì¶œ
            top_k_scores, top_k_indices = torch.topk(scores, k=min(k, len(scores)))
            
            # 3. Relevance ê³„ì‚° (target itemì´ top-kì— ìˆìœ¼ë©´ í•´ë‹¹ ìœ„ì¹˜ì— 1, ì—†ìœ¼ë©´ 0)
            relevance = torch.zeros(k, device=predicted_scores.device)
            target_item = target_items[i]
            
            # Top-kì—ì„œ target itemì˜ ìœ„ì¹˜ ì°¾ê¸°
            target_positions = (top_k_indices == target_item).nonzero(as_tuple=True)[0]
            if len(target_positions) > 0:
                position = target_positions[0].item()
                relevance[position] = 1.0
            
            # 4. DCG ê³„ì‚°
            if relevance.sum() > 0:
                dcg = calculate_dcg(relevance.unsqueeze(0), k=k)[0]
                
                # 5. IDCG (Ideal DCG) ê³„ì‚° - ì´ìƒì ì¸ ê²½ìš° (targetì´ 1ìœ„)
                ideal_relevance = torch.zeros(k, device=predicted_scores.device)
                ideal_relevance[0] = 1.0
                idcg = calculate_dcg(ideal_relevance.unsqueeze(0), k=k)[0]
                
                # 6. NDCG = DCG / IDCG
                ndcg_scores[i] = dcg / (idcg + 1e-10)
            else:
                # Targetì´ top-kì— ì—†ìœ¼ë©´ NDCG = 0
                ndcg_scores[i] = 0.0
    
    return ndcg_scores


def calculate_hit_rate(
    predicted_scores: torch.Tensor,
    target_items: List[int],
    history_items: List[List[int]],
    k: int = 10,
    use_negatives_only: bool = False,
) -> torch.Tensor:
    """
    Hit@K ê³„ì‚° (targetì´ top-kì— ìˆìœ¼ë©´ 1, ì—†ìœ¼ë©´ 0)
    
    Args:
        predicted_scores: [batch_size, num_items] ë˜ëŠ” [batch_size, 1+num_negs]
        target_items: [batch_size]
        history_items: [batch_size, *]
        k: Top-K
        use_negatives_only: Trueì´ë©´ target+negativesë§Œ ì‚¬ìš©
    
    Returns:
        Hit scores [batch_size]
    """
    batch_size = predicted_scores.shape[0]
    hit_scores = torch.zeros(batch_size, device=predicted_scores.device)
    
    if use_negatives_only:
        # Target + negativesë§Œ ê³ ë ¤
        for i in range(batch_size):
            scores = predicted_scores[i]
            k_actual = min(k, len(scores))
            
            # Top-K ì¶”ì¶œ
            _, top_k_indices = torch.topk(scores, k=k_actual)
            
            # Target (index 0)ì´ top-kì— ìˆëŠ”ì§€ í™•ì¸
            if 0 in top_k_indices:
                hit_scores[i] = 1.0
    else:
        # ì „ì²´ ì•„ì´í…œ ê³ ë ¤ (ê¸°ì¡´ ë¡œì§)
        for i in range(batch_size):
            # íˆìŠ¤í† ë¦¬ ì•„ì´í…œ ì œì™¸
            scores = predicted_scores[i].clone()
            if history_items[i]:
                history_mask = torch.zeros_like(scores, dtype=torch.bool, device=scores.device)
                history_mask[history_items[i]] = True
                history_mask[target_items[i]] = False
                scores[history_mask] = -float('inf')
            
            # Top-K ì¶”ì¶œ
            _, top_k_indices = torch.topk(scores, k=min(k, len(scores)))
            
            # Targetì´ top-kì— ìˆëŠ”ì§€ í™•ì¸
            if target_items[i] in top_k_indices:
                hit_scores[i] = 1.0
    
    return hit_scores


def calculate_mrr(
    predicted_scores: torch.Tensor,
    target_items: List[int],
    history_items: List[List[int]],
    k: int = 10,
    use_negatives_only: bool = False,
) -> torch.Tensor:
    """
    MRR (Mean Reciprocal Rank) ê³„ì‚°
    
    Args:
        predicted_scores: [batch_size, num_items] ë˜ëŠ” [batch_size, 1+num_negs]
        target_items: [batch_size]
        history_items: [batch_size, *]
        k: Top-K
        use_negatives_only: Trueì´ë©´ target+negativesë§Œ ì‚¬ìš©
    
    Returns:
        MRR scores [batch_size]
    """
    batch_size = predicted_scores.shape[0]
    mrr_scores = torch.zeros(batch_size, device=predicted_scores.device)
    
    if use_negatives_only:
        # Target + negativesë§Œ ê³ ë ¤
        for i in range(batch_size):
            scores = predicted_scores[i]
            k_actual = min(k, len(scores))
            
            # Top-K ì¶”ì¶œ
            _, top_k_indices = torch.topk(scores, k=k_actual)
            
            # Target (index 0)ì˜ rank ì°¾ê¸°
            target_positions = (top_k_indices == 0).nonzero(as_tuple=True)[0]
            if len(target_positions) > 0:
                rank = target_positions[0].item() + 1  # 1-indexed rank
                mrr_scores[i] = 1.0 / rank
    else:
        # ì „ì²´ ì•„ì´í…œ ê³ ë ¤ (ê¸°ì¡´ ë¡œì§)
        for i in range(batch_size):
            # íˆìŠ¤í† ë¦¬ ì•„ì´í…œ ì œì™¸
            scores = predicted_scores[i].clone()
            if history_items[i]:
                history_mask = torch.zeros_like(scores, dtype=torch.bool)
                history_mask[history_items[i]] = True
                scores[history_mask] = -float('inf')
            
            # Top-K ì¶”ì¶œ
            _, top_k_indices = torch.topk(scores, k=min(k, len(scores)))
            
            # Targetì˜ rank ì°¾ê¸°
            target_positions = (top_k_indices == target_items[i]).nonzero(as_tuple=True)[0]
            if len(target_positions) > 0:
                rank = target_positions[0].item() + 1  # 1-indexed rank
                mrr_scores[i] = 1.0 / rank
    
    return mrr_scores


class RecRewardFrunction:
    """
    TRLê³¼ í˜¸í™˜ë˜ëŠ” ë¦¬ì›Œë“œ í•¨ìˆ˜ í´ë˜ìŠ¤
    Ray RetrievalServiceì™€ í†µí•©
    """
    
    def __init__(
        self,
        retrieval_service_name: str = "RetrievalService",
        namespace: str = "rl4rec",
        data_name: str = "beauty",
        reward_type: str = "ndcg",
        k: int = 10,
        normalize: bool = True,
        test_target: bool = False,
    ):
        """
        Args:
            retrieval_service_name: Ray actor ì´ë¦„
            namespace: Ray namespace
            data_name: ë°ì´í„°ì…‹ ì´ë¦„
            reward_type: ë¦¬ì›Œë“œ íƒ€ì… ('ndcg', 'hit', 'mrr', 'mixed')
            k: Top-K ê°’
            normalize: ë¦¬ì›Œë“œ ì •ê·œí™” ì—¬ë¶€
        """
        self.__name__ = "RecRewardFrunction"
        self.retrieval_service_name = retrieval_service_name
        self.namespace = namespace
        self.data_name = data_name
        self.reward_type = reward_type
        self.k = k
        self.normalize = normalize
        self.test_target = test_target

        # RetrievalService ì—°ê²°
        try:
            self.retrieval_service = ray.get_actor(
                retrieval_service_name,
                namespace=namespace
            )
            print(f"âœ“ Connected to {retrieval_service_name}")
        except ValueError as e:
            raise RuntimeError(
                f"Failed to connect to {retrieval_service_name}. "
                f"Make sure retrieval service is running."
            ) from e

        #load item metadata
        with open(f"data/{data_name}/meta_text_fix.json", "r") as f:
            self.item_metadata = json.load(f)
        self.item_metadata = {int(k): v["title"] + "\n" + v["brand"] + "\n" + v["category"] for k, v in self.item_metadata.items()}
    
    def __call__(
        self,
        generated_texts: List[str],
        targets: List[int],
        histories: List[List[int]],
        neg_items: Optional[List[List[int]]] = None,
        **kwargs
    ) -> torch.Tensor:
        """
        TRL í˜¸í™˜ ë¦¬ì›Œë“œ í•¨ìˆ˜
        <query> íƒœê·¸ê°€ ìˆëŠ” ê²½ìš° íƒœê·¸ ë‚´ë¶€ì˜ í…ìŠ¤íŠ¸ë§Œ ì‚¬ìš©
        
        Args:
            generated_texts: [batch_size] ìƒì„±ëœ í…ìŠ¤íŠ¸
            targets: [batch_size] íƒ€ê²Ÿ ì•„ì´í…œ ID
            histories: [batch_size, *] íˆìŠ¤í† ë¦¬ ì•„ì´í…œ ID
            neg_items: [batch_size, num_negs] Negative ì•„ì´í…œ ID (optional)
        
        Returns:
            rewards: [batch_size] ë¦¬ì›Œë“œ ê°’
        """
        # <query> íƒœê·¸ê°€ ìˆìœ¼ë©´ ì¶”ì¶œ, ì—†ìœ¼ë©´ ì›ë³¸ ì‚¬ìš©
        processed_texts = [extract_query_from_tags(text, tag="query") for text in generated_texts]
        
        # add target text to generated_texts
        if self.test_target:
            processed_texts = [self.item_metadata[target] + "\n" + processed_text for processed_text, target in zip(processed_texts, targets)]  

        # 1. RetrievalServiceë¥¼ í†µí•´ ìœ ì‚¬ë„ ì ìˆ˜ ê³„ì‚°
        use_negatives_only = neg_items is not None
        
        scores_ref = self.retrieval_service.calculate_reward.remote(
            processed_texts,
            data_name=self.data_name,
            targets=targets if use_negatives_only else None,
            neg_items=neg_items,
        )
        scores = ray.get(scores_ref)  # [batch_size, num_items] or [batch_size, 1+num_negs]
        
        # 2. ë¦¬ì›Œë“œ íƒ€ì…ì— ë”°ë¼ ê³„ì‚°
        if self.reward_type == "ndcg":
            rewards = calculate_ndcg(scores, targets, histories, k=self.k, use_negatives_only=use_negatives_only)
        elif self.reward_type == "hit":
            rewards = calculate_hit_rate(scores, targets, histories, k=self.k, use_negatives_only=use_negatives_only)
        elif self.reward_type == "mrr":
            rewards = calculate_mrr(scores, targets, histories, k=self.k, use_negatives_only=use_negatives_only)
        elif self.reward_type == "mixed":
            # NDCG + Hit@Kì˜ ê°€ì¤‘ í‰ê· 
            ndcg = calculate_ndcg(scores, targets, histories, k=self.k, use_negatives_only=use_negatives_only)
            hit = calculate_hit_rate(scores, targets, histories, k=self.k, use_negatives_only=use_negatives_only)
            rewards = 0.7 * ndcg + 0.3 * hit
        else:
            raise ValueError(f"Unknown reward_type: {self.reward_type}")
        
        # 3. ì •ê·œí™” (optional)
        if self.normalize and rewards.std() > 0:
            rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-8)
        
        return rewards
    
    def compute_metrics(
        self,
        prompts: List[str],
        completions: List[str],
        completion_ids: List[List[int]],
        targets: List[int],
        histories: List[List[int]],
        neg_items: Optional[List[List[int]]] = None,
    ) -> Dict[str, float]:
        """
        í‰ê°€ ë©”íŠ¸ë¦­ ê³„ì‚° (NDCG, Hit@K, MRR ëª¨ë‘)
        
        Args:
            neg_items: [batch_size, num_negs] Negative ì•„ì´í…œ ID (optional)
        
        Returns:
            ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬
        """
        # RetrievalServiceë¥¼ í†µí•´ ìœ ì‚¬ë„ ì ìˆ˜ ê³„ì‚°
        use_negatives_only = neg_items is not None
        
        scores_ref = self.retrieval_service.calculate_reward.remote(
            completions,
            data_name=self.data_name,
            targets=targets if use_negatives_only else None,
            neg_items=neg_items,
        )
        scores = ray.get(scores_ref)
        
        # ëª¨ë“  ë©”íŠ¸ë¦­ ê³„ì‚°
        ndcg = calculate_ndcg(scores, targets, histories, k=self.k, use_negatives_only=use_negatives_only)
        hit = calculate_hit_rate(scores, targets, histories, k=self.k, use_negatives_only=use_negatives_only)
        mrr = calculate_mrr(scores, targets, histories, k=self.k, use_negatives_only=use_negatives_only)
        
        return {
            f"ndcg@{self.k}": ndcg.mean().item(),
            f"hit@{self.k}": hit.mean().item(),
            f"mrr@{self.k}": mrr.mean().item(),
        }


# TRL PPOTrainerì™€ í˜¸í™˜ë˜ëŠ” ë˜í¼ í•¨ìˆ˜
def create_reward_function(
    retrieval_service_name: str = "RetrievalService",
    namespace: str = "rl4rec",
    data_name: str = "beauty",
    reward_type: str = "ndcg",
    k: int = 10,
) -> RecRewardFrunction:
    """
    TRL PPOTrainerì—ì„œ ì‚¬ìš©í•  ë¦¬ì›Œë“œ í•¨ìˆ˜ ìƒì„±
    
    Usage:
        reward_fn = create_reward_function(reward_type="ndcg", k=10)
        rewards = reward_fn(generated_texts, targets, histories)
    """
    return RecRewardFrunction(
        retrieval_service_name=retrieval_service_name,
        namespace=namespace,
        data_name=data_name,
        reward_type=reward_type,
        k=k,
    )


def load_negative_pool(data_name: str, data_dir: str = "data", k: int = 10) -> Dict[int, List[int]]:
    """
    negative.txt íŒŒì¼ì—ì„œ negative pool ë¡œë“œ
    
    íŒŒì¼ í˜•ì‹: ê° ë¼ì¸ì€ "user_id neg_item1 neg_item2 ... neg_itemN"
    
    Args:
        data_name: ë°ì´í„°ì…‹ ì´ë¦„ (e.g., "beauty")
        data_dir: ë°ì´í„° ë””ë ‰í† ë¦¬
    
    Returns:
        user_idë¥¼ í‚¤ë¡œ í•˜ëŠ” negative items ë¦¬ìŠ¤íŠ¸ ë”•ì…”ë„ˆë¦¬
    """
    negative_file = Path(data_dir) / data_name / "negative.txt"
    
    if not negative_file.exists():
        raise FileNotFoundError(f"Negative pool file not found: {negative_file}")
    
    print(f"ğŸ“¦ Loading negative pool from: {negative_file}")
    negative_pool = {}
    
    with open(negative_file, 'r') as f:
        for line in f:
            parts = line.strip().split()
            user_id = int(parts[0])
            neg_items = [int(item_id) for item_id in parts[1:]]
            #random sample k items
            # neg_items = neg_items[:k-1]
            # neg_items = np.random.choice(neg_items, size=k-1, replace=False).tolist()
            negative_pool[user_id] = neg_items
    
    print(f"âœ“ Loaded negative pool for {len(negative_pool)} users")
    if len(negative_pool) > 0:
        sample_user = next(iter(negative_pool))
        print(f"  Example: User {sample_user} has {len(negative_pool[sample_user])} negative items")
    
    return negative_pool


class SimilarHistoryItemMentionReward:
    """
    ìœ ì € êµ¬ë§¤ê¸°ë¡ ì¤‘ íƒ€ê²Ÿê³¼ ê°€ì¥ ìœ ì‚¬ë„ê°€ ë†’ì€ ì•„ì´í…œì˜ titleì„ ì–¸ê¸‰í•  ê²½ìš° ë³´ìƒ
    ì„ë² ë”© ìœ ì‚¬ë„ ê¸°ë°˜ìœ¼ë¡œ ê°€ì¥ ìœ ì‚¬í•œ ì•„ì´í…œì„ ìºì‹±í•˜ê³ , titleì˜ ì²« 3ë‹¨ì–´ë¥¼ ì–¸ê¸‰í•˜ë©´ reward 1ì  ë¶€ì—¬
    """
    
    def __init__(
        self,
        data_name: str,
        item_embeddings: torch.Tensor,
        uid_2_target: Dict[int, int],
        device: str = "cuda",
        data_dir: str = "data",
        use_position_weight: bool = False,
        position_decay: float = 1.0,
        similarity_threshold: float = 0.7,
    ):
        """
        Args:
            data_name: ë°ì´í„°ì…‹ ì´ë¦„
            item_embeddings: ì‚¬ì „ ê³„ì‚°ëœ ì•„ì´í…œ ì„ë² ë”© [num_items+1, emb_dim]
            uid_2_target: ì‚¬ìš©ì ID to íƒ€ê²Ÿ ì•„ì´í…œ ID ë§¤í•‘
            device: ë””ë°”ì´ìŠ¤
            data_dir: ë°ì´í„° ë””ë ‰í† ë¦¬
            use_position_weight: ìœ„ì¹˜ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ì‚¬ìš© ì—¬ë¶€
                                Trueì´ë©´ í…ìŠ¤íŠ¸ ì•ìª½ì— ì–¸ê¸‰ë ìˆ˜ë¡ ë” ë†’ì€ ë³´ìƒ
            position_decay: ìœ„ì¹˜ ê¸°ë°˜ ê°ì†Œìœ¨ (0.0 ~ 1.0)
                          0.0 = ìœ„ì¹˜ ë¬´ê´€í•˜ê²Œ ë™ì¼ ë³´ìƒ
                          1.0 = í…ìŠ¤íŠ¸ ëì—ì„œëŠ” ë³´ìƒ 0
                          ì˜ˆ: 0.5ì´ë©´ í…ìŠ¤íŠ¸ ëì—ì„œ ë³´ìƒì´ ì ˆë°˜ìœ¼ë¡œ ê°ì†Œ
            similarity_threshold: ìœ ì‚¬ë„ ì—­ì¹˜ (0.0 ~ 1.0)
                                ì´ ê°’ ì´í•˜ì´ë©´ ë§ˆì§€ë§‰ ìƒí˜¸ì‘ìš© ì•„ì´í…œì„ ì„ íƒ
        """
        self.__name__ = "SimilarHistoryItemMentionReward"
        self.data_name = data_name
        self.item_embeddings = item_embeddings
        self.device = device
        self.use_position_weight = use_position_weight
        self.position_decay = position_decay
        self.similarity_threshold = similarity_threshold
        
        # ì•„ì´í…œ ë©”íƒ€ë°ì´í„° ë¡œë“œ (title, brand, category)
        with open(f"{data_dir}/{data_name}/meta_text_fix.json", "r") as f:
            self.item_metadata = json.load(f)
        
        print(f"âœ“ SimilarHistoryItemMentionReward initialization started")
        print(f"  - Loaded metadata for {len(self.item_metadata)} items")
        print(f"  - Similarity threshold: {self.similarity_threshold}")
        print(f"    â†’ If max similarity < threshold, use last interacted item")
        if self.use_position_weight:
            print(f"  - Position-based weighting: ENABLED (decay={self.position_decay})")
            print(f"    â†’ Earlier mentions get higher rewards")
        else:
            print(f"  - Position-based weighting: DISABLED")
        
        # ìºì‹œ: user_id -> (most_similar_history_item_id, max_similarity)
        self.similarity_cache = {}
        
        # ì „ì²´ ë°ì´í„°ì— ëŒ€í•´ ë¯¸ë¦¬ ìœ ì‚¬í•œ ì•„ì´í…œ ê³„ì‚°
        print(f"  - Pre-computing most similar history items for all users...")
        self._precompute_similar_items(uid_2_target, data_name, data_dir)
        print(f"âœ“ Pre-computed similar items for {len(self.similarity_cache)} user-target pairs")
    
    def _precompute_similar_items(
        self,
        uid_2_target: Dict[int, int],
        data_name: str,
        data_dir: str
    ):
        """
        ì „ì²´ ë°ì´í„°ì— ëŒ€í•´ íƒ€ê²Ÿê³¼ ê°€ì¥ ìœ ì‚¬í•œ íˆìŠ¤í† ë¦¬ ì•„ì´í…œì„ ë¯¸ë¦¬ ê³„ì‚°
        ìœ ì‚¬ë„ê°€ ì—­ì¹˜ ì´í•˜ì´ë©´ ë§ˆì§€ë§‰ ìƒí˜¸ì‘ìš© ì•„ì´í…œì„ ì„ íƒ
        
        Args:
            uid_2_target: ì‚¬ìš©ì ID to íƒ€ê²Ÿ ì•„ì´í…œ ID ë§¤í•‘
            data_name: ë°ì´í„°ì…‹ ì´ë¦„
            data_dir: ë°ì´í„° ë””ë ‰í† ë¦¬
        """
        # sequential_data.txtì—ì„œ íˆìŠ¤í† ë¦¬ ì •ë³´ ë¡œë“œ
        sequential_file = f"{data_dir}/{data_name}/sequential_data.txt"
        
        # ì •ê·œí™”ëœ ì„ë² ë”© ë¯¸ë¦¬ ê³„ì‚° (ì „ì²´ ì•„ì´í…œ)
        normalized_embeddings = torch.nn.functional.normalize(self.item_embeddings, p=2, dim=1)
        
        fallback_count = 0  # ì—­ì¹˜ ë¯¸ë§Œìœ¼ë¡œ ë§ˆì§€ë§‰ ì•„ì´í…œ ì‚¬ìš©í•œ íšŸìˆ˜
        
        with open(sequential_file, 'r') as f:
            for line in f:
                parts = [int(p) for p in line.strip().split()]
                user_id = parts[0]
                history = parts[1:-3]  # Train setì˜ history
                target_id = parts[-3]   # Train setì˜ target
                
                # uid_2_targetì— í•´ë‹¹í•˜ëŠ” ì‚¬ìš©ìë§Œ ì²˜ë¦¬
                if user_id not in uid_2_target:
                    continue
                
                # íˆìŠ¤í† ë¦¬ê°€ ë¹„ì–´ìˆìœ¼ë©´ ìŠ¤í‚µ
                if len(history) == 0:
                    continue
                
                # íƒ€ê²Ÿ ì„ë² ë”© (ì •ê·œí™”ë¨)
                target_emb = normalized_embeddings[target_id]  # [emb_dim]
                
                # íˆìŠ¤í† ë¦¬ ì„ë² ë”© (ì •ê·œí™”ë¨)
                history_ids = torch.tensor(history, dtype=torch.long, device=self.device)
                history_embs = normalized_embeddings[history_ids]  # [history_len, emb_dim]
                
                # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
                similarities = torch.mm(target_emb.unsqueeze(0), history_embs.T).squeeze(0)  # [history_len]
                
                # ê°€ì¥ ìœ ì‚¬í•œ ì•„ì´í…œ ì°¾ê¸°
                max_similarity = similarities.max().item()
                most_similar_idx = similarities.argmax().item()
                
                # ìœ ì‚¬ë„ê°€ ì—­ì¹˜ ì´í•˜ì´ë©´ ë§ˆì§€ë§‰ ìƒí˜¸ì‘ìš© ì•„ì´í…œ ì„ íƒ
                if max_similarity < self.similarity_threshold:
                    selected_item_id = history[-1]  # ë§ˆì§€ë§‰ ì•„ì´í…œ
                    fallback_count += 1
                else:
                    selected_item_id = history[most_similar_idx]
                
                # ìºì‹œì— ì €ì¥ (ì•„ì´í…œ IDì™€ ìµœëŒ€ ìœ ì‚¬ë„)
                self.similarity_cache[user_id] = (selected_item_id, max_similarity)
        
        # í†µê³„ ì¶œë ¥
        total_users = len(self.similarity_cache)
        if total_users > 0:
            fallback_ratio = (fallback_count / total_users) * 100
            print(f"  - Fallback to last item: {fallback_count}/{total_users} ({fallback_ratio:.1f}%)")

    
    def _get_most_similar_history_item(
        self,
        user_id: int,
    ) -> int:
        """
        íˆìŠ¤í† ë¦¬ ì¤‘ íƒ€ê²Ÿê³¼ ê°€ì¥ ìœ ì‚¬í•œ ì•„ì´í…œ ì°¾ê¸° (ìºì‹œì—ì„œ ê°€ì ¸ì˜¤ê±°ë‚˜ ì‹¤ì‹œê°„ ê³„ì‚°)
        ìœ ì‚¬ë„ê°€ ì—­ì¹˜ ì´í•˜ì´ë©´ ë§ˆì§€ë§‰ ìƒí˜¸ì‘ìš© ì•„ì´í…œ ë°˜í™˜
        
        Args:
            user_id: ì‚¬ìš©ì ID
            
        Returns:
            selected_item_id: ì„ íƒëœ íˆìŠ¤í† ë¦¬ ì•„ì´í…œ ID
                            (ìœ ì‚¬ë„ ì—­ì¹˜ ì´ìƒ: ê°€ì¥ ìœ ì‚¬í•œ ì•„ì´í…œ,
                             ìœ ì‚¬ë„ ì—­ì¹˜ ë¯¸ë§Œ: ë§ˆì§€ë§‰ ìƒí˜¸ì‘ìš© ì•„ì´í…œ)
        """        
        selected_item_id, _ = self.similarity_cache[user_id]
        return selected_item_id
    
    def _get_first_three_words(self, title: str) -> str:
        """
        Titleì˜ ì²« 3ë‹¨ì–´ ì¶”ì¶œ
        
        Args:
            title: ì•„ì´í…œ title
            
        Returns:
            first_three_words: ì²« 3ë‹¨ì–´ë¥¼ ê³µë°±ìœ¼ë¡œ ì—°ê²°í•œ ë¬¸ìì—´ (ì†Œë¬¸ì)
        """
        words = title.strip().split()
        first_three = " ".join(words[:3])
        return first_three.lower()
    
    def _calculate_position_weight(self, position: int, text_length: int) -> float:
        """
        ìœ„ì¹˜ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ê³„ì‚°
        
        Args:
            position: ì–¸ê¸‰ëœ ìœ„ì¹˜ (ë¬¸ì ì¸ë±ìŠ¤)
            text_length: ì „ì²´ í…ìŠ¤íŠ¸ ê¸¸ì´
            
        Returns:
            weight: ìœ„ì¹˜ ê¸°ë°˜ ê°€ì¤‘ì¹˜ (0.0 ~ 1.0)
                   ì•ìª½ì¼ìˆ˜ë¡ 1.0ì— ê°€ê¹ê³ , ë’¤ìª½ì¼ìˆ˜ë¡ ê°ì†Œ
        """
        if text_length == 0:
            return 1.0
        
        # ìƒëŒ€ì  ìœ„ì¹˜ ê³„ì‚° (0.0 = ë§¨ ì•, 1.0 = ë§¨ ë’¤)
        position_ratio = position / text_length
        
        # ê°€ì¤‘ì¹˜ ê³„ì‚°: 1.0 - (position_ratio * decay)
        # decay=0.0 â†’ ìœ„ì¹˜ ë¬´ê´€í•˜ê²Œ 1.0
        # decay=1.0 â†’ ë§¨ ë’¤ì—ì„œëŠ” 0.0
        # decay=0.5 â†’ ë§¨ ë’¤ì—ì„œëŠ” 0.5
        weight = 1.0 - (position_ratio * self.position_decay)
        
        return max(0.0, weight)  # ìµœì†Œê°’ 0.0 ë³´ì¥
    
    def __call__(
        self,
        generated_texts: List[str],
        targets: List[int],
        histories: List[List[int]],
        user_ids: List[int],
        **kwargs
    ) -> List[float]:
        """
        ìƒì„±ëœ í…ìŠ¤íŠ¸ì—ì„œ ìœ ì‚¬í•œ íˆìŠ¤í† ë¦¬ ì•„ì´í…œì˜ title ì–¸ê¸‰ ì—¬ë¶€ë¥¼ í™•ì¸í•˜ì—¬ ë³´ìƒ
        <query> íƒœê·¸ê°€ ìˆëŠ” ê²½ìš° íƒœê·¸ ë‚´ë¶€ì˜ í…ìŠ¤íŠ¸ë§Œ ê²€ì‚¬
        
        Args:
            generated_texts: [batch_size] ìƒì„±ëœ í…ìŠ¤íŠ¸
            targets: [batch_size] íƒ€ê²Ÿ ì•„ì´í…œ ID
            histories: [batch_size, *] íˆìŠ¤í† ë¦¬ ì•„ì´í…œ ID ë¦¬ìŠ¤íŠ¸
            user_ids: [batch_size] ì‚¬ìš©ì ID
            
        Returns:
            rewards: [batch_size] ë³´ìƒ ê°’
                    - use_position_weight=False: 0 ë˜ëŠ” 1.0
                    - use_position_weight=True: 0 ~ 1.0 (ìœ„ì¹˜ì— ë”°ë¼ ê°€ì¤‘)
        """
        rewards = []
        
        for gen_text, target_id, history_ids, user_id in zip(generated_texts, targets, histories, user_ids):
            reward = 0.0
            
            # <query> íƒœê·¸ê°€ ìˆìœ¼ë©´ ì¶”ì¶œ, ì—†ìœ¼ë©´ ì›ë³¸ ì‚¬ìš©
            processed_text = extract_query_from_tags(gen_text, tag="query")
            
            # ê°€ì¥ ìœ ì‚¬í•œ íˆìŠ¤í† ë¦¬ ì•„ì´í…œ ì°¾ê¸°
            most_similar_item_id = self._get_most_similar_history_item(user_id)
            
            # í•´ë‹¹ ì•„ì´í…œì˜ title ê°€ì ¸ì˜¤ê¸°
            if str(most_similar_item_id) in self.item_metadata:
                item_title = self.item_metadata[str(most_similar_item_id)]["title"]
                first_three_words = self._get_first_three_words(item_title)
                
                # ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ì— ì²« 3ë‹¨ì–´ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸ (ëŒ€ì†Œë¬¸ì ë¬´ì‹œ)
                processed_text_lower = processed_text.lower()
                if first_three_words in processed_text_lower:
                    if self.use_position_weight:
                        # ìœ„ì¹˜ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ì ìš©
                        position = processed_text_lower.find(first_three_words)
                        text_length = len(processed_text_lower)
                        weight = self._calculate_position_weight(position, text_length)
                        reward = 1.0 * weight
                    else:
                        # ìœ„ì¹˜ ë¬´ê´€í•˜ê²Œ 1.0ì 
                        reward = 1.0
            
            rewards.append(reward)
        
        return rewards


class MetadataMentionReward:
    """
    íƒ€ê²Ÿ ì•„ì´í…œì˜ ë©”íƒ€ë°ì´í„°(ë¸Œëœë“œ, ì¹´í…Œê³ ë¦¬ ë“±)ë¥¼ ì–¸ê¸‰í• ìˆ˜ë¡ ë³´ìƒì„ ì œê³µí•˜ëŠ” ë¦¬ì›Œë“œ í•¨ìˆ˜.
    
    íŠ¹ì§•:
    1. ë©”íƒ€ë°ì´í„°ì˜ ë‹¨ì–´ë“¤ì„ ë§ì´ ì–¸ê¸‰í• ìˆ˜ë¡ ë¦¬ì›Œë“œ ì¦ê°€
    2. íˆìŠ¤í† ë¦¬ ì•„ì´í…œì˜ ë©”íƒ€ë°ì´í„° ì¤‘ íƒ€ê²Ÿì— ì—†ëŠ” ë‹¨ì–´ë¥¼ ì–¸ê¸‰í•˜ë©´ íŒ¨ë„í‹° ì ìš©
    3. ìƒì„±ëœ í…ìŠ¤íŠ¸ì˜ ê¸¸ì´ì— ë°˜ë¹„ë¡€í•˜ë„ë¡ ë¦¬ì›Œë“œ ì •ê·œí™”
    4. ë¶ˆìš©ì–´(none, a, the ë“±)ëŠ” ë¦¬ì›Œë“œ ê³„ì‚°ì—ì„œ ì œì™¸
    """
    
    def __init__(
        self,
        data_name: str,
        device: str = "cuda",
        data_dir: str = "data",
        base_reward: float = 0.1,
        length_penalty_alpha: float = 0.5,
        min_length: int = 10,
        history_penalty_weight: float = 0.01,
    ):
        """
        Args:
            data_name: ë°ì´í„°ì…‹ ì´ë¦„
            device: ë””ë°”ì´ìŠ¤
            data_dir: ë°ì´í„° ë””ë ‰í† ë¦¬
            base_reward: ë©”íƒ€ë°ì´í„° ë‹¨ì–´ë‹¹ ê¸°ë³¸ ë³´ìƒ ì ìˆ˜
            length_penalty_alpha: ê¸¸ì´ íŒ¨ë„í‹° ê°•ë„ (0~1, ë†’ì„ìˆ˜ë¡ ê¸´ í…ìŠ¤íŠ¸ì— ë¶ˆë¦¬)
            min_length: ìµœì†Œ í…ìŠ¤íŠ¸ ê¸¸ì´ (ì´ë³´ë‹¤ ì§§ìœ¼ë©´ íŒ¨ë„í‹° ì—†ìŒ)
            history_penalty_weight: íˆìŠ¤í† ë¦¬ ë©”íƒ€ë°ì´í„° ì˜ëª» ì–¸ê¸‰ì‹œ íŒ¨ë„í‹° ê°€ì¤‘ì¹˜
        """
        self.__name__ = "MetadataMentionReward"
        self.data_name = data_name
        self.device = device
        self.base_reward = base_reward
        self.length_penalty_alpha = length_penalty_alpha
        self.min_length = min_length
        self.history_penalty_weight = history_penalty_weight
        # NLTK stopwords ë‹¤ìš´ë¡œë“œ ë° ë¡œë“œ (í•œë²ˆë§Œ ì‹¤í–‰)
        try:
            self.stopwords = set(stopwords.words('english'))
        except LookupError:
            print("Downloading NLTK stopwords...")
            nltk.download('stopwords', quiet=True)
            self.stopwords = set(stopwords.words('english'))
        
        # ì¶”ê°€ ë¶ˆìš©ì–´ (ë„ë©”ì¸ íŠ¹í™”)
        additional_stopwords = {'none', 'null', 'n/a', 'na'}
        self.stopwords.update(additional_stopwords)
        
        # ì•„ì´í…œ ë©”íƒ€ë°ì´í„° ë¡œë“œ
        with open(f"{data_dir}/{data_name}/meta_text_fix.json", "r") as f:
            item_metadata = json.load(f)
            item_metadata = {int(k): v for k, v in item_metadata.items()}
        
        # ê° ì•„ì´í…œì˜ ë©”íƒ€ë°ì´í„° ë‹¨ì–´ ì§‘í•©ì„ ë¯¸ë¦¬ ì¶”ì¶œ
        self.item_metadata_words = {}
        for item_id, meta in item_metadata.items():
            words = set()
            if "title" in meta and meta["title"]:
                title_words = self._extract_words(str(meta["title"]))
                words.update(title_words)
            
            # ë¸Œëœë“œ ì¶”ì¶œ
            if "brand" in meta and meta["brand"]:
                brand_words = self._extract_words(str(meta["brand"]))
                words.update(brand_words)
            
            # ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ
            # if "category" in meta and meta["category"]:
            #     category_words = self._extract_words(str(meta["category"]))
            #     words.update(category_words)
            
            self.item_metadata_words[item_id] = words
        
        print(f"âœ“ MetadataMentionReward initialized")
        print(f"  - Loaded metadata for {len(self.item_metadata_words)} items")
        print(f"  - Base reward: {self.base_reward}")
        print(f"  - Length penalty alpha: {self.length_penalty_alpha}")
        print(f"  - History penalty weight: {self.history_penalty_weight}")
        print(f"  - Stopwords excluded: {len(self.stopwords)} (NLTK English + custom)")
    
    def _extract_words(self, text: str) -> set:
        """
        í…ìŠ¤íŠ¸ì—ì„œ ë‹¨ì–´ë¥¼ ì¶”ì¶œí•˜ê³  ë¶ˆìš©ì–´ë¥¼ ì œê±°
        
        Args:
            text: ì…ë ¥ í…ìŠ¤íŠ¸
            
        Returns:
            ë¶ˆìš©ì–´ê°€ ì œê±°ëœ ë‹¨ì–´ ì§‘í•© (ì†Œë¬¸ì)
        """
        # ì•ŒíŒŒë²³ê³¼ ìˆ«ìë§Œ ë‚¨ê¸°ê³  ê³µë°±ìœ¼ë¡œ êµ¬ë¶„
        import re
        words = re.findall(r'\b[a-zA-Z0-9]+\b', text.lower())
        
        # ë¶ˆìš©ì–´ ì œê±° ë° ê¸¸ì´ê°€ 1ì¸ ë‹¨ì–´ ì œê±°
        filtered_words = {w for w in words if w not in self.stopwords and len(w) > 1}
        
        return filtered_words
    
    def __call__(
        self,
        generated_texts: List[str],
        targets: List[int],
        histories: List[List[int]],
        **kwargs
    ) -> List[float]:
        """
        ìƒì„±ëœ í…ìŠ¤íŠ¸ì—ì„œ íƒ€ê²Ÿ ì•„ì´í…œì˜ ë©”íƒ€ë°ì´í„° ì–¸ê¸‰ë„ë¥¼ í‰ê°€í•˜ì—¬ ë³´ìƒ
        íˆìŠ¤í† ë¦¬ ì•„ì´í…œì˜ ë©”íƒ€ë°ì´í„° ì¤‘ íƒ€ê²Ÿì— ì—†ëŠ” ë‹¨ì–´ ì–¸ê¸‰ì‹œ íŒ¨ë„í‹° ì ìš©
        <query> íƒœê·¸ê°€ ìˆëŠ” ê²½ìš° íƒœê·¸ ë‚´ë¶€ì˜ í…ìŠ¤íŠ¸ë§Œ ê²€ì‚¬
        
        Args:
            generated_texts: [batch_size] ìƒì„±ëœ í…ìŠ¤íŠ¸
            targets: [batch_size] íƒ€ê²Ÿ ì•„ì´í…œ ID
            **kwargs: histories (List[List[int]]): [batch_size, seq_len] íˆìŠ¤í† ë¦¬ ì•„ì´í…œ ID
            
        Returns:
            rewards: [batch_size] ë³´ìƒ ê°’
        """
        rewards = []
        
        for idx, (gen_text, target_id) in enumerate(zip(generated_texts, targets)):
            # <query> íƒœê·¸ê°€ ìˆìœ¼ë©´ ì¶”ì¶œ, ì—†ìœ¼ë©´ ì›ë³¸ ì‚¬ìš©
            processed_text = extract_query_from_tags(gen_text, tag="query")
            
            # íƒ€ê²Ÿ ì•„ì´í…œì˜ ë©”íƒ€ë°ì´í„° ë‹¨ì–´
            target_words = self.item_metadata_words.get(target_id, set())
            
            if not target_words:
                rewards.append(0.0)
                continue
            
            # ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ì—ì„œ ë‹¨ì–´ ì¶”ì¶œ
            gen_words = self._extract_words(processed_text)
            
            # ë©”íƒ€ë°ì´í„° ë‹¨ì–´ê°€ ìƒì„±ëœ í…ìŠ¤íŠ¸ì— ëª‡ ê°œë‚˜ ì–¸ê¸‰ë˜ì—ˆëŠ”ì§€ ì¹´ìš´íŠ¸
            matched_words = target_words.intersection(gen_words)
            match_count = len(matched_words)
            
            # ê¸°ë³¸ ë¦¬ì›Œë“œ ê³„ì‚° (ì–¸ê¸‰ëœ ë©”íƒ€ë°ì´í„° ë‹¨ì–´ ìˆ˜ì— ë¹„ë¡€)
            reward = match_count * self.base_reward
            
            # íˆìŠ¤í† ë¦¬ ì•„ì´í…œ ë©”íƒ€ë°ì´í„° íŒ¨ë„í‹° ê³„ì‚°
            if histories is not None and idx < len(histories):
                history_items = histories[idx]
                
                # íˆìŠ¤í† ë¦¬ ì•„ì´í…œë“¤ì˜ ë©”íƒ€ë°ì´í„° ë‹¨ì–´ ìˆ˜ì§‘
                history_words = set()
                for hist_id in history_items:
                    hist_words = self.item_metadata_words.get(hist_id, set())
                    history_words.update(hist_words)
                
                # íˆìŠ¤í† ë¦¬ì—ë§Œ ìˆê³  íƒ€ê²Ÿì—ëŠ” ì—†ëŠ” ë‹¨ì–´ (ì˜ëª» ì–¸ê¸‰í•˜ë©´ ì•ˆë˜ëŠ” ë‹¨ì–´)
                wrong_words = history_words - target_words
                
                # ìƒì„±ëœ í…ìŠ¤íŠ¸ì—ì„œ ì˜ëª»ëœ ë‹¨ì–´ê°€ ì–¸ê¸‰ëœ ê°œìˆ˜
                wrong_mention_count = len(wrong_words.intersection(gen_words))
                
                # íŒ¨ë„í‹° ì ìš©
                penalty = wrong_mention_count * self.history_penalty_weight
                reward = reward - penalty
                reward = max(reward, 0.0)
            
            # ê¸¸ì´ íŒ¨ë„í‹° ì ìš©: ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ ê¸¸ì´ ê¸°ì¤€
            text_length = len(processed_text.split())
            if text_length > self.min_length:
                # length_factor: í…ìŠ¤íŠ¸ê°€ ê¸¸ìˆ˜ë¡ ì‘ì•„ì§ (0~1)
                length_factor = 1.0 / (1.0 + self.length_penalty_alpha * (text_length - self.min_length) / self.min_length)
                reward = reward * length_factor
            
            rewards.append(reward)
        
        return rewards


class ItemPreferenceMentionReward:
    """
    íƒ€ê²Ÿ ì•„ì´í…œì˜ ë©”íƒ€ë°ì´í„°ì™€ ì•„ì´í…œ ê°„ ì„ í˜¸ë„ ì •ë³´ë¥¼ ê²°í•©í•˜ì—¬ ë³´ìƒì„ ì œê³µí•˜ëŠ” ë¦¬ì›Œë“œ í•¨ìˆ˜
    
    íŠ¹ì§•:
    1. data_processed/{data_name}_gemma-3-1b-it_item_item_preference.jsonì—ì„œ ì„ í˜¸ë„ ì •ë³´ ë¡œë“œ
    2. ê¸°ì¡´ ë©”íƒ€ë°ì´í„°(ë¸Œëœë“œ, ì¹´í…Œê³ ë¦¬ ë“±)ì™€ ì„ í˜¸ë„ í…ìŠ¤íŠ¸ë¥¼ ê²°í•©í•˜ì—¬ ë‹¨ì–´ ì§‘í•© ìƒì„±
    3. ì¿¼ë¦¬ì— í¬í•¨ëœ ê³µí†µ ë‹¨ì–´ê°€ ë§ì„ìˆ˜ë¡ ë” í° ë³´ìƒ ì œê³µ
    4. ë¶ˆìš©ì–´(stopwords) ìë™ ì œê±°
    """
    
    def __init__(
        self,
        data_name: str,
        device: str = "cuda",
        data_dir: str = "data",
        data_processed_dir: str = "data_processed",
        base_reward: float = 0.1,
        length_penalty_alpha: float = 0.5,
        min_length: int = 10,
    ):
        """
        Args:
            data_name: ë°ì´í„°ì…‹ ì´ë¦„
            device: ë””ë°”ì´ìŠ¤
            data_dir: ë©”íƒ€ë°ì´í„° ë””ë ‰í† ë¦¬
            data_processed_dir: ì•„ì´í…œ ì„ í˜¸ë„ ë°ì´í„° ë””ë ‰í† ë¦¬
            base_reward: ë‹¨ì–´ë‹¹ ê¸°ë³¸ ë³´ìƒ ì ìˆ˜
            length_penalty_alpha: ê¸¸ì´ íŒ¨ë„í‹° ê°•ë„ (0~1, ë†’ì„ìˆ˜ë¡ ê¸´ í…ìŠ¤íŠ¸ì— ë¶ˆë¦¬)
            min_length: ìµœì†Œ í…ìŠ¤íŠ¸ ê¸¸ì´ (ì´ë³´ë‹¤ ì§§ìœ¼ë©´ íŒ¨ë„í‹° ì—†ìŒ)
        """
        self.__name__ = "ItemPreferenceMentionReward"
        self.data_name = data_name
        self.device = device
        self.base_reward = base_reward
        self.length_penalty_alpha = length_penalty_alpha
        self.min_length = min_length
        
        # NLTK stopwords ë‹¤ìš´ë¡œë“œ ë° ë¡œë“œ
        try:
            self.stopwords = set(stopwords.words('english'))
        except LookupError:
            print("Downloading NLTK stopwords...")
            nltk.download('stopwords', quiet=True)
            self.stopwords = set(stopwords.words('english'))
        
        # ì¶”ê°€ ë¶ˆìš©ì–´ (ë„ë©”ì¸ íŠ¹í™”)
        additional_stopwords = {'none', 'null', 'n/a', 'na'}
        self.stopwords.update(additional_stopwords)
        
        # ì•„ì´í…œ ë©”íƒ€ë°ì´í„° ë¡œë“œ
        print(f"ğŸ“¦ Loading item metadata from {data_dir}/{data_name}/meta_text_fix.json")
        with open(f"{data_dir}/{data_name}/meta_text_fix.json", "r") as f:
            item_metadata = json.load(f)
            item_metadata = {int(k): v for k, v in item_metadata.items()}
        
        # ì•„ì´í…œ ì„ í˜¸ë„ ì •ë³´ ë¡œë“œ
        preference_file = f"{data_processed_dir}/{data_name}_gemma-3-1b-it_item_item_preference.json"
        print(f"ğŸ“¦ Loading item preference from {preference_file}")
        with open(preference_file, "r") as f:
            item_preference = json.load(f)
            item_preference = {int(k): v for k, v in item_preference.items()}
        
        # ê° ì•„ì´í…œì˜ ë©”íƒ€ë°ì´í„° + ì„ í˜¸ë„ ë‹¨ì–´ ì§‘í•©ì„ ë¯¸ë¦¬ ì¶”ì¶œ
        self.item_combined_words = {}
        for item_id, meta in item_metadata.items():
            words = set()
            
            # ë©”íƒ€ë°ì´í„°ì—ì„œ ë‹¨ì–´ ì¶”ì¶œ
            if "title" in meta and meta["title"]:
                title_words = self._extract_words(str(meta["title"]))
                words.update(title_words)
            
            if "brand" in meta and meta["brand"]:
                brand_words = self._extract_words(str(meta["brand"]))
                words.update(brand_words)
            
            # ì„ í˜¸ë„ ì •ë³´ì—ì„œ ë‹¨ì–´ ì¶”ì¶œ
            if item_id in item_preference:
                preference_words = self._extract_words(item_preference[item_id])
                words.update(preference_words)
            
            self.item_combined_words[item_id] = words
        
        print(f"âœ“ ItemPreferenceMentionReward initialized")
        print(f"  - Loaded metadata for {len(item_metadata)} items")
        print(f"  - Loaded preference for {len(item_preference)} items")
        print(f"  - Combined word sets created for {len(self.item_combined_words)} items")
        print(f"  - Base reward: {self.base_reward}")
        print(f"  - Length penalty alpha: {self.length_penalty_alpha}")
        print(f"  - Stopwords excluded: {len(self.stopwords)} (NLTK English + custom)")
        
        # í†µê³„ ì¶œë ¥
        if len(self.item_combined_words) > 0:
            word_counts = [len(words) for words in self.item_combined_words.values()]
            avg_words = sum(word_counts) / len(word_counts)
            max_words = max(word_counts)
            min_words = min(word_counts)
            print(f"  - Word set statistics: Min={min_words}, Max={max_words}, Avg={avg_words:.1f}")
    
    def _extract_words(self, text: str) -> set:
        """
        í…ìŠ¤íŠ¸ì—ì„œ ë‹¨ì–´ë¥¼ ì¶”ì¶œí•˜ê³  ë¶ˆìš©ì–´ë¥¼ ì œê±°
        
        Args:
            text: ì…ë ¥ í…ìŠ¤íŠ¸
            
        Returns:
            ë¶ˆìš©ì–´ê°€ ì œê±°ëœ ë‹¨ì–´ ì§‘í•© (ì†Œë¬¸ì)
        """
        # ì•ŒíŒŒë²³ê³¼ ìˆ«ìë§Œ ë‚¨ê¸°ê³  ê³µë°±ìœ¼ë¡œ êµ¬ë¶„
        words = re.findall(r'\b[a-zA-Z0-9]+\b', text.lower())
        
        # ë¶ˆìš©ì–´ ì œê±° ë° ê¸¸ì´ê°€ 1ì¸ ë‹¨ì–´ ì œê±°
        filtered_words = {w for w in words if w not in self.stopwords and len(w) > 1}
        
        return filtered_words
    
    def __call__(
        self,
        generated_texts: List[str],
        targets: List[int],
        **kwargs
    ) -> List[float]:
        """
        ìƒì„±ëœ í…ìŠ¤íŠ¸ì—ì„œ íƒ€ê²Ÿ ì•„ì´í…œì˜ ë©”íƒ€ë°ì´í„°+ì„ í˜¸ë„ ë‹¨ì–´ ì–¸ê¸‰ë„ë¥¼ í‰ê°€í•˜ì—¬ ë³´ìƒ
        <query> íƒœê·¸ê°€ ìˆëŠ” ê²½ìš° íƒœê·¸ ë‚´ë¶€ì˜ í…ìŠ¤íŠ¸ë§Œ ê²€ì‚¬
        
        Args:
            generated_texts: [batch_size] ìƒì„±ëœ í…ìŠ¤íŠ¸
            targets: [batch_size] íƒ€ê²Ÿ ì•„ì´í…œ ID
            
        Returns:
            rewards: [batch_size] ë³´ìƒ ê°’
        """
        rewards = []
        
        for gen_text, target_id in zip(generated_texts, targets):
            # íƒ€ê²Ÿ ì•„ì´í…œì˜ ë©”íƒ€ë°ì´í„°+ì„ í˜¸ë„ ë‹¨ì–´
            target_words = self.item_combined_words.get(target_id, set())
            
            if not target_words:
                rewards.append(0.0)
                continue
            
            # ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ì—ì„œ ë‹¨ì–´ ì¶”ì¶œ
            gen_words = self._extract_words(gen_text)
            
            # ê³µí†µ ë‹¨ì–´ê°€ ëª‡ ê°œë‚˜ ì–¸ê¸‰ë˜ì—ˆëŠ”ì§€ ì¹´ìš´íŠ¸
            matched_words = target_words.intersection(gen_words)
            match_count = len(matched_words)
            
            # ê¸°ë³¸ ë¦¬ì›Œë“œ ê³„ì‚° (ì–¸ê¸‰ëœ ë‹¨ì–´ ìˆ˜ì— ë¹„ë¡€)
            reward = match_count * self.base_reward
            
            # ê¸¸ì´ íŒ¨ë„í‹° ì ìš©: ìƒì„±ëœ í…ìŠ¤íŠ¸ ê¸¸ì´ ê¸°ì¤€
            text_length = len(gen_text.split())
            if text_length > self.min_length:
                # length_factor: í…ìŠ¤íŠ¸ê°€ ê¸¸ìˆ˜ë¡ ì‘ì•„ì§ (0~1)
                length_factor = 1.0 / (1.0 + self.length_penalty_alpha * (text_length - self.min_length) / self.min_length)
                reward = reward * length_factor
            
            rewards.append(reward)
        
        return rewards


class FormatComplianceReward:
    """
    ìƒì„±ëœ í…ìŠ¤íŠ¸ê°€ íŠ¹ì • XML-like í¬ë§·ì„ ì¤€ìˆ˜í•˜ëŠ”ì§€ í™•ì¸í•˜ëŠ” ë¦¬ì›Œë“œ í•¨ìˆ˜
    
    ìš”êµ¬ í¬ë§·:
    <thinking>...</thinking>
    <window>...</window>
    <items>...</items>
    <query>...</query>
    
    ê° íƒœê·¸ê°€ ì¡´ì¬í•˜ê³  ì˜¬ë°”ë¥´ê²Œ ì—´ë¦¬ê³  ë‹«íˆë©´ ë¶€ë¶„ ì ìˆ˜ ë¶€ì—¬
    """
    
    def __init__(
        self,
        required_tags: List[str] = None,
        reward_per_tag: float = 0.25,
        strict_order: bool = False,
        case_sensitive: bool = False,
    ):
        """
        Args:
            required_tags: í•„ìˆ˜ íƒœê·¸ ë¦¬ìŠ¤íŠ¸ (default: ["thinking", "window", "items", "query"])
            reward_per_tag: ê° íƒœê·¸ë‹¹ ë³´ìƒ ì ìˆ˜ (default: 0.25, 4ê°œ íƒœê·¸ * 0.25 = 1.0)
            strict_order: íƒœê·¸ ìˆœì„œë¥¼ ì—„ê²©í•˜ê²Œ ì²´í¬í• ì§€ ì—¬ë¶€ (default: False)
            case_sensitive: ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—¬ë¶€ (default: False)
        """
        self.__name__ = "FormatComplianceReward"
        
        if required_tags is None:
            self.required_tags = ["thinking", "window", "items", "query"]
        else:
            self.required_tags = required_tags
        
        self.reward_per_tag = reward_per_tag
        self.strict_order = strict_order
        self.case_sensitive = case_sensitive
        
        print(f"âœ“ FormatComplianceReward initialized")
        print(f"  - Required tags: {self.required_tags}")
        print(f"  - Reward per tag: {self.reward_per_tag}")
        print(f"  - Strict order: {self.strict_order}")
        print(f"  - Case sensitive: {self.case_sensitive}")
        print(f"  - Max reward: {len(self.required_tags) * self.reward_per_tag}")
    
    def _check_tag_exists(self, text: str, tag: str) -> bool:
        """
        íƒœê·¸ê°€ ì˜¬ë°”ë¥´ê²Œ ì—´ë¦¬ê³  ë‹«íˆëŠ”ì§€ í™•ì¸
        
        Args:
            text: ê²€ì‚¬í•  í…ìŠ¤íŠ¸
            tag: íƒœê·¸ ì´ë¦„ (ì˜ˆ: "thinking")
            
        Returns:
            True if both opening and closing tags exist, False otherwise
        """
        if not self.case_sensitive:
            text = text.lower()
            tag = tag.lower()
        
        open_tag = f"<{tag}>"
        close_tag = f"</{tag}>"
        
        return open_tag in text and close_tag in text
    
    def _check_tag_order(self, text: str) -> bool:
        """
        íƒœê·¸ê°€ ì˜¬ë°”ë¥¸ ìˆœì„œë¡œ ë‚˜íƒ€ë‚˜ëŠ”ì§€ í™•ì¸
        
        Args:
            text: ê²€ì‚¬í•  í…ìŠ¤íŠ¸
            
        Returns:
            True if tags appear in correct order, False otherwise
        """
        if not self.case_sensitive:
            text = text.lower()
        
        last_position = -1
        
        for tag in self.required_tags:
            open_tag = f"<{tag}>"
            if not self.case_sensitive:
                tag = tag.lower()
            
            position = text.find(open_tag)
            
            if position == -1:
                return False
            
            if position < last_position:
                return False
            
            last_position = position
        
        return True
    
    def __call__(
        self,
        generated_texts: List[str],
        **kwargs
    ) -> List[float]:
        """
        ìƒì„±ëœ í…ìŠ¤íŠ¸ì˜ í¬ë§· ì¤€ìˆ˜ë„ë¥¼ í‰ê°€í•˜ì—¬ ë³´ìƒ
        
        Args:
            generated_texts: [batch_size] ìƒì„±ëœ í…ìŠ¤íŠ¸
            
        Returns:
            rewards: [batch_size] ë³´ìƒ ê°’ (0.0 ~ max_reward)
        """
        rewards = []
        
        for gen_text in generated_texts:
            reward = 0.0
            
            # 1. ê° íƒœê·¸ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
            valid_tags = 0
            for tag in self.required_tags:
                if self._check_tag_exists(gen_text, tag):
                    valid_tags += 1
                    reward += self.reward_per_tag
            
            # 2. ì—„ê²©í•œ ìˆœì„œ ì²´í¬ (ì˜µì…˜)
            if self.strict_order and valid_tags > 0:
                if not self._check_tag_order(gen_text):
                    # ìˆœì„œê°€ í‹€ë¦¬ë©´ ë³´ìƒì„ ì ˆë°˜ìœ¼ë¡œ ê°ì†Œ
                    reward = reward * 0.5
            
            rewards.append(reward)
        
        return rewards


class LocalEmbeddingRewardFunction:
    """
    ë¡œì»¬ ì„ë² ë”© ê¸°ë°˜ ë¦¬ì›Œë“œ í•¨ìˆ˜
    DB ëŒ€ì‹  ìì²´ì ìœ¼ë¡œ negative item ì„ë² ë”©ì„ ê³„ì‚°í•˜ì—¬ NDCGë¥¼ rewardë¡œ í™œìš©
    """
    
    def __init__(
        self,
        args: argparse.Namespace,
        uid_2_target: Dict[int, int],
    ):
        """
        Args:
            uid_2_target: ì‚¬ìš©ì ID to íƒ€ê²Ÿ ì•„ì´í…œ ID ë§¤í•‘
            data_name: ë°ì´í„°ì…‹ ì´ë¦„
            k: Top-K ê°’
            reward_type: ë¦¬ì›Œë“œ íƒ€ì… ('ndcg', 'hit', 'mrr', 'mixed')
            emb_model_name: ì„ë² ë”© ëª¨ë¸ ì´ë¦„
            emb_type: ì„ë² ë”© íƒ€ì… (review_description, title, etc.)
            device: ë””ë°”ì´ìŠ¤
            emb_batch_size: ì„ë² ë”© ê³„ì‚° ë°°ì¹˜ í¬ê¸°
            data_dir: ë°ì´í„° ë””ë ‰í† ë¦¬
            normalize: ë¦¬ì›Œë“œ ì •ê·œí™” ì—¬ë¶€
            novelty_reward: Novelty ë¦¬ì›Œë“œ ì‚¬ìš© ì—¬ë¶€ (True/False)
                           Novelty = NDCG Ã— popularity_weight
            novelty_coef: Novelty ë¦¬ì›Œë“œ ê³„ìˆ˜ (default: 1.0)
            novelty_target_rank: (ì‚¬ìš© ì•ˆí•¨, backward compatibility)
            novelty_mode: (ì‚¬ìš© ì•ˆí•¨, backward compatibility)
            novelty_annealing: Novelty annealing ì‚¬ìš© ì—¬ë¶€
                              Trueì´ë©´ í•™ìŠµ ì§„í–‰ë„ì— ë”°ë¼ novelty ë¹„ìœ¨ì„ 0â†’1ë¡œ ì„ í˜• ì¦ê°€
                              Final reward = (1-ratio)*base + ratio*novelty_coef*novelty
            popularity_coef: Popularity ë¦¬ì›Œë“œ ê³„ìˆ˜ (0.0 = ì‚¬ìš© ì•ˆí•¨)
                            ì •ë‹µì¸ ê²½ìš°ì—ë§Œ popularity bonus ì¶”ê°€
            target_emb_reward: íƒ€ê²Ÿ ì„ë² ë”© ìœ ì‚¬ë„ ë¦¬ì›Œë“œ ì‚¬ìš© ì—¬ë¶€
            target_emb_coef: íƒ€ê²Ÿ ì„ë² ë”© ë¦¬ì›Œë“œ ê³„ìˆ˜
            infonce_reward: InfoNCE (ëŒ€ì¡° í•™ìŠµ) ë¦¬ì›Œë“œ ì‚¬ìš© ì—¬ë¶€
            infonce_coef: InfoNCE ë¦¬ì›Œë“œ ê³„ìˆ˜
            infonce_temperature: InfoNCE temperature íŒŒë¼ë¯¸í„° (default: 0.07)
            infonce_emb_type: InfoNCEìš© ì„ë² ë”© íƒ€ì… (Noneì´ë©´ emb_typeê³¼ ë™ì¼)
            proxy_label_reward: Proxy label ë¦¬ì›Œë“œ ì‚¬ìš© ì—¬ë¶€
                               Trueì´ë©´ íƒ€ê²Ÿê³¼ ìœ ì‚¬í•œ ìƒìœ„ proxy_kê°œ ì•„ì´í…œë„ ë¶€ë¶„ì ìœ¼ë¡œ ì •ë‹µìœ¼ë¡œ ì·¨ê¸‰
                               ê¸°ì¡´ base_rewardì— ì¶”ê°€ë¡œ ë”í•´ì§
            proxy_k: Proxy labelë¡œ ì‚¬ìš©í•  ìœ ì‚¬í•œ ì•„ì´í…œ ê°œìˆ˜
            proxy_label_coef: Proxy label ë¦¬ì›Œë“œ ê³„ìˆ˜
            proxy_label_cutoff: Proxy label ìœ ì‚¬ë„ ì—­ì¹˜ (default: 0.0)
                               ì´ ê°’ ë¯¸ë§Œì˜ ìœ ì‚¬ë„ë¥¼ ê°€ì§„ ì•„ì´í…œì€ proxy labelì—ì„œ ì œì™¸
                               ì˜ˆ: 0.5ë¡œ ì„¤ì •í•˜ë©´ ìœ ì‚¬ë„ 0.5 ë¯¸ë§Œ ì•„ì´í…œì€ í•„í„°ë§
            max_steps: ìµœëŒ€ í•™ìŠµ ìŠ¤í… ìˆ˜ (novelty annealing ê³„ì‚°ì— ì‚¬ìš©)
        """
        self.__name__ = "LocalEmbeddingRewardFunction"
        self.args = args
        self.data_name = args.data_name
        self.reward_type = args.reward_type
        self.k = args.k
        self.normalize = args.normalize_rewards
        self.device = args.device
        self.emb_batch_size = args.emb_batch_size
        self.uid_2_target = uid_2_target  # Store for full item pool ranking
        
        # Novelty ê´€ë ¨ íŒŒë¼ë¯¸í„°
        self.novelty_reward = args.novelty_reward
        self.novelty_coef = args.novelty_coef
        self.novelty_target_rank = args.novelty_target_rank
        self.novelty_mode = args.novelty_mode
        self.novelty_annealing = args.novelty_annealing
        
        # Popularity ê´€ë ¨ íŒŒë¼ë¯¸í„°
        self.popularity_coef = args.popularity_coef
        
        # Target embedding ìœ ì‚¬ë„ ë¦¬ì›Œë“œ íŒŒë¼ë¯¸í„°
        self.target_emb_reward = args.target_emb_reward
        self.target_emb_file = args.target_emb_file
        self.target_emb_coef = args.target_emb_coef
        
        # InfoNCE ë¦¬ì›Œë“œ íŒŒë¼ë¯¸í„°
        self.infonce_reward = args.infonce_reward
        self.infonce_coef = args.infonce_coef
        self.infonce_temperature = args.infonce_temperature
        self.infonce_emb_type = args.infonce_emb_type if args.infonce_emb_type is not None else args.emb_type
        
        # Proxy label ë¦¬ì›Œë“œ íŒŒë¼ë¯¸í„°
        if hasattr(args, "proxy_label_reward"):
            self.proxy_label_reward = args.proxy_label_reward
            self.proxy_k = args.proxy_k
            self.proxy_label_coef = args.proxy_label_coef
            self.proxy_label_cutoff = args.proxy_label_cutoff if hasattr(args, "proxy_label_cutoff") else 0.98
        else:
            self.proxy_label_reward = False
            self.proxy_k = 0
            self.proxy_label_coef = 0
            self.proxy_label_cutoff = 0.0
        
        # Anchor-Guided GRPO íŒŒë¼ë¯¸í„°
        if hasattr(args, "anchor_reward"):
            self.anchor_reward = args.anchor_reward
            self.anchor_coef = args.anchor_coef
            self.anchor_radius_start = args.anchor_radius_start
            self.anchor_radius_end = args.anchor_radius_end
            self.anchor_penalty_mode = args.anchor_penalty_mode
            self.anchor_penalty_value = args.anchor_penalty_value
        else:
            self.anchor_reward = False
            self.anchor_coef = 1.0
            self.anchor_radius_start = 0.5
            self.anchor_radius_end = 1.0
            self.anchor_penalty_mode = "soft"
            self.anchor_penalty_value = -1.0
        
        # Adaptive Threshold Reward íŒŒë¼ë¯¸í„°
        if hasattr(args, "adaptive_threshold_reward"):
            self.adaptive_threshold_reward = args.adaptive_threshold_reward
            self.adaptive_threshold_coef = args.adaptive_threshold_coef
            self.adaptive_tau_min = args.adaptive_tau_min
        else:
            self.adaptive_threshold_reward = False
            self.adaptive_threshold_coef = 1.0
            self.adaptive_tau_min = 0.0
        
        # History Proxy Threshold Reward íŒŒë¼ë¯¸í„°
        if hasattr(args, "history_proxy_threshold_reward"):
            self.history_proxy_threshold_reward = args.history_proxy_threshold_reward
            self.history_proxy_threshold_coef = args.history_proxy_threshold_coef
        else:
            self.history_proxy_threshold_reward = False
            self.history_proxy_threshold_coef = 1.0
        
        # Training ê´€ë ¨ íŒŒë¼ë¯¸í„°
        self.max_steps = args.max_steps
        
        # Reward ë¶„í•´ ì¶”ì ì„ ìœ„í•œ ë³€ìˆ˜ (wandb ë¡œê¹…ìš©)
        self.last_base_rewards = None
        self.last_proxy_label_rewards = None
        self.last_target_emb_rewards = None
        self.last_infonce_rewards = None
        self.last_anchor_rewards = None
        self.last_adaptive_threshold_rewards = None
        self.last_history_proxy_threshold_rewards = None
        
        print(f"ğŸ’° Reward configuration:")
        print(f"  - Reward type: {self.reward_type}")
        print(f"  - Top-K: {self.k}")
        print(f"  - Normalize: {self.normalize}")
        if self.novelty_reward:
            print(f"  - Novelty reward: ENABLED")
            print(f"  - Novelty coefficient: {self.novelty_coef}")
            print(f"  - Novelty = NDCG Ã— popularity_weight (ì¸ê¸° ì—†ëŠ” ì•„ì´í…œ ì¥ë ¤)")
            if self.novelty_annealing:
                print(f"  - Novelty annealing: ENABLED")
                print(f"  - Novelty ratio will increase linearly from 0 to 1 over {self.max_steps} steps")
                print(f"  - Final reward = (1-ratio)*base + ratio*novelty")
        if self.popularity_coef > 0:
            print(f"  - Popularity coefficient: {self.popularity_coef}")
            print(f"  - Popularity bonus for unpopular items (when correct)")
        if self.target_emb_reward:
            print(f"  - Target embedding reward: ENABLED")
            print(f"  - Target embedding file: {self.target_emb_file}")
            print(f"  - Target embedding coefficient: {self.target_emb_coef}")
            print(f"  - Reward based on cosine similarity with target embedding")
        if self.infonce_reward:
            print(f"  - InfoNCE reward: ENABLED")
            print(f"  - InfoNCE coefficient: {self.infonce_coef}")
            print(f"  - InfoNCE temperature: {self.infonce_temperature}")
            print(f"  - InfoNCE embedding type: {self.infonce_emb_type}")
            print(f"  - Contrastive learning: maximize target similarity, minimize negative similarity")
        if self.proxy_label_reward:
            print(f"  - Proxy label reward: ENABLED")
            print(f"  - Proxy K: {self.proxy_k}")
            print(f"  - Proxy label coefficient: {self.proxy_label_coef}")
            print(f"  - Proxy label cutoff: {self.proxy_label_cutoff}")
            print(f"    â†’ Items with similarity < {self.proxy_label_cutoff} will be excluded from proxy labels")
            print(f"  - Use top-{self.proxy_k} similar items as soft labels with similarity-weighted NDCG")
            print(f"  - Final reward = base_reward + proxy_label_coef * proxy_label_ndcg")
        if self.anchor_reward:
            print(f"  - Anchor-Guided GRPO (AG-GRPO): ENABLED")
            print(f"  - Anchor coefficient: {self.anchor_coef}")
            print(f"  - Anchor radius: {self.anchor_radius_start} â†’ {self.anchor_radius_end} (curriculum learning)")
            print(f"  - Penalty mode: {self.anchor_penalty_mode}")
            if self.anchor_penalty_mode == "hard":
                print(f"  - Hard penalty value: {self.anchor_penalty_value}")
            print(f"  - Reward based on similarity with last item (anchor) embedding")
            print(f"  - Gradually expands exploration radius as training progresses")
        if self.adaptive_threshold_reward:
            print(f"  - Adaptive Threshold Reward: ENABLED")
            print(f"  - Adaptive threshold coefficient: {self.adaptive_threshold_coef}")
            print(f"  - Minimum threshold (tau_min): {self.adaptive_tau_min}")
            print(f"  - Uses dynamic threshold based on historical item similarity (S_base)")
            print(f"  - Reward = 1 if CosSim(query, target) > max(tau_min, S_base), else 0")
        if self.history_proxy_threshold_reward:
            print(f"  - History Proxy Threshold Reward: ENABLED")
            print(f"  - History proxy threshold coefficient: {self.history_proxy_threshold_coef}")
            print(f"  - Uses most similar history item to target as proxy")
            print(f"  - Reward = max(0, CosSim(query, proxy) - mean(CosSim(query, other_history)))")
        
        # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
        print(f"ğŸ¤– Loading embedding model: {args.emb_model_name}")
        from sentence_transformers import SentenceTransformer
        self.emb_model = SentenceTransformer(args.emb_model_name, device=self.device)
        print(f"âœ“ Embedding model loaded on {self.device}")

        total_user_count = 0
        sequential_file = f"data/{self.data_name}/sequential_data.txt"        
        with open(sequential_file, 'r') as f:
            for line in f:
                total_user_count += 1
        
        # k > 100ì´ë©´ ì „ì²´ ì•„ì´í…œ í’€ ì‚¬ìš©, ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ negative pool ì‚¬ìš©
        self.use_full_item_pool = (self.k > 100)
        if self.use_full_item_pool:
            print(f"âš ï¸ k={self.k} > 100: Using full item pool for ranking (no negative sampling)")
            self.negative_pool = None
            self.candidate_tensor = None  # Will use full item embeddings
        else:
            # Negative pool ë¡œë“œ
            self.negative_pool = load_negative_pool(self.data_name, args.data_dir, self.k)
            # prepare candidate set, target comes first
            self.candidate_tensor = self._prepare_candidate_tensor(total_user_count, uid_2_target, self.negative_pool)
        
        # ì‚¬ì „ ê³„ì‚°ëœ ì•„ì´í…œ ì„ë² ë”© ë¡œë“œ
        emb_model_name_dir = args.emb_model_name.split("/")[-1]
        item_embedding_file_path = f"data_emb/{self.data_name}_{args.emb_type}_{emb_model_name_dir}_emb.pt"
        print(f"ğŸ“¦ Loading pre-computed item embeddings from: {item_embedding_file_path}")
        self.item_embeddings = torch.load(item_embedding_file_path, map_location=self.device)
        print(f"âœ“ Loaded embeddings for {len(self.item_embeddings)} items")
        
        # InfoNCEìš© ì¶”ê°€ ì„ë² ë”© ë¡œë“œ (í•„ìš” ì‹œ)
        if self.infonce_reward and self.infonce_emb_type != args.emb_type:
            infonce_embedding_file_path = f"data_emb/{self.data_name}_{self.infonce_emb_type}_{emb_model_name_dir}_emb.pt"
            print(f"ğŸ“¦ Loading InfoNCE embeddings from: {infonce_embedding_file_path}")
            self.infonce_item_embeddings = torch.load(infonce_embedding_file_path, map_location=self.device)
            print(f"âœ“ Loaded InfoNCE embeddings for {len(self.infonce_item_embeddings)} items")
        else:
            # ê°™ì€ ì„ë² ë”© ì‚¬ìš©
            self.infonce_item_embeddings = self.item_embeddings if self.infonce_reward else None
        
        # Proxy labelì„ ìœ„í•œ ì•„ì´í…œ ê°„ ìœ ì‚¬ë„ ë¡œë“œ ë˜ëŠ” ê³„ì‚°
        if self.proxy_label_reward:
            # ì €ì¥ëœ proxy labels íŒŒì¼ í™•ì¸
            # args.proxy_label_fileì´ ì§€ì •ë˜ì–´ ìˆìœ¼ë©´ ê·¸ê²ƒì„ ì‚¬ìš©, ì•„ë‹ˆë©´ ìë™ ìƒì„±
            if hasattr(args, 'proxy_label_file') and args.proxy_label_file is not None:
                proxy_labels_file = args.proxy_label_file
                print(f"ğŸ“¦ Using user-specified proxy labels file: {proxy_labels_file}")
            else:
                proxy_labels_file = f"data_emb/{self.data_name}_proxy_labels_k100_{args.emb_type}_{emb_model_name_dir}.json"
                print(f"ğŸ“¦ Using auto-generated proxy labels path: {proxy_labels_file}")
            
            proxy_labels_path = Path(proxy_labels_file)
            
            if proxy_labels_path.exists():
                print(f"âœ“ Loading pre-computed proxy labels from: {proxy_labels_file}")
                self.item_proxy_labels = self._load_proxy_labels(proxy_labels_path)
                print(f"âœ“ Loaded proxy labels for {len(self.item_proxy_labels)} items")
            else:
                print(f"âš ï¸  Pre-computed proxy labels not found: {proxy_labels_file}")
                print(f"   Computing proxy labels on-the-fly (this may take time)...")
                exit()

        else:
            self.item_proxy_labels = None
        
        # Target embeddings ì¤€ë¹„ (target_emb_reward ì‚¬ìš© ì‹œ)
        if self.target_emb_reward:
            self.target_embeddings = self._prepare_target_embeddings(uid_2_target)
            print(f"âœ“ Prepared target embeddings for {len(uid_2_target)} users")
        else:
            self.target_embeddings = None
        
        # Last item (anchor) embeddings ì¤€ë¹„ (anchor_reward ì‚¬ìš© ì‹œ)
        if self.anchor_reward:
            self.last_item_embeddings = self._prepare_last_item_embeddings()
            print(f"âœ“ Prepared last item (anchor) embeddings for anchor-guided exploration")
        else:
            self.last_item_embeddings = None
        
        # History items ì¤€ë¹„ (adaptive_threshold_reward ì‚¬ìš© ì‹œ)
        if self.adaptive_threshold_reward:
            self.user_history_items = self._prepare_user_history_items()
            print(f"âœ“ Prepared user history items for adaptive threshold reward")
        else:
            self.user_history_items = None
        
        # History proxy items ì¤€ë¹„ (history_proxy_threshold_reward ì‚¬ìš© ì‹œ)
        if self.history_proxy_threshold_reward:
            self.user_history_items = self._prepare_user_history_items()
            self.user_history_proxy_items = self._prepare_user_history_proxy_items(uid_2_target)
            print(f"âœ“ Prepared user history proxy items for history proxy threshold reward")
        else:
            self.user_history_proxy_items = None
        
        # ì•„ì´í…œ ì¸ê¸°ë„ ê³„ì‚° (train setì—ì„œ)
        # Novelty ë˜ëŠ” Popularity reward ì‚¬ìš© ì‹œ í•„ìš”
        if self.novelty_reward or self.popularity_coef > 0:
            self.item_popularity_weights = self._compute_item_popularity(
                uid_2_target, self.negative_pool, self.data_name, args.data_dir
            )
        else:
            self.item_popularity_weights = None

    def _prepare_candidate_tensor(self, total_user_count: int, uid_2_target: Dict[int, int], neg_pool: Dict[int, List[int]]) -> torch.Tensor:
        candidate_tensor = torch.zeros(total_user_count+1, len(list(neg_pool.values())[0])+1, dtype=torch.long)
        for uid, target_id in uid_2_target.items():
            candidate_tensor[uid] = torch.tensor([target_id] + neg_pool[uid], dtype=torch.long)
        return candidate_tensor
    
    def _load_proxy_labels(self, file_path: Path) -> Dict[int, Tuple[torch.Tensor, torch.Tensor]]:
        """
        ì €ì¥ëœ proxy labelsë¥¼ ë¡œë“œ
        
        Args:
            file_path: proxy labels JSON íŒŒì¼ ê²½ë¡œ
            
        Returns:
            item_proxy_labels: Dict[item_id, (proxy_item_ids, proxy_similarities)]
        """
        with open(file_path, 'r') as f:
            proxy_labels_json = json.load(f)
        
        # JSONì—ì„œ ë¡œë“œí•œ ë°ì´í„°ë¥¼ í…ì„œë¡œ ë³€í™˜
        item_proxy_labels = {}
        total_filtered = 0  # í•„í„°ë§ëœ proxy ê°œìˆ˜ í†µê³„
        
        for item_id_str, proxy_list in proxy_labels_json.items():
            item_id = int(item_id_str)
            
            # 1. proxy_k ê°œìˆ˜ë§Œí¼ ìë¥´ê¸°
            proxy_list = proxy_list[:self.proxy_k]
            
            # 2. cutoff ì´í•˜ì˜ ì•„ì´í…œ í•„í„°ë§
            if self.proxy_label_cutoff > 0:
                filtered_proxy_list = [(pid, sim) for pid, sim in proxy_list if sim >= self.proxy_label_cutoff]
                total_filtered += len(proxy_list) - len(filtered_proxy_list)
                proxy_list = filtered_proxy_list
            
            # 3. í•„í„°ë§ í›„ ë‚¨ì€ proxyê°€ ìˆëŠ” ê²½ìš°ë§Œ ì €ì¥
            if len(proxy_list) > 0:
                # List[Tuple[item_id, similarity]]ë¥¼ ë‘ ê°œì˜ í…ì„œë¡œ ë¶„ë¦¬
                proxy_ids = torch.tensor([p[0] for p in proxy_list], dtype=torch.long, device=self.device)
                proxy_sims = torch.tensor([p[1] for p in proxy_list], dtype=torch.float32, device=self.device)
                
                item_proxy_labels[item_id] = (proxy_ids, proxy_sims)
        
        # í•„í„°ë§ í†µê³„ ì¶œë ¥
        if self.proxy_label_cutoff > 0:
            print(f"  - Filtered {total_filtered} proxy labels below cutoff {self.proxy_label_cutoff}")
            avg_proxies = sum(len(v[0]) for v in item_proxy_labels.values()) / max(1, len(item_proxy_labels))
            print(f"  - Average proxies per item after filtering: {avg_proxies:.2f}")
        
        return item_proxy_labels
    
    def _prepare_target_embeddings(self, uid_2_target: Dict[int, int]) -> torch.Tensor:
        """
        ê° ì‚¬ìš©ìì˜ íƒ€ê²Ÿ ì•„ì´í…œ ì„ë² ë”©ì„ ì¤€ë¹„
        
        Args:
            uid_2_target: ì‚¬ìš©ì ID to íƒ€ê²Ÿ ì•„ì´í…œ ID ë§¤í•‘
            
        Returns:
            target_embeddings: [max_uid+1, emb_dim] ê° ì‚¬ìš©ìì˜ íƒ€ê²Ÿ ì„ë² ë”©
        """
        if self.target_emb_file is not None:
            target_embeddings = torch.load(f"data_emb/{self.args.target_emb_file}", map_location=self.device)
            return target_embeddings
        
        max_uid = max(uid_2_target.keys())
        emb_dim = self.item_embeddings.shape[1]
        
        # ì‚¬ìš©ìë³„ íƒ€ê²Ÿ ì„ë² ë”© í…ì„œ ì´ˆê¸°í™”
        target_embeddings = torch.zeros(max_uid + 1, emb_dim, device=self.device)
        
        for uid, target_id in uid_2_target.items():
            target_embeddings[uid] = self.item_embeddings[target_id]
        
        return target_embeddings
    
    def _prepare_last_item_embeddings(self) -> torch.Tensor:
        """
        ê° ì‚¬ìš©ìì˜ ë§ˆì§€ë§‰ ì•„ì´í…œ (ì•µì»¤) ì„ë² ë”©ì„ ì¤€ë¹„
        sequential_data.txtì—ì„œ validation set ê¸°ì¤€ ë§ˆì§€ë§‰ ì•„ì´í…œ ì½ê¸°
        
        Returns:
            last_item_embeddings: [max_uid+1, emb_dim] ê° ì‚¬ìš©ìì˜ ë§ˆì§€ë§‰ ì•„ì´í…œ ì„ë² ë”©
        """
        print(f"ğŸ“¦ Preparing last item (anchor) embeddings from sequential data...")
        
        sequential_file = f"data/{self.data_name}/sequential_data.txt"
        
        # ì‚¬ìš©ìë³„ ë§ˆì§€ë§‰ ì•„ì´í…œ ID ìˆ˜ì§‘
        uid_2_last_item = {}
        max_uid = 0
        
        with open(sequential_file, 'r') as f:
            for line in f:
                parts = [int(p) for p in line.strip().split()]
                user_id = parts[0]
                last_item_id = parts[-3]  # ë§ˆì§€ë§‰ ì•„ì´í…œ
                uid_2_last_item[user_id] = last_item_id
                max_uid = max(max_uid, user_id)
        
        # ì„ë² ë”© í…ì„œ ì´ˆê¸°í™”
        emb_dim = self.item_embeddings.shape[1]
        last_item_embeddings = torch.zeros(max_uid + 1, emb_dim, device=self.device)
        
        # ë§ˆì§€ë§‰ ì•„ì´í…œ ì„ë² ë”© ì±„ìš°ê¸°
        for uid, last_item_id in uid_2_last_item.items():
            last_item_embeddings[uid] = self.item_embeddings[last_item_id]
        
        print(f"  Total users with last item: {len(uid_2_last_item)}")
        print(f"  Max user ID: {max_uid}")
        
        return last_item_embeddings
    
    def _prepare_user_history_items(self) -> Dict[int, torch.Tensor]:
        """
        ê° ì‚¬ìš©ìì˜ ê³¼ê±° êµ¬ë§¤ ì•„ì´í…œ ëª©ë¡ì„ ì¤€ë¹„ (adaptive threshold rewardìš©)
        sequential_data.txtì—ì„œ train set history ì½ê¸°
        
        Returns:
            user_history_items: Dict[user_id, history_item_ids_tensor]
        """
        print(f"ğŸ“¦ Preparing user history items from sequential data...")
        
        sequential_file = f"data/{self.data_name}/sequential_data.txt"
        
        # ì‚¬ìš©ìë³„ íˆìŠ¤í† ë¦¬ ì•„ì´í…œ ìˆ˜ì§‘
        user_history_items = {}
        
        with open(sequential_file, 'r') as f:
            for line in f:
                parts = [int(p) for p in line.strip().split()]
                user_id = parts[0]
                history = parts[1:-3]  # Train setì˜ history
                
                # íˆìŠ¤í† ë¦¬ê°€ ë¹„ì–´ìˆìœ¼ë©´ ìŠ¤í‚µ
                if len(history) == 0:
                    continue
                
                # í…ì„œë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥
                user_history_items[user_id] = torch.tensor(history, dtype=torch.long, device=self.device)
        
        print(f"  Total users with history: {len(user_history_items)}")
        
        # í†µê³„ ì¶œë ¥
        if len(user_history_items) > 0:
            history_lengths = [len(h) for h in user_history_items.values()]
            avg_length = sum(history_lengths) / len(history_lengths)
            min_length = min(history_lengths)
            max_length = max(history_lengths)
            print(f"  History length - Min: {min_length}, Max: {max_length}, Avg: {avg_length:.2f}")
        
        return user_history_items
    
    def _prepare_user_history_proxy_items(self, uid_2_target: Dict[int, int]) -> Dict[int, int]:
        """
        ê° ì‚¬ìš©ìì— ëŒ€í•´ íƒ€ê²Ÿ ì•„ì´í…œê³¼ ê°€ì¥ ìœ ì‚¬í•œ íˆìŠ¤í† ë¦¬ ì•„ì´í…œì„ ì‚¬ì „ì— ê³„ì‚°
        (history proxy threshold rewardìš©)
        
        ì „ëµ:
        - íƒ€ê²Ÿ ì•„ì´í…œê³¼ ê° íˆìŠ¤í† ë¦¬ ì•„ì´í…œì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°
        - ê°€ì¥ ìœ ì‚¬ë„ê°€ ë†’ì€ íˆìŠ¤í† ë¦¬ ì•„ì´í…œì„ proxyë¡œ ì €ì¥
        
        Args:
            uid_2_target: ì‚¬ìš©ì ID to íƒ€ê²Ÿ ì•„ì´í…œ ID ë§¤í•‘
        
        Returns:
            user_history_proxy_items: Dict[user_id, proxy_item_id]
        """
        print(f"ğŸ“¦ Pre-computing most similar history items to target for each user...")
        
        user_history_proxy_items = {}
        
        # ì•„ì´í…œ ì„ë² ë”© ì •ê·œí™” (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°ì„ ìœ„í•´)
        item_embeddings_norm = torch.nn.functional.normalize(self.item_embeddings, p=2, dim=1)
        
        users_with_proxy = 0
        users_without_history = 0
        
        for uid, target_id in uid_2_target.items():
            # íˆìŠ¤í† ë¦¬ê°€ ì—†ëŠ” ê²½ìš° ìŠ¤í‚µ
            if uid not in self.user_history_items:
                users_without_history += 1
                continue
            
            history_item_ids = self.user_history_items[uid]  # [history_len]
            
            # íƒ€ê²Ÿ ì„ë² ë”©
            target_emb = item_embeddings_norm[target_id].unsqueeze(0)  # [1, emb_dim]
            
            # íˆìŠ¤í† ë¦¬ ì„ë² ë”©
            history_embs = item_embeddings_norm[history_item_ids]  # [history_len, emb_dim]
            
            # íƒ€ê²Ÿê³¼ íˆìŠ¤í† ë¦¬ ì•„ì´í…œë“¤ì˜ ìœ ì‚¬ë„ ê³„ì‚°
            similarities = torch.mm(target_emb, history_embs.T).squeeze(0)  # [history_len]
            
            # ê°€ì¥ ìœ ì‚¬ë„ê°€ ë†’ì€ íˆìŠ¤í† ë¦¬ ì•„ì´í…œ ì„ íƒ
            max_sim_idx = similarities.argmax().item()
            proxy_item_id = history_item_ids[max_sim_idx].item()
            max_similarity = similarities[max_sim_idx].item()
            
            user_history_proxy_items[uid] = proxy_item_id
            users_with_proxy += 1
            
            # ë””ë²„ê¹…: ì²˜ìŒ 5ëª…ì˜ ì‚¬ìš©ì ì •ë³´ ì¶œë ¥
            if users_with_proxy <= 5:
                print(f"  User {uid}: Target={target_id}, Proxy={proxy_item_id}, "
                      f"Similarity={max_similarity:.4f}, History len={len(history_item_ids)}")
        
        print(f"  Total users with proxy: {users_with_proxy}")
        print(f"  Users without history: {users_without_history}")
        
        return user_history_proxy_items
    
    def _compute_item_popularity(
        self, 
        uid_2_target: Dict[int, int], 
        neg_pool: Dict[int, List[int]],
        data_name: str,
        data_dir: str
    ) -> torch.Tensor:
        """
        Train setì—ì„œ ì•„ì´í…œ ì¸ê¸°ë„ ê³„ì‚° (log ì—­ìˆ˜ ì‚¬ìš©)
        
        Args:
            uid_2_target: ì‚¬ìš©ì ID to íƒ€ê²Ÿ ì•„ì´í…œ ID ë§¤í•‘
            neg_pool: Negative pool
            data_name: ë°ì´í„°ì…‹ ì´ë¦„
            data_dir: ë°ì´í„° ë””ë ‰í† ë¦¬
        
        Returns:
            item_popularity_weights: [num_items] ê° ì•„ì´í…œì˜ ì¸ê¸°ë„ ê°€ì¤‘ì¹˜
                                     ì¸ê¸° ì—†ëŠ” ì•„ì´í…œì¼ìˆ˜ë¡ ë†’ì€ ê°’
        """
        print(f"ğŸ“Š Computing item popularity weights from train set...")
        
        # sequential_data.txtì—ì„œ train set ë¡œë“œ (target_index=-3)
        sequential_file = f"{data_dir}/{data_name}/sequential_data.txt"
        item_counts = {}
        
        with open(sequential_file, 'r') as f:
            for line in f:
                parts = [int(p) for p in line.strip().split()]
                user_id = parts[0]
                history = parts[1:-3]  # Train setì˜ history
                target = parts[-3]  # Train setì˜ target
                
                # Historyì˜ ëª¨ë“  ì•„ì´í…œ ì¹´ìš´íŠ¸
                for item_id in history:
                    item_counts[item_id] = item_counts.get(item_id, 0) + 1
                
                # Target ì•„ì´í…œë„ ì¹´ìš´íŠ¸
                item_counts[target] = item_counts.get(target, 0) + 1
        
        # ì „ì²´ ì•„ì´í…œ ìˆ˜ íŒŒì•…
        all_item_ids = set(item_counts.keys())
        max_item_id = max(all_item_ids) if all_item_ids else 0
        
        print(f"  Total unique items in train set: {len(all_item_ids)}")
        print(f"  Max item ID: {max_item_id}")
        
        # ì¸ê¸°ë„ ê°€ì¤‘ì¹˜ ê³„ì‚°: log(count + 1)ì˜ ì—­ìˆ˜
        # ì¸ê¸° ë§ì€ ì•„ì´í…œ -> ë‚®ì€ ê°€ì¤‘ì¹˜
        # ì¸ê¸° ì—†ëŠ” ì•„ì´í…œ -> ë†’ì€ ê°€ì¤‘ì¹˜
        item_weights = torch.ones(max_item_id + 1, device=self.device)
        
        for item_id, count in item_counts.items():
            # log(count + 1)ì˜ ì—­ìˆ˜
            item_weights[item_id] = 1.0 / np.log(count + 1)
        
        # ì •ê·œí™” (í‰ê· ì´ 1ì´ ë˜ë„ë¡)
        # ë‚˜íƒ€ë‚˜ì§€ ì•Šì€ ì•„ì´í…œì€ 1.0 ìœ ì§€
        appeared_mask = torch.zeros(max_item_id + 1, dtype=torch.bool, device=self.device)
        for item_id in all_item_ids:
            appeared_mask[item_id] = True
        
        if appeared_mask.sum() > 0:
            mean_weight = item_weights[appeared_mask].mean()
            item_weights[appeared_mask] = item_weights[appeared_mask] / mean_weight
        
        print(f"  Item popularity weight statistics:")
        print(f"    Min: {item_weights[appeared_mask].min().item():.4f}")
        print(f"    Max: {item_weights[appeared_mask].max().item():.4f}")
        print(f"    Mean: {item_weights[appeared_mask].mean().item():.4f}")
        print(f"    Std: {item_weights[appeared_mask].std().item():.4f}")
        
        # ì˜ˆì‹œ ì¶œë ¥
        sorted_counts = sorted(item_counts.items(), key=lambda x: x[1], reverse=True)
        print(f"  Top-5 popular items:")
        for item_id, count in sorted_counts[:5]:
            weight = item_weights[item_id].item()
            print(f"    Item {item_id}: count={count}, weight={weight:.4f}")
        
        print(f"  Bottom-5 popular items:")
        for item_id, count in sorted_counts[-5:]:
            weight = item_weights[item_id].item()
            print(f"    Item {item_id}: count={count}, weight={weight:.4f}")
        
        print(f"âœ“ Item popularity weights computed")
        
        return item_weights
    
    def _encode_texts(self, generated_texts: List[str]) -> torch.Tensor:
        """
        ìƒì„±ëœ í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
        <query> íƒœê·¸ê°€ ìˆëŠ” ê²½ìš° íƒœê·¸ ë‚´ë¶€ì˜ í…ìŠ¤íŠ¸ë§Œ ì‚¬ìš©
        
        Args:
            generated_texts: [batch_size] ìƒì„±ëœ í…ìŠ¤íŠ¸
            
        Returns:
            embeddings: [batch_size, emb_dim] ì„ë² ë”©
        """
        # <query> íƒœê·¸ê°€ ìˆìœ¼ë©´ ì¶”ì¶œ, ì—†ìœ¼ë©´ ì›ë³¸ ì‚¬ìš©
        processed_texts = [extract_query_from_tags(text, tag="query") for text in generated_texts]
        
        embeddings = self.emb_model.encode(
            processed_texts,
            convert_to_tensor=True,
            show_progress_bar=False,
            device=self.device,
            batch_size=self.emb_batch_size,
        )
        return embeddings
    
    def _compute_similarity_scores(
        self,
        query_embeddings: torch.Tensor,
        user_ids: torch.Tensor,
        return_scores: bool = False,
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
        """
        Compute similarity scores between query embeddings and candidate set
        Args:
            query_embeddings: [batch_size, emb_dim] query embeddings
            user_ids: [batch_size] user ids
            return_scores: if True, return (ranks, scores), otherwise return (ranks, None)
        Returns:
            ranks: [batch_size] ranks of target items
            scores: [batch_size, num_candidates] similarity scores (only if return_scores=True)
        """
        if self.use_full_item_pool:
            # ì „ì²´ ì•„ì´í…œ í’€ì— ëŒ€í•´ ìœ ì‚¬ë„ ê³„ì‚°
            # query_embeddings: [batch_size, emb_dim]
            # item_embeddings: [num_items, emb_dim]
            scores = torch.mm(query_embeddings, self.item_embeddings.T)  # [batch_size, num_items]
            
            # ê° ì‚¬ìš©ìì˜ target item ê°€ì ¸ì˜¤ê¸°
            target_item_ids = torch.tensor(
                [self.uid_2_target[uid] for uid in user_ids],
                device=self.device
            )  # [batch_size]
            
            # Target itemì˜ ì ìˆ˜
            target_scores = scores[torch.arange(scores.size(0), device=self.device), target_item_ids]  # [batch_size]
            
            # Rank ê³„ì‚°: targetë³´ë‹¤ ë†’ì€ ì ìˆ˜ë¥¼ ê°€ì§„ ì•„ì´í…œì˜ ê°œìˆ˜ + 1
            ranks = (scores > target_scores.unsqueeze(1)).sum(dim=1) + 1
        else:
            # Negative pool ê¸°ë°˜ ê³„ì‚° (ê¸°ì¡´ ë¡œì§)
            batch_candidate_tensor = self.candidate_tensor[user_ids]
            scores = torch.bmm(query_embeddings.unsqueeze(1), self.item_embeddings[batch_candidate_tensor].transpose(1, 2)).squeeze(1)
            target_scores = scores[:, 0].unsqueeze(1)
            ranks = (scores > target_scores).sum(dim=1) + 1
        
        if return_scores:
            return ranks, scores
        else:
            return ranks, None
    
    def _compute_proxy_label_ndcg(
        self,
        query_embeddings: torch.Tensor,
        user_ids: torch.Tensor,
        predicted_scores: torch.Tensor,
    ) -> torch.Tensor:
        """
        Proxy label ê¸°ë°˜ NDCG ê³„ì‚°
        ë¯¸ë¦¬ ê³„ì‚°ëœ ì•„ì´í…œ ê°„ ìœ ì‚¬ë„ë¥¼ í™œìš©í•˜ì—¬ íƒ€ê²Ÿì˜ proxy labelsì„ soft labelë¡œ ì‚¬ìš©
        
        Args:
            query_embeddings: [batch_size, emb_dim] ì¿¼ë¦¬ ì„ë² ë”©
            user_ids: [batch_size] ì‚¬ìš©ì ID
            predicted_scores: [batch_size, num_items] ë˜ëŠ” [batch_size, num_candidates] ì˜ˆì¸¡ ì ìˆ˜
            
        Returns:
            ndcg_rewards: [batch_size] Proxy label ê¸°ë°˜ NDCG ë¦¬ì›Œë“œ
        """
        batch_size = len(user_ids)
        ndcg_rewards = torch.zeros(batch_size, device=self.device)
        
        # íƒ€ê²Ÿ ì•„ì´í…œ ID ê°€ì ¸ì˜¤ê¸°
        target_item_ids = torch.tensor(
            [self.uid_2_target[uid] for uid in user_ids],
            device=self.device
        )  # [batch_size]
        
        if self.use_full_item_pool:
            # ì „ì²´ ì•„ì´í…œ í’€ ì‚¬ìš©
            num_items = len(self.item_embeddings)
            
            for i in range(batch_size):
                target_id = target_item_ids[i].item()
                
                # # íƒ€ê²Ÿ ì•„ì´í…œì˜ ë¯¸ë¦¬ ê³„ì‚°ëœ proxy labels ê°€ì ¸ì˜¤ê¸°
                # if target_id not in self.item_proxy_labels:
                #     # Proxy labelsê°€ ì—†ìœ¼ë©´ íƒ€ê²Ÿë§Œ 1.0
                #     proxy_ids = torch.tensor([], dtype=torch.long, device=self.device)
                #     proxy_sims = torch.tensor([], dtype=torch.float32, device=self.device)
                # else:
                proxy_ids, proxy_sims = self.item_proxy_labels[target_id]
                
                # Relevance scores ìƒì„±: íƒ€ê²Ÿ ìì‹ ì€ 1.0, proxyëŠ” ìœ ì‚¬ë„ ë¹„ë¡€
                relevance_scores = torch.zeros(num_items, device=self.device)
                relevance_scores[target_id] = 1.0  # íƒ€ê²Ÿ ìì‹ 
                relevance_scores[proxy_ids] = proxy_sims  # Proxy labels
                
                # ì˜ˆì¸¡ ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ Top-K ì¶”ì¶œ
                pred_scores = predicted_scores[i]  # [num_items]
                top_k_pred_scores, top_k_pred_indices = torch.topk(pred_scores, k=min(self.k, len(pred_scores)))
                
                # Top-K ì˜ˆì¸¡ ê²°ê³¼ì—ì„œ relevance ì¶”ì¶œ
                predicted_relevance = relevance_scores[top_k_pred_indices]  # [k]
                
                # DCG ê³„ì‚°
                dcg = calculate_dcg(predicted_relevance.unsqueeze(0), k=self.k)[0]
                
                # IDCG ê³„ì‚° (ì´ìƒì ì¸ ê²½ìš°: relevanceê°€ ë†’ì€ ìˆœì„œëŒ€ë¡œ ì •ë ¬)
                ideal_relevance, _ = torch.sort(relevance_scores, descending=True)
                ideal_relevance = ideal_relevance[:self.k]
                idcg = calculate_dcg(ideal_relevance.unsqueeze(0), k=self.k)[0]
                
                # NDCG ê³„ì‚°
                if idcg > 0:
                    ndcg_rewards[i] = dcg / (idcg + 1e-10)
                else:
                    ndcg_rewards[i] = 0.0
        else:
            # Candidate set ê¸°ë°˜ ê³„ì‚°
            for i in range(batch_size):
                target_id = target_item_ids[i].item()
                
                # Candidate set ê°€ì ¸ì˜¤ê¸°
                batch_candidate_tensor = self.candidate_tensor[user_ids[i]]  # [num_candidates]
                num_candidates = len(batch_candidate_tensor)
                
                # íƒ€ê²Ÿ ì•„ì´í…œì˜ ë¯¸ë¦¬ ê³„ì‚°ëœ proxy labels ê°€ì ¸ì˜¤ê¸°
                if target_id not in self.item_proxy_labels:
                    # Proxy labelsê°€ ì—†ìœ¼ë©´ íƒ€ê²Ÿë§Œ 1.0
                    proxy_ids = torch.tensor([], dtype=torch.long, device=self.device)
                    proxy_sims = torch.tensor([], dtype=torch.float32, device=self.device)
                else:
                    proxy_ids, proxy_sims = self.item_proxy_labels[target_id]
                
                # Candidate set ë‚´ì—ì„œ relevance scores ìƒì„±
                relevance_scores = torch.zeros(num_candidates, device=self.device)
                
                # íƒ€ê²Ÿ ì•„ì´í…œì´ candidate setì— ìˆëŠ” ìœ„ì¹˜ ì°¾ê¸° (ë³´í†µ index 0)
                target_mask = batch_candidate_tensor == target_id
                if target_mask.any():
                    target_idx_in_candidates = target_mask.nonzero(as_tuple=True)[0][0]
                    relevance_scores[target_idx_in_candidates] = 1.0
                
                # Proxy labelsë„ candidate setì— ìˆëŠ”ì§€ í™•ì¸í•˜ê³  relevance í• ë‹¹
                if len(proxy_ids) > 0:
                    for proxy_id, proxy_sim in zip(proxy_ids, proxy_sims):
                        proxy_mask = batch_candidate_tensor == proxy_id.item()
                        if proxy_mask.any():
                            proxy_idx_in_candidates = proxy_mask.nonzero(as_tuple=True)[0][0]
                            relevance_scores[proxy_idx_in_candidates] = proxy_sim.item()
                
                # ì˜ˆì¸¡ ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ Top-K ì¶”ì¶œ
                pred_scores = predicted_scores[i]  # [num_candidates]
                top_k_pred_scores, top_k_pred_indices = torch.topk(pred_scores, k=min(self.k, len(pred_scores)))
                
                # Top-K ì˜ˆì¸¡ ê²°ê³¼ì—ì„œ relevance ì¶”ì¶œ
                predicted_relevance = relevance_scores[top_k_pred_indices]  # [k]
                
                # DCG ê³„ì‚°
                dcg = calculate_dcg(predicted_relevance.unsqueeze(0), k=self.k)[0]
                
                # IDCG ê³„ì‚°
                ideal_relevance, _ = torch.sort(relevance_scores, descending=True)
                ideal_relevance = ideal_relevance[:self.k]
                idcg = calculate_dcg(ideal_relevance.unsqueeze(0), k=self.k)[0]
                
                # NDCG ê³„ì‚°
                if idcg > 0:
                    ndcg_rewards[i] = dcg / (idcg + 1e-10)
                else:
                    ndcg_rewards[i] = 0.0
        
        return ndcg_rewards
    
    def _compute_target_embedding_reward(
        self,
        query_embeddings: torch.Tensor,
        user_ids: torch.Tensor,
    ) -> torch.Tensor:
        """
        íƒ€ê²Ÿ ì„ë² ë”©ê³¼ì˜ ìœ ì‚¬ë„ ê¸°ë°˜ ë¦¬ì›Œë“œ ê³„ì‚°
        
        ìƒì„±ëœ í…ìŠ¤íŠ¸ê°€ íƒ€ê²Ÿ ì„ë² ë”©ê³¼ ìœ ì‚¬í• ìˆ˜ë¡, 
        ê·¸ë¦¬ê³  negative ì„ë² ë”©ë“¤ë³´ë‹¤ íƒ€ê²Ÿê³¼ ë” ìœ ì‚¬í• ìˆ˜ë¡ ë†’ì€ ë¦¬ì›Œë“œ
        
        Args:
            query_embeddings: [batch_size, emb_dim] ì¿¼ë¦¬ ì„ë² ë”©
            user_ids: [batch_size] ì‚¬ìš©ì ID
            
        Returns:
            rewards: [batch_size] íƒ€ê²Ÿ ì„ë² ë”© ìœ ì‚¬ë„ ë¦¬ì›Œë“œ
                    (íƒ€ê²Ÿ ìœ ì‚¬ë„ - negative í‰ê·  ìœ ì‚¬ë„)
        """
        # L2 ì •ê·œí™” (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ìœ„í•´)
        query_embeddings = torch.nn.functional.normalize(query_embeddings, p=2, dim=1)
        
        # 2. íƒ€ê²Ÿ ì„ë² ë”©ê³¼ì˜ ìœ ì‚¬ë„ ê³„ì‚°
        target_embs = self.target_embeddings[user_ids]  # [batch_size, emb_dim]
        target_embs = torch.nn.functional.normalize(target_embs, p=2, dim=1)
        
        target_similarities = (query_embeddings * target_embs).sum(dim=1)  # [batch_size]
        
        if self.use_full_item_pool:
            # ì „ì²´ ì•„ì´í…œ í’€ ì‚¬ìš©: íƒ€ê²Ÿì„ ì œì™¸í•œ ëª¨ë“  ì•„ì´í…œì„ negativeë¡œ ì‚¬ìš©
            # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„ ìœ„í•´ ìƒ˜í”Œë§í•˜ê±°ë‚˜, ì „ì²´ ì•„ì´í…œì˜ í‰ê·  ìœ ì‚¬ë„ë¥¼ ê³„ì‚°
            # ì—¬ê¸°ì„œëŠ” ì „ì²´ ì•„ì´í…œ ì„ë² ë”©ì˜ í‰ê·  ìœ ì‚¬ë„ë¥¼ ì‚¬ìš©
            all_item_embs = torch.nn.functional.normalize(self.item_embeddings, p=2, dim=1)  # [num_items, emb_dim]
            
            # íƒ€ê²Ÿ ì•„ì´í…œ ID
            target_item_ids = torch.tensor(
                [self.uid_2_target[uid] for uid in user_ids],
                device=self.device
            )  # [batch_size]
            
            # ì „ì²´ ì•„ì´í…œê³¼ì˜ ìœ ì‚¬ë„ ê³„ì‚°
            all_similarities = torch.mm(query_embeddings, all_item_embs.T)  # [batch_size, num_items]
            
            # íƒ€ê²Ÿì„ ì œì™¸í•œ í‰ê·  ìœ ì‚¬ë„ ê³„ì‚°
            batch_size = all_similarities.size(0)
            num_items = all_similarities.size(1)
            
            # íƒ€ê²Ÿ ë§ˆìŠ¤í¬ ìƒì„±
            mask = torch.ones(batch_size, num_items, device=self.device, dtype=torch.bool)
            mask[torch.arange(batch_size, device=self.device), target_item_ids] = False
            
            # íƒ€ê²Ÿì„ ì œì™¸í•œ negativeë“¤ì˜ í‰ê·  ìœ ì‚¬ë„
            negative_mean_similarities = all_similarities[mask].view(batch_size, -1).mean(dim=1)
            rewards = target_similarities - torch.clamp(negative_mean_similarities, min=0.0)
        else:
            # # 3. Negative ì„ë² ë”©ë“¤ê³¼ì˜ í‰ê·  ìœ ì‚¬ë„ ê³„ì‚° (ê¸°ì¡´ ë¡œì§)
            # batch_candidate_tensor = self.candidate_tensor[user_ids]  # [batch_size, k]
            # negative_ids = batch_candidate_tensor[:, 1:]  # [batch_size, k-1] (ì²« ë²ˆì§¸ëŠ” target ì œì™¸)
            
            # # ì „ì²´ íƒ€ê²Ÿ ì„ë² ë”©ê³¼ì˜ ìœ ì‚¬ë„ ê³„ì‚°
            # all_similarities = torch.mm(query_embeddings, self.target_embeddings.T)  # [batch_size, num_users]
            
            # # í‰ê·  ìœ ì‚¬ë„ ê³„ì‚°
            # negative_mean_similarities = all_similarities.mean(dim=1)  # [batch_size]
            rewards = target_similarities
                
        return rewards
    
    def _compute_infonce_reward(
        self,
        query_embeddings: torch.Tensor,
        user_ids: torch.Tensor,
    ) -> torch.Tensor:
        """
        InfoNCE (ëŒ€ì¡° í•™ìŠµ) ë¦¬ì›Œë“œ ê³„ì‚°
        
        InfoNCE lossë¥¼ rewardë¡œ ë³€í™˜:
        reward = log(exp(sim(q, pos)/tau) / (exp(sim(q, pos)/tau) + sum(exp(sim(q, neg_i)/tau))))
        
        íƒ€ê²Ÿê³¼ì˜ ìœ ì‚¬ë„ëŠ” ë†’ì´ê³ , negativeë“¤ê³¼ì˜ ìœ ì‚¬ë„ëŠ” ë‚®ì¶”ë„ë¡ ì¥ë ¤
        
        Args:
            query_embeddings: [batch_size, emb_dim] ì¿¼ë¦¬ ì„ë² ë”©
            user_ids: [batch_size] ì‚¬ìš©ì ID
            
        Returns:
            rewards: [batch_size] InfoNCE ë¦¬ì›Œë“œ (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)
        """
        # L2 ì •ê·œí™” (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ìœ„í•´)
        query_embeddings = torch.nn.functional.normalize(query_embeddings, p=2, dim=1)
        
        if self.use_full_item_pool:
            # ì „ì²´ ì•„ì´í…œ í’€ ì‚¬ìš©
            # íƒ€ê²Ÿ ì•„ì´í…œ ID
            target_ids = torch.tensor(
                [self.uid_2_target[uid] for uid in user_ids],
                device=self.device
            )  # [batch_size]
            
            # InfoNCEìš© ì„ë² ë”© ì‚¬ìš©
            target_embs = self.infonce_item_embeddings[target_ids]  # [batch_size, emb_dim]
            target_embs = torch.nn.functional.normalize(target_embs, p=2, dim=1)
            
            all_item_embs = torch.nn.functional.normalize(self.infonce_item_embeddings, p=2, dim=1)  # [num_items, emb_dim]
            
            # 1. íƒ€ê²Ÿê³¼ì˜ ìœ ì‚¬ë„ ê³„ì‚°
            pos_sim = (query_embeddings * target_embs).sum(dim=1)  # [batch_size]
            pos_sim = pos_sim / self.infonce_temperature
            
            # 2. ì „ì²´ ì•„ì´í…œë“¤ê³¼ì˜ ìœ ì‚¬ë„ ê³„ì‚° (íƒ€ê²Ÿ ì œì™¸)
            all_sims = torch.mm(query_embeddings, all_item_embs.T)  # [batch_size, num_items]
            all_sims = all_sims / self.infonce_temperature
            
            # 3. InfoNCE ê³„ì‚°
            # log(exp(pos_sim) / sum(exp(all_sims)))
            log_sum_exp = torch.logsumexp(all_sims, dim=1)  # [batch_size]
            infonce_rewards = pos_sim - log_sum_exp  # [batch_size]
        else:
            # Candidate tensor ê°€ì ¸ì˜¤ê¸° (ê¸°ì¡´ ë¡œì§)
            batch_candidate_tensor = self.candidate_tensor[user_ids]  # [batch_size, k]
            target_ids = batch_candidate_tensor[:, 0]  # [batch_size] - target
            negative_ids = batch_candidate_tensor[:, 1:]  # [batch_size, k-1] - negatives
            
            # InfoNCEìš© ì„ë² ë”© ì‚¬ìš©
            target_embs = self.infonce_item_embeddings[target_ids]  # [batch_size, emb_dim]
            target_embs = torch.nn.functional.normalize(target_embs, p=2, dim=1)
            
            negative_embs = self.infonce_item_embeddings[negative_ids]  # [batch_size, k-1, emb_dim]
            negative_embs = torch.nn.functional.normalize(negative_embs, p=2, dim=2)
            
            # 1. íƒ€ê²Ÿê³¼ì˜ ìœ ì‚¬ë„ ê³„ì‚°
            pos_sim = (query_embeddings * target_embs).sum(dim=1)  # [batch_size]
            pos_sim = pos_sim / self.infonce_temperature
            
            # 2. Negativeë“¤ê³¼ì˜ ìœ ì‚¬ë„ ê³„ì‚°
            neg_sims = torch.bmm(
                query_embeddings.unsqueeze(1),  # [batch_size, 1, emb_dim]
                negative_embs.transpose(1, 2)   # [batch_size, emb_dim, k-1]
            ).squeeze(1)  # [batch_size, k-1]
            neg_sims = neg_sims / self.infonce_temperature
            
            # 3. InfoNCE ê³„ì‚°
            # log(exp(pos_sim) / (exp(pos_sim) + sum(exp(neg_sims))))
            # = pos_sim - log(exp(pos_sim) + sum(exp(neg_sims)))
            # = pos_sim - logsumexp([pos_sim, neg_sims])
            
            all_sims = torch.cat([pos_sim.unsqueeze(1), neg_sims], dim=1)  # [batch_size, k]
            log_sum_exp = torch.logsumexp(all_sims, dim=1)  # [batch_size]
            
            infonce_rewards = pos_sim - log_sum_exp  # [batch_size]
        
        return infonce_rewards
    
    def _compute_anchor_reward(
        self,
        query_embeddings: torch.Tensor,
        user_ids: torch.Tensor,
        current_step: Optional[int] = None,
    ) -> torch.Tensor:
        """
        Anchor-Guided GRPO ë¦¬ì›Œë“œ ê³„ì‚°
        ë§ˆì§€ë§‰ ì•„ì´í…œ ì„ë² ë”©ê³¼ì˜ ìœ ì‚¬ë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¦¬ì›Œë“œ ê³„ì‚°
        ë™ì  ë°˜ê²½(radius) ì œì–´: í•™ìŠµ ì´ˆê¸°ì—ëŠ” ì¢ì€ ë°˜ê²½, í›„ê¸°ì—ëŠ” ë„“ì€ ë°˜ê²½
        
        Args:
            query_embeddings: [batch_size, emb_dim] ì¿¼ë¦¬ ì„ë² ë”©
            user_ids: [batch_size] ì‚¬ìš©ì ID
            current_step: í˜„ì¬ í•™ìŠµ step (Noneì´ë©´ ì¤‘ê°„ê°’ ì‚¬ìš©)
            
        Returns:
            rewards: [batch_size] ì•µì»¤ ë¦¬ì›Œë“œ
                    soft mode: similarity (ë°˜ê²½ ë‚´ì™¸ ëª¨ë‘ ìœ ì‚¬ë„ ë¦¬ì›Œë“œ)
                    hard mode: similarity if in radius, else penalty_value
        """
        # L2 ì •ê·œí™” (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ìœ„í•´)
        query_embeddings = torch.nn.functional.normalize(query_embeddings, p=2, dim=1)
        
        # ë§ˆì§€ë§‰ ì•„ì´í…œ ì„ë² ë”© ê°€ì ¸ì˜¤ê¸°
        last_item_embs = self.last_item_embeddings[user_ids]  # [batch_size, emb_dim]
        last_item_embs = torch.nn.functional.normalize(last_item_embs, p=2, dim=1)
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° (similarity âˆˆ [-1, 1])
        similarities = (query_embeddings * last_item_embs).sum(dim=1)  # [batch_size]
        
        # # ë™ì  ë°˜ê²½ ê³„ì‚° (curriculum learning)
        # if current_step is not None:
        #     # í•™ìŠµ ì§„í–‰ë„ì— ë”°ë¼ ë°˜ê²½ ì„ í˜• ì¦ê°€
        #     progress = min(1.0, current_step / max(1, self.max_steps))
        #     current_radius = self.anchor_radius_start + progress * (self.anchor_radius_end - self.anchor_radius_start)
        # else:
        #     # Step ì •ë³´ê°€ ì—†ìœ¼ë©´ ì¤‘ê°„ê°’ ì‚¬ìš©
        current_radius = (self.anchor_radius_start + self.anchor_radius_end) / 2.0
        
        if self.anchor_penalty_mode == "soft":
            # Soft mode: ìœ ì‚¬ë„ë¥¼ ê·¸ëŒ€ë¡œ ë¦¬ì›Œë“œë¡œ ì‚¬ìš©
            # ë°˜ê²½ ë‚´ì™¸ êµ¬ë¶„ ì—†ì´, ìœ ì‚¬ë„ê°€ ë†’ì„ìˆ˜ë¡ ë†’ì€ ë¦¬ì›Œë“œ
            rewards = similarities
        elif self.anchor_penalty_mode == "hard":
            # Hard mode: ë°˜ê²½ ë‚´ì— ìˆìœ¼ë©´ ìœ ì‚¬ë„ ë¦¬ì›Œë“œ, ë²—ì–´ë‚˜ë©´ í˜ë„í‹°
            # current_radiusë¥¼ thresholdë¡œ ì‚¬ìš©
            in_radius = similarities >= current_radius  # [batch_size] boolean
            rewards = torch.where(in_radius, similarities, torch.tensor(self.anchor_penalty_value, device=self.device))
        else:
            raise ValueError(f"Unknown anchor_penalty_mode: {self.anchor_penalty_mode}")
        
        return rewards
    
    def _compute_adaptive_threshold_reward(
        self,
        query_embeddings: torch.Tensor,
        user_ids: torch.Tensor,
    ) -> torch.Tensor:
        """
        ì ì‘í˜• ì„ê³„ê°’ ë³´ìƒ (Adaptive Threshold Reward) ê³„ì‚°
        
        ì „ëµ: ê³¼ê±° êµ¬ë§¤ ì•„ì´í…œë“¤ê³¼ì˜ í‰ê·  ìœ ì‚¬ë„(S_base)ë¥¼ ë™ì  ì„ê³„ê°’ìœ¼ë¡œ ì‚¬ìš©
        
        ìˆ˜ì‹:
            S_base = mean(CosSim(query, history_items))
            threshold = max(tau_min, S_base)
            R = 1 if CosSim(query, target) > threshold else 0
        
        ì˜ë¯¸: "ì ì–´ë„ ê³¼ê±°ì— ìƒ€ë˜ ë¬¼ê±´ë“¤ë³´ë‹¤ëŠ” ì •ë‹µì— ë” ë¹„ìŠ·í•´ì•¼ ì •ë‹µìœ¼ë¡œ ì¸ì •í•´ì£¼ê² ë‹¤"
        
        Args:
            query_embeddings: [batch_size, emb_dim] ì¿¼ë¦¬ ì„ë² ë”©
            user_ids: [batch_size] ì‚¬ìš©ì ID
            
        Returns:
            rewards: [batch_size] ì ì‘í˜• ì„ê³„ê°’ ë¦¬ì›Œë“œ (0 ë˜ëŠ” 1)
        """
        batch_size = len(user_ids)
        rewards = torch.zeros(batch_size, device=self.device)
        
        # L2 ì •ê·œí™” (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ìœ„í•´)
        query_embeddings_norm = torch.nn.functional.normalize(query_embeddings, p=2, dim=1)
        
        # íƒ€ê²Ÿ ì•„ì´í…œ ID ê°€ì ¸ì˜¤ê¸°
        if self.use_full_item_pool:
            target_item_ids = torch.tensor(
                [self.uid_2_target[uid] for uid in user_ids],
                device=self.device
            )  # [batch_size]
        else:
            batch_candidate_tensor = self.candidate_tensor[user_ids]  # [batch_size, k]
            target_item_ids = batch_candidate_tensor[:, 0]  # [batch_size]
        
        # íƒ€ê²Ÿ ì•„ì´í…œ ì„ë² ë”© ê°€ì ¸ì˜¤ê¸°
        target_item_embs = self.item_embeddings[target_item_ids]  # [batch_size, emb_dim]
        target_item_embs_norm = torch.nn.functional.normalize(target_item_embs, p=2, dim=1)
        
        # ì¿¼ë¦¬ì™€ íƒ€ê²Ÿ ì•„ì´í…œì˜ ìœ ì‚¬ë„ ê³„ì‚°
        query_target_similarity = (query_embeddings_norm * target_item_embs_norm).sum(dim=1)  # [batch_size]
        
        for i, uid in enumerate(user_ids):
            uid_item = uid.item() if isinstance(uid, torch.Tensor) else uid
            
            # 1. ê³¼ê±° êµ¬ë§¤ ì•„ì´í…œë“¤ì˜ ì„ë² ë”© ê°€ì ¸ì˜¤ê¸°
            if uid_item not in self.user_history_items:
                # íˆìŠ¤í† ë¦¬ê°€ ì—†ìœ¼ë©´ tau_minì„ ì„ê³„ê°’ìœ¼ë¡œ ì‚¬ìš©
                threshold = self.adaptive_tau_min
            else:
                history_item_ids = self.user_history_items[uid_item]  # [history_len]
                
                # 2. íˆìŠ¤í† ë¦¬ ì•„ì´í…œ ì„ë² ë”© ê°€ì ¸ì˜¤ê¸°
                history_item_embs = self.item_embeddings[history_item_ids]  # [history_len, emb_dim]
                history_item_embs_norm = torch.nn.functional.normalize(history_item_embs, p=2, dim=1)
                
                # 3. ì¿¼ë¦¬ì™€ íˆìŠ¤í† ë¦¬ ì•„ì´í…œë“¤ì˜ ìœ ì‚¬ë„ ê³„ì‚° í›„ í‰ê·  êµ¬í•˜ê¸° (S_base)
                query_history_similarities = torch.mm(
                    query_embeddings_norm[i].unsqueeze(0),  # [1, emb_dim]
                    history_item_embs_norm.T  # [emb_dim, history_len]
                ).squeeze(0)  # [history_len]
                
                s_base = query_history_similarities.mean().item()
                
                # 4. ë™ì  ì„ê³„ê°’ = max(tau_min, S_base)
                threshold = max(self.adaptive_tau_min, s_base)
            
            # 5. ì¿¼ë¦¬ì™€ ì •ë‹µ ì•„ì´í…œì˜ ìœ ì‚¬ë„ê°€ ì„ê³„ê°’ë³´ë‹¤ í° ë§Œí¼ ë¦¬ì›Œë“œ
            rewards[i] = max(0, query_target_similarity[i].item() - threshold)
        
        return rewards
    
    def _compute_history_proxy_threshold_reward(
        self,
        query_embeddings: torch.Tensor,
        user_ids: torch.Tensor,
    ) -> torch.Tensor:
        """
        íˆìŠ¤í† ë¦¬ Proxy ì„ê³„ê°’ ë³´ìƒ (History Proxy Threshold Reward) ê³„ì‚°
        
        ì „ëµ: íƒ€ê²Ÿê³¼ ê°€ì¥ ìœ ì‚¬í•œ íˆìŠ¤í† ë¦¬ ì•„ì´í…œ(proxy)ì„ ì‚¬ìš©í•˜ì—¬
              ë‚˜ë¨¸ì§€ íˆìŠ¤í† ë¦¬ ì•„ì´í…œë“¤ê³¼ì˜ í‰ê·  ìœ ì‚¬ë„ë¥¼ ì„ê³„ê°’ìœ¼ë¡œ ì„¤ì •
        
        ìˆ˜ì‹:
            proxy = argmax_i CosSim(target, history_i)
            other_history = history - {proxy}
            S_threshold = mean(CosSim(query, other_history))
            R = max(0, CosSim(query, proxy) - S_threshold)
        
        ì˜ë¯¸: "ì¿¼ë¦¬ê°€ ê³¼ê±° ì•„ì´í…œë“¤ í‰ê· ë³´ë‹¤ íƒ€ê²Ÿê³¼ ë¹„ìŠ·í•œ ì•„ì´í…œ(proxy)ì— ë” ê°€ê¹Œìš¸ìˆ˜ë¡ ë†’ì€ ë³´ìƒ"
        
        Args:
            query_embeddings: [batch_size, emb_dim] ì¿¼ë¦¬ ì„ë² ë”©
            user_ids: [batch_size] ì‚¬ìš©ì ID
            
        Returns:
            rewards: [batch_size] íˆìŠ¤í† ë¦¬ Proxy ì„ê³„ê°’ ë¦¬ì›Œë“œ
        """
        batch_size = len(user_ids)
        rewards = torch.zeros(batch_size, device=self.device)
        
        # L2 ì •ê·œí™” (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ìœ„í•´)
        query_embeddings_norm = torch.nn.functional.normalize(query_embeddings, p=2, dim=1)
        
        for i, uid in enumerate(user_ids):
            uid_item = uid.item() if isinstance(uid, torch.Tensor) else uid
            
            # 1. Proxy ì•„ì´í…œ ID ê°€ì ¸ì˜¤ê¸° (ì‚¬ì „ì— ê³„ì‚°ë¨)
            if uid_item not in self.user_history_proxy_items:
                # Proxyê°€ ì—†ìœ¼ë©´ ë¦¬ì›Œë“œ 0
                continue
            
            proxy_item_id = self.user_history_proxy_items[uid_item]
            history_item_ids = self.user_history_items[uid_item]  # [history_len]
            
            # 2. Proxy ì•„ì´í…œ ì„ë² ë”© ê°€ì ¸ì˜¤ê¸°
            proxy_item_emb = self.item_embeddings[proxy_item_id]  # [emb_dim]
            proxy_item_emb_norm = torch.nn.functional.normalize(proxy_item_emb.unsqueeze(0), p=2, dim=1)
            
            # 3. ì¿¼ë¦¬ì™€ Proxy ì•„ì´í…œì˜ ìœ ì‚¬ë„ ê³„ì‚°
            query_proxy_similarity = (query_embeddings_norm[i] * proxy_item_emb_norm.squeeze(0)).sum().item()
            
            # 4. ë‚˜ë¨¸ì§€ íˆìŠ¤í† ë¦¬ ì•„ì´í…œë“¤ê³¼ì˜ í‰ê·  ìœ ì‚¬ë„ ê³„ì‚° (ì„ê³„ê°’)
            # Proxyë¥¼ ì œì™¸í•œ íˆìŠ¤í† ë¦¬ ì•„ì´í…œë“¤
            other_history_mask = history_item_ids != proxy_item_id
            other_history_item_ids = history_item_ids[other_history_mask]
            
            if len(other_history_item_ids) > 0:
                # ë‚˜ë¨¸ì§€ íˆìŠ¤í† ë¦¬ ì•„ì´í…œ ì„ë² ë”©
                other_history_embs = self.item_embeddings[other_history_item_ids]  # [other_len, emb_dim]
                other_history_embs_norm = torch.nn.functional.normalize(other_history_embs, p=2, dim=1)
                
                # ì¿¼ë¦¬ì™€ ë‚˜ë¨¸ì§€ íˆìŠ¤í† ë¦¬ ì•„ì´í…œë“¤ì˜ ìœ ì‚¬ë„ í‰ê· 
                query_other_similarities = torch.mm(
                    query_embeddings_norm[i].unsqueeze(0),  # [1, emb_dim]
                    other_history_embs_norm.T  # [emb_dim, other_len]
                ).squeeze(0)  # [other_len]
                
                s_threshold = query_other_similarities.mean().item()
            else:
                # íˆìŠ¤í† ë¦¬ê°€ proxy í•˜ë‚˜ë¿ì¸ ê²½ìš°, ì„ê³„ê°’ 0
                s_threshold = 0.0
            
            # 5. ì¿¼ë¦¬-proxy ìœ ì‚¬ë„ê°€ ì„ê³„ê°’ì„ ë„˜ëŠ” ë§Œí¼ ë¦¬ì›Œë“œ
            rewards[i] = max(0, query_proxy_similarity - s_threshold)
        
        return rewards
    
    def __call__(
        self,
        generated_texts: List[str],
        user_ids: torch.Tensor,
        **kwargs
    ) -> torch.Tensor:
        """
        TRL í˜¸í™˜ ë¦¬ì›Œë“œ í•¨ìˆ˜
        
        Args:
            generated_texts: [batch_size] ìƒì„±ëœ í…ìŠ¤íŠ¸
            user_ids: [batch_size] ì‚¬ìš©ì ID (required)
            **kwargs: ì¶”ê°€ íŒŒë¼ë¯¸í„° (targets, histories, trainer_state ë“±)
        
        Returns:
            rewards: [batch_size] ë¦¬ì›Œë“œ ê°’ 
            
            If proxy_label_reward=True:
                rewards = base_reward + proxy_label_coef Ã— proxy_label_ndcg
                (íƒ€ê²Ÿê³¼ ìœ ì‚¬í•œ ì•„ì´í…œë“¤ë„ ë¶€ë¶„ì ìœ¼ë¡œ ì •ë‹µìœ¼ë¡œ ì·¨ê¸‰)
            
            If novelty_reward=True and novelty_annealing=False:
                rewards = novelty_coef Ã— (NDCG Ã— popularity_weight)
                (ì¸ê¸° ì—†ëŠ” ì•„ì´í…œì„ ë†’ì€ rankë¡œ ì˜ˆì¸¡í• ìˆ˜ë¡ ë†’ì€ ë³´ìƒ)
            
            If novelty_reward=True and novelty_annealing=True:
                novelty_ratio = current_step / max_steps (0 â†’ 1 ì„ í˜• ì¦ê°€)
                rewards = (1 - novelty_ratio) * base_reward + novelty_ratio * novelty_coef * novelty_reward
            
            Else:
                rewards = base_reward (NDCG/Hit/MRR ë“±)
        """
        
        # ìƒì„±ëœ í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜ (í•œ ë²ˆë§Œ ìˆ˜í–‰)
        query_embeddings = self._encode_texts(generated_texts)
        
        # ê¸°ì¡´ rank ê¸°ë°˜ ë¦¬ì›Œë“œ ê³„ì‚°
        # rank ê³„ì‚° (target + negatives)
        ranks, _ = self._compute_similarity_scores(query_embeddings, user_ids, return_scores=False)
        
        # ê¸°ë³¸ ë¦¬ì›Œë“œ íƒ€ì…ì— ë”°ë¼ ê³„ì‚°
        if self.reward_type == "ndcg":
            base_rewards = calculate_ndcg_from_rank(ranks, k=self.k)
        elif self.reward_type == "hit":
            base_rewards = calculate_hit_from_rank(ranks, k=self.k)
        elif self.reward_type == "mrr":
            base_rewards = calculate_mrr_from_rank(ranks, k=self.k)
        elif self.reward_type == "mixed":
            ndcg = calculate_ndcg_from_rank(ranks, k=self.k)
            hit = calculate_hit_from_rank(ranks, k=self.k)
            base_rewards = 0.7 * ndcg + 0.3 * hit
        else:
            raise ValueError(f"Unknown reward_type: {self.reward_type}")

        # Base rewardsë¥¼ wandb ë¡œê¹…ì„ ìœ„í•´ ì €ì¥
        self.last_base_rewards = base_rewards.detach().cpu()

        # Proxy label reward ì‚¬ìš© ì—¬ë¶€ì— ë”°ë¼ ë¶„ê¸°
        if self.proxy_label_reward:
            # Proxy label ë¦¬ì›Œë“œ ì‚¬ìš© ì‹œ: ê¸°ì¡´ base_reward + proxy_label_reward
            # ì˜ˆì¸¡ ì ìˆ˜ë„ í•¨ê»˜ ê³„ì‚° í•„ìš”
            _, predicted_scores = self._compute_similarity_scores(query_embeddings, user_ids, return_scores=True)
            # 2. Proxy label NDCG ê³„ì‚°
            proxy_label_rewards = self._compute_proxy_label_ndcg(query_embeddings, user_ids, predicted_scores)
            
            # Wandb ë¡œê¹…ì„ ìœ„í•´ ì €ì¥
            self.last_proxy_label_rewards = proxy_label_rewards.detach().cpu()
            
            # 3. ë‘ ë¦¬ì›Œë“œë¥¼ í•©ì‚°
            base_rewards = base_rewards + self.proxy_label_coef * proxy_label_rewards
        else:
            self.last_proxy_label_rewards = None
        
        
        # Novelty reward ì‚¬ìš© ì—¬ë¶€ì— ë”°ë¼ ë¶„ê¸°
        if self.novelty_reward and self.item_popularity_weights is not None:
            # Novelty reward ê³„ì‚°
            if self.use_full_item_pool:
                # ì „ì²´ ì•„ì´í…œ í’€ ì‚¬ìš© ì‹œ: uid_2_targetì—ì„œ ì§ì ‘ ê°€ì ¸ì˜¤ê¸°
                target_item_ids = torch.tensor(
                    [self.uid_2_target[uid] for uid in user_ids],
                    device=self.device
                )  # [batch_size]
            else:
                # Negative pool ì‚¬ìš© ì‹œ: candidate_tensorì—ì„œ ê°€ì ¸ì˜¤ê¸°
                batch_candidate_tensor = self.candidate_tensor[user_ids]  # [batch_size, k]
                target_item_ids = batch_candidate_tensor[:, 0]  # [batch_size] - targetì€ í•­ìƒ ì²« ë²ˆì§¸
            
            # Target itemì˜ popularity weight
            item_weights = self.item_popularity_weights[target_item_ids]  # [batch_size]
            
            # Novelty = NDCG Ã— popularity_weight
            novelty_rewards = calculate_novelty_ndcg(
                ranks, 
                item_weights=item_weights,
                k=self.k,
            )
            
            # Novelty annealing ì ìš© ì—¬ë¶€ì— ë”°ë¼ ë¶„ê¸°
            if self.novelty_annealing:
                # trainer_stateì—ì„œ í˜„ì¬ step ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                trainer_state = kwargs.get("trainer_state", None)
                
                if trainer_state is not None and hasattr(trainer_state, "global_step"):
                    current_step = trainer_state.global_step
                    # Novelty ratio: 0 (ì´ˆë°˜) â†’ 1 (í›„ë°˜) ì„ í˜• ì¦ê°€
                    novelty_ratio = min(1.0, current_step / max(1, self.max_steps))
                else:
                    # trainer_stateê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ 0.5 ì‚¬ìš© (ì¤‘ê°„ê°’)
                    novelty_ratio = 0.5
                
                # ì„ í˜• ë³´ê°„: (1-ratio)*base + ratio*novelty, novelty_coef ì ìš©
                rewards = (1.0 - novelty_ratio) * base_rewards + novelty_ratio * self.novelty_coef * novelty_rewards
            else:
                # Annealing ì—†ì´ novelty rewardë§Œ ì‚¬ìš©, novelty_coef ì ìš©
                rewards = self.novelty_coef * novelty_rewards
        else:
            # ê¸°ë³¸ ë¦¬ì›Œë“œ ì‚¬ìš©
            rewards = base_rewards
        
        # Target embedding ìœ ì‚¬ë„ ë¦¬ì›Œë“œ ì¶”ê°€
        if self.target_emb_reward and self.target_embeddings is not None:
            target_emb_rewards = self._compute_target_embedding_reward(query_embeddings, user_ids)
            self.last_target_emb_rewards = target_emb_rewards.detach().cpu()
            rewards = rewards + self.target_emb_coef * target_emb_rewards
        else:
            self.last_target_emb_rewards = None
        
        # InfoNCE ë¦¬ì›Œë“œ ì¶”ê°€
        if self.infonce_reward and self.infonce_item_embeddings is not None:
            infonce_rewards = self._compute_infonce_reward(query_embeddings, user_ids)
            self.last_infonce_rewards = infonce_rewards.detach().cpu()
            rewards = rewards + self.infonce_coef * infonce_rewards
        else:
            self.last_infonce_rewards = None
        
        # Anchor-Guided GRPO ë¦¬ì›Œë“œ ì¶”ê°€
        if self.anchor_reward and self.last_item_embeddings is not None:
            # trainer_stateì—ì„œ í˜„ì¬ step ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            trainer_state = kwargs.get("trainer_state", None)
            current_step = None
            if trainer_state is not None and hasattr(trainer_state, "global_step"):
                current_step = trainer_state.global_step
            
            anchor_rewards = self._compute_anchor_reward(query_embeddings, user_ids, current_step)
            self.last_anchor_rewards = anchor_rewards.detach().cpu()
            rewards = rewards + self.anchor_coef * anchor_rewards
        else:
            self.last_anchor_rewards = None
        
        # Adaptive Threshold ë¦¬ì›Œë“œ ì¶”ê°€
        if self.adaptive_threshold_reward and self.user_history_items is not None:
            adaptive_threshold_rewards = self._compute_adaptive_threshold_reward(query_embeddings, user_ids)
            self.last_adaptive_threshold_rewards = adaptive_threshold_rewards.detach().cpu()
            rewards = rewards + self.adaptive_threshold_coef * adaptive_threshold_rewards
        else:
            self.last_adaptive_threshold_rewards = None
        
        # History Proxy Threshold ë¦¬ì›Œë“œ ì¶”ê°€
        if self.history_proxy_threshold_reward and self.user_history_proxy_items is not None:
            history_proxy_threshold_rewards = self._compute_history_proxy_threshold_reward(query_embeddings, user_ids)
            self.last_history_proxy_threshold_rewards = history_proxy_threshold_rewards.detach().cpu()
            rewards = rewards + self.history_proxy_threshold_coef * history_proxy_threshold_rewards
        else:
            self.last_history_proxy_threshold_rewards = None
        
        # ì •ê·œí™” (optional)
        if self.normalize and rewards.std() > 0:
            rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-8)
        
        return rewards
    
    def get_reward_breakdown(self) -> Dict[str, torch.Tensor]:
        """
        ë§ˆì§€ë§‰ ê³„ì‚°ëœ ë¦¬ì›Œë“œì˜ êµ¬ì„± ìš”ì†Œë“¤ì„ ë°˜í™˜
        Wandb ë¡œê¹… ë“±ì— ì‚¬ìš©
        
        Returns:
            Dict[str, torch.Tensor]: ë¦¬ì›Œë“œ êµ¬ì„± ìš”ì†Œë“¤
                - "base_reward": ê¸°ë³¸ ë¦¬ì›Œë“œ (NDCG/Hit/MRR)
                - "proxy_label_reward": Proxy label ë¦¬ì›Œë“œ (ì‚¬ìš© ì‹œ)
                - "target_emb_reward": Target embedding ìœ ì‚¬ë„ ë¦¬ì›Œë“œ (ì‚¬ìš© ì‹œ)
                - "infonce_reward": InfoNCE ë¦¬ì›Œë“œ (ì‚¬ìš© ì‹œ)
                - "anchor_reward": Anchor-Guided GRPO ë¦¬ì›Œë“œ (ì‚¬ìš© ì‹œ)
                - "adaptive_threshold_reward": Adaptive Threshold ë¦¬ì›Œë“œ (ì‚¬ìš© ì‹œ)
                - "history_proxy_threshold_reward": History Proxy Threshold ë¦¬ì›Œë“œ (ì‚¬ìš© ì‹œ)
        """
        breakdown = {}
        
        if self.last_base_rewards is not None:
            breakdown["base_reward"] = self.last_base_rewards
        
        if self.last_proxy_label_rewards is not None:
            breakdown["proxy_label_reward"] = self.last_proxy_label_rewards
        
        if self.last_target_emb_rewards is not None:
            breakdown["target_emb_reward"] = self.last_target_emb_rewards
        
        if self.last_infonce_rewards is not None:
            breakdown["infonce_reward"] = self.last_infonce_rewards
        
        if self.last_anchor_rewards is not None:
            breakdown["anchor_reward"] = self.last_anchor_rewards
        
        if self.last_adaptive_threshold_rewards is not None:
            breakdown["adaptive_threshold_reward"] = self.last_adaptive_threshold_rewards
        
        if self.last_history_proxy_threshold_rewards is not None:
            breakdown["history_proxy_threshold_reward"] = self.last_history_proxy_threshold_rewards
        
        return breakdown
