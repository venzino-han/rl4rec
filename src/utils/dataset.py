"""
Dataset utilities for RL4Rec
ì¶”ì²œ ì‹œìŠ¤í…œ í•™ìŠµì„ ìœ„í•œ ë°ì´í„°ì…‹ ë° ë°ì´í„°ë¡œë” ìœ í‹¸ë¦¬í‹°
"""

import os
import json
import pickle
import numpy as np
import argparse
import pandas as pd
from pathlib import Path
from typing import List, Dict, Optional, Tuple

import torch
from torch.utils.data import Dataset, DataLoader

from utils.prompt_templates import PROMPT_TEMPLATES, PROMPT_TEMPLATES_YELP

class PromptGenerator:
    """
    ì‚¬ìš©ì ì‹œí€€ìŠ¤ë¡œë¶€í„° í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” í´ë˜ìŠ¤
    
    ì§€ì›í•˜ëŠ” í”„ë¡¬í”„íŠ¸ íƒ€ì…:
    - 'preference': ì‚¬ìš©ì ì„ í˜¸ë„ ë¬˜ì‚¬
    - 'next_item': ë‹¤ìŒ ì•„ì´í…œ ì˜ˆì¸¡
    - 'recommendation': ì¶”ì²œ ì•„ì´í…œ ìƒì„±
    - 'user_profile': ì‚¬ìš©ì í”„ë¡œí•„ ìƒì„±
    - 'recent_preference': ìµœê·¼ ì„ í˜¸ë„ ë¬˜ì‚¬
    """
    
    def __init__(
        self,
        item_metadata: Dict,
        data_name: str = None,
        prompt_type: str = 'seq_rec',
        use_brand: bool = True,
        use_category: bool = True,
        use_description: bool = False,
        use_features: bool = False,
        use_last_item: bool = False,
        use_date: bool = True,
        max_history_len: int = 8,
        history_text_max_length: int = 100,
        use_reviews: bool = False,
        days_filter: int = None,
        tokenizer = None,
        apply_chat_template: bool = True,
        emphasize_recent_item: bool = False,
        include_target_date: bool = False,
        use_sasrec: bool = False,
        sasrec_top_k: int = 5,
        use_relative_date: bool = False,
    ):
        """
        Args:
            item_metadata: ì•„ì´í…œ ë©”íƒ€ë°ì´í„° ë”•ì…”ë„ˆë¦¬
            data_name: ë°ì´í„°ì…‹ ì´ë¦„ (ë‚ ì§œ ì •ë³´ ë¡œë“œì— ì‚¬ìš©)
            prompt_type: í”„ë¡¬í”„íŠ¸ íƒ€ì… ('preference', 'next_item', 'recommendation', 'user_profile', 'recent_preference', 'reasoning')
            use_brand: ë¸Œëœë“œ ì •ë³´ í¬í•¨ ì—¬ë¶€
            use_category: ì¹´í…Œê³ ë¦¬ ì •ë³´ í¬í•¨ ì—¬ë¶€
            use_description: ì„¤ëª… ì •ë³´ í¬í•¨ ì—¬ë¶€
            use_features: íŠ¹ì§• ì •ë³´ í¬í•¨ ì—¬ë¶€
            use_last_item: ë§ˆì§€ë§‰ ì•„ì´í…œ ê°•ì¡° ì—¬ë¶€
            use_date: ë‚ ì§œ ì •ë³´ í¬í•¨ ì—¬ë¶€ (íˆìŠ¤í† ë¦¬ ë° ìµœê·¼ êµ¬ë§¤ ê°•ì¡°ì— ì‚¬ìš©)
            max_history_len: ìµœëŒ€ íˆìŠ¤í† ë¦¬ ê¸¸ì´
            history_text_max_length: íˆìŠ¤í† ë¦¬ í…ìŠ¤íŠ¸ ìµœëŒ€ ë‹¨ì–´ ìˆ˜ (review textì—ë„ ì ìš©)
            use_reviews: ë¦¬ë·° í…ìŠ¤íŠ¸ í¬í•¨ ì—¬ë¶€
            days_filter: ìµœê·¼ Nì¼ ì´ë‚´ì˜ ë¦¬ë·°ë§Œ í¬í•¨ (Noneì´ë©´ í•„í„°ë§ ì•ˆí•¨)
            tokenizer: í† í¬ë‚˜ì´ì € (ì±— í…œí”Œë¦¿ ì ìš©ì— í•„ìš”, ì„ íƒì )
            apply_chat_template: ì±— í…œí”Œë¦¿ ì ìš© ì—¬ë¶€
            emphasize_recent_item: ìµœê·¼ êµ¬ë§¤ ì•„ì´í…œì„ ìƒì„¸í•˜ê²Œ ê°•ì¡°í• ì§€ ì—¬ë¶€ ("This user's most recent purchase is..." í˜•ì‹, use_dateê°€ Trueë©´ êµ¬ë§¤ ë‚ ì§œë„ í¬í•¨)
            include_target_date: íƒ€ê²Ÿ/ë ˆì´ë¸” ì•„ì´í…œì˜ êµ¬ë§¤ ë‚ ì§œë¥¼ í”„ë¡¬í”„íŠ¸ ë§ˆì§€ë§‰ì— í¬í•¨í• ì§€ ì—¬ë¶€
            use_sasrec: SASRec ì¶”ì²œ ê²°ê³¼ë¥¼ í”„ë¡¬í”„íŠ¸ì— í¬í•¨í• ì§€ ì—¬ë¶€
            sasrec_top_k: SASRec ì¶”ì²œ ê²°ê³¼ì—ì„œ ìƒìœ„ Kê°œ ì•„ì´í…œë§Œ í¬í•¨
            use_relative_date: ìƒëŒ€ ë‚ ì§œ í‘œê¸° ì‚¬ìš© ì—¬ë¶€ (Trueë©´ íƒ€ê²Ÿ ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ "(D-10)", "(D-20)" í˜•ì‹ìœ¼ë¡œ í‘œì‹œ)
        """
        self.item_metadata = item_metadata
        self.data_name = data_name
        self.use_brand = use_brand
        self.use_category = use_category
        self.use_description = use_description
        self.use_features = use_features
        self.use_last_item = emphasize_recent_item
        self.use_date = use_date
        self.max_history_len = max_history_len
        self.history_text_max_length = history_text_max_length
        self.use_reviews = use_reviews
        self.days_filter = days_filter
        self.tokenizer = tokenizer
        self.apply_chat_template = apply_chat_template
        self.include_target_date = include_target_date
        self.use_sasrec = use_sasrec
        self.sasrec_top_k = sasrec_top_k
        self.use_relative_date = use_relative_date
        
        # ë°ì´í„°ì…‹ì— ë”°ë¼ ì ì ˆí•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„ íƒ
        if data_name == 'yelp':
            self.templates = PROMPT_TEMPLATES_YELP
            print(f"âœ“ Using PROMPT_TEMPLATES_YELP for data_name='{data_name}'")
        else:
            self.templates = PROMPT_TEMPLATES
        
        # í”„ë¡¬í”„íŠ¸ íƒ€ì… ì„¤ì •
        if prompt_type not in self.templates:
            print(f"âš ï¸  Unknown prompt type '{prompt_type}'. Available types: {list(self.templates.keys())}")
            print(f"   Using default 'recent_preference' type.")
            self.prompt_type = 'recent_preference'
        else:
            self.prompt_type = prompt_type
            print(f"âœ“ Using prompt type: '{self.prompt_type}'")
        
        # user2reviews_with_date.json ë¡œë“œ
        self.user_reviews_with_date = {}
        if data_name:
            date_file_path = f"data/{data_name}/user2reviews_with_date.json"
            if os.path.exists(date_file_path):
                print(f"Loading date information from: {date_file_path}")
                with open(date_file_path, 'r') as f:
                    self.user_reviews_with_date = json.load(f)
                print(f"âœ“ Loaded date information for {len(self.user_reviews_with_date)} users")
            else:
                print(f"âš ï¸  Date file not found: {date_file_path}. Dates will not be included.")
                self.use_date = False
        
        # SASRec ì¶”ì²œ ê²°ê³¼ ë¡œë“œ
        self.sasrec_predictions = {}
        if self.use_sasrec and data_name:
            print(f"ğŸ” SASRec recommendations will be loaded per split in RecommendationDataset")
    
    def generate_prompt(self, item_ids: List[int], user_id: Optional[int] = None, target_item_id: Optional[int] = None, sasrec_items: Optional[List[int]] = None) -> str:
        """
        ì‚¬ìš©ì ì‹œí€€ìŠ¤ë¡œë¶€í„° í”„ë¡¬í”„íŠ¸ ìƒì„±
        
        Args:
            item_ids: ì•„ì´í…œ ID ë¦¬ìŠ¤íŠ¸
            user_id: ì‚¬ìš©ì ID (ë‚ ì§œ ì •ë³´ ì¡°íšŒìš©, ì„ íƒì )
            target_timestamp: íƒ€ê²Ÿ íƒ€ì„ìŠ¤íƒ¬í”„ (days_filter ì ìš©ì‹œ ê¸°ì¤€, ì„ íƒì )
            target_item_id: íƒ€ê²Ÿ/ë ˆì´ë¸” ì•„ì´í…œ ID (íƒ€ê²Ÿ ë‚ ì§œ í¬í•¨ìš©, ì„ íƒì )
            sasrec_items: SASRec ì¶”ì²œ ì•„ì´í…œ ID ë¦¬ìŠ¤íŠ¸ (ì„ íƒì )
        
        Returns:
            ìƒì„±ëœ í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´
        """
        # ì‚¬ìš©ìì˜ ë¦¬ë·° ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        user_reviews = []
        if user_id is not None:
            user_id_str = str(user_id)
            user_reviews = self.user_reviews_with_date.get(user_id_str, [])
        
        # ì•„ì´í…œ IDë¥¼ í‚¤ë¡œ í•˜ëŠ” ë¦¬ë·° ë§¤í•‘ ìƒì„±
        item_to_review = {}
        if user_reviews:
            for review in user_reviews:
                item_id = int(review.get('item_id', -1))
                if item_id != -1:
                    item_to_review[item_id] = review
        
        # íˆìŠ¤í† ë¦¬ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        history_text_list = []
        
        # íƒ€ê²Ÿ íƒ€ì„ìŠ¤íƒ¬í”„ ê°€ì ¸ì˜¤ê¸° (ìƒëŒ€ ë‚ ì§œ ê³„ì‚° ë° days_filterì— ì‚¬ìš©)
        target_timestamp = 0
        if target_item_id is not None and target_item_id in item_to_review:
            target_timestamp = int(item_to_review[target_item_id].get('timestamp', 0))
        
        # ê° ì•„ì´í…œ ì²˜ë¦¬
        for idx, item_id in enumerate(item_ids):
            item_data = self.item_metadata.get(item_id)
            if item_data is None:
                # ë©”íƒ€ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° ìŠ¤í‚µ
                print(f"âš ï¸  Item metadata not found for item {item_id}")
                continue

            # ì‹œê°„ í•„í„°ë§ (days_filterê°€ ì„¤ì •ë˜ì–´ ìˆê³  target_timestampê°€ ì£¼ì–´ì§„ ê²½ìš°)
            if self.days_filter is not None and target_timestamp > 0:
                review = item_to_review.get(item_id)
                if review:
                    timestamp = int(review.get('timestamp', 0))
                    if target_timestamp - timestamp > self.days_filter * 24 * 60 * 60:
                        # print(f"âš ï¸  Item timestamp is too old for item {item_id}")
                        continue
            
            item_title = item_data.get('title', 'Unknown Item')
            item_brand = item_data.get('brand', 'Unknown Brand')
            item_categories = item_data.get('category', 'Unknown Category')
            item_description = item_data.get('description', '')
        
            item_history_text = ""
            # ë‚ ì§œ ì •ë³´ ì¶”ê°€
            if self.use_date and item_id in item_to_review:
                if self.use_relative_date and target_timestamp > 0:
                    # ìƒëŒ€ ë‚ ì§œ ê³„ì‚° (D-N í˜•ì‹)
                    item_timestamp = int(item_to_review[item_id].get('timestamp', 0))
                    if item_timestamp > 0:
                        days_diff = (target_timestamp - item_timestamp) // (24 * 60 * 60)
                        item_history_text += f"Date: (D-{days_diff})\n"
                else:
                    # ì ˆëŒ€ ë‚ ì§œ í‘œì‹œ
                    item_date = item_to_review[item_id].get('date', '')
                    if item_date:
                        item_history_text += f"Date: {item_date}\n"
            
            # ê¸°ë³¸ íˆìŠ¤í† ë¦¬ í¬ë§·
            item_history_text += f"Item Title: {item_title}\n"
            
            if self.use_brand:
                item_history_text += f"Brand: {item_brand}\n"
            
            if self.use_category:
                item_history_text += f"Categories: {item_categories}\n"
            
            if self.use_description and item_description:
                item_description = item_description.replace("\n", " ")
                if len(item_description.split()) > self.history_text_max_length:
                    item_description = " ".join(
                        item_description.split()[:self.history_text_max_length]
                    ) + "..."
                item_history_text += f"Description: {item_description}\n"
            
            # ë¦¬ë·° í…ìŠ¤íŠ¸ ì¶”ê°€
            if self.use_reviews and item_id in item_to_review:
                review_text = item_to_review[item_id].get('text', '')
                # limit review text words
                if review_text and len(review_text.split()) > self.history_text_max_length:
                    review_text = " ".join(review_text.split()[:self.history_text_max_length])
                if review_text:
                    item_history_text += f"Review:\n{review_text}\n"
            
            history_text_list.append(item_history_text)
        
        # íˆìŠ¤í† ë¦¬ê°€ ë¹„ì–´ìˆëŠ” ê²½ìš° ë§ˆì§€ë§‰ ì•„ì´í…œì´ë¼ë„ í¬í•¨
        if len(history_text_list) == 0 and len(item_ids) > 0:
            last_item_id = item_ids[-1]

            item_title = self.item_metadata.get(last_item_id, {}).get('title', 'Unknown Item')
            item_brand = self.item_metadata.get(last_item_id, {}).get('brand', 'Unknown Brand')
            item_categories = self.item_metadata.get(last_item_id, {}).get('category', 'Unknown Category')
            item_description = self.item_metadata.get(last_item_id, {}).get('description', '')

            item_history_text = ""
            if self.use_date and last_item_id in item_to_review:
                if self.use_relative_date and target_timestamp > 0:
                    # ìƒëŒ€ ë‚ ì§œ ê³„ì‚° (D-N í˜•ì‹)
                    item_timestamp = int(item_to_review[last_item_id].get('timestamp', 0))
                    if item_timestamp > 0:
                        days_diff = (target_timestamp - item_timestamp) // (24 * 60 * 60)
                        item_history_text += f"Date: (D-{days_diff})\n"
                else:
                    # ì ˆëŒ€ ë‚ ì§œ í‘œì‹œ
                    item_date = item_to_review[last_item_id].get('date', '')
                    if item_date:
                        item_history_text += f"Date: {item_date}\n"

            item_history_text += f"Item Title: {item_title}\n"
            if self.use_brand:
                item_history_text += f"Brand: {item_brand}\n"
            if self.use_category:
                item_history_text += f"Categories: {item_categories}\n"
            
            if self.use_reviews and last_item_id in item_to_review:
                review_text = item_to_review[last_item_id].get('text', '')
                if review_text and len(review_text.split()) > self.history_text_max_length:
                    review_text = " ".join(review_text.split()[:self.history_text_max_length])
                if review_text:
                    item_history_text += f"Review:\n{review_text}\n"
            
            history_text_list.append(item_history_text)
        
        # íˆìŠ¤í† ë¦¬ ê¸¸ì´ ì œì•½ ì ìš©
        if len(history_text_list) > self.max_history_len:
            history_text_list = history_text_list[-self.max_history_len:]
        
        # ìµœì¢… íˆìŠ¤í† ë¦¬ í…ìŠ¤íŠ¸ êµ¬ì„±
        history_text = "\n\n".join(
            f"{i+1}. {history}" for i, history in enumerate(history_text_list)
        )
        
        # ë§ˆì§€ë§‰ ì•„ì´í…œ ê°•ì¡°
        if self.use_last_item and len(item_ids) > 0:
            last_item = self.item_metadata.get(item_ids[-1], {})
            last_item_title = last_item.get('title', 'Unknown Item')
            history_text += f"\n\n`{last_item_title}` is the most recently purchased item."
        
        # ì„ íƒëœ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ê°€ì ¸ì˜¤ê¸°
        template = self.templates[self.prompt_type]
        
        # íƒ€ê²Ÿ ì•„ì´í…œ ë‚ ì§œ ì¶”ê°€
        target_date = ""
        if self.include_target_date and target_item_id is not None:
            if target_item_id in item_to_review:
                target_date = item_to_review[target_item_id].get('date', '')
            
            if target_date:
                target_date = f"- **Target Purchase Date:**: {target_date}\n"
        
        # SASRec ì¶”ì²œ ê²°ê³¼ ì„¹ì…˜ ìƒì„±
        sasrec_section = ""
        if self.use_sasrec and sasrec_items and len(sasrec_items) > 0:
            # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì—ì„œ sasrec_sectionì´ ìˆëŠ”ì§€ í™•ì¸
            template = self.templates[self.prompt_type]
            if 'sasrec_section' in template:
                sasrec_section = template['sasrec_section']
                
                # SASRec ì¶”ì²œ ì•„ì´í…œë“¤ì˜ ì •ë³´ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
                sasrec_text_list = []
                for idx, item_id in enumerate(sasrec_items[:self.sasrec_top_k]):
                    item_data = self.item_metadata.get(item_id)
                    if item_data is None:
                        continue
                    
                    item_title = item_data.get('title', 'Unknown Item')
                    # limit title length to 100 words
                    item_title = " ".join(item_title.split()[:100])
                    
                    sasrec_item_text = f"{idx+1}. {item_title}"
                    sasrec_text_list.append(sasrec_item_text)
                
                if sasrec_text_list:
                    sasrec_section += "\n" + "\n".join(sasrec_text_list) + "\n\n"
        
        # ìµœì¢… í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = (
            f"{template['head']}\n\n"
            f"{target_date}\n"
            f"- **User Purchase History:**\n"
            f"{history_text}\n"
            f"{sasrec_section}"
            f"{template['tail']}\n"
        )
        
        # ì±— í…œí”Œë¦¿ ì ìš©
        if self.apply_chat_template and self.tokenizer is not None:
            messages = [{"role": "user", "content": prompt}]
            prompt = self.tokenizer.apply_chat_template(
                messages, 
                tokenize=False, 
                add_generation_prompt=True
            )
        
        return prompt


class RecommendationDataset(Dataset):
    """
    ì¶”ì²œ ì‹œìŠ¤í…œìš© ë°ì´í„°ì…‹
    ì‚¬ìš©ì íˆìŠ¤í† ë¦¬, íƒ€ê²Ÿ ì•„ì´í…œ, í”„ë¡¬í”„íŠ¸ë¥¼ í¬í•¨
    """
    
    def __init__(
        self,
        data_name: str,
        item_metadata: Dict,
        prompt_generator: PromptGenerator,
        split: str = "train",
        num_negs: int = 0,
        num_items: Optional[int] = None,
    ):
        """
        Args:
            data_name: ë°ì´í„°ì…‹ ì´ë¦„
            item_metadata: ì•„ì´í…œ ë©”íƒ€ë°ì´í„° ë”•ì…”ë„ˆë¦¬
            prompt_generator: í”„ë¡¬í”„íŠ¸ ìƒì„±ê¸°
            split: ë°ì´í„° ë¶„í•  ("train", "valid", "test")
            num_negs: ì‚¬ì „ ìƒ˜í”Œë§í•  negative ì•„ì´í…œ ìˆ˜ (0ì´ë©´ ë¹„í™œì„±í™”)
            num_items: ì „ì²´ ì•„ì´í…œ ìˆ˜ (negative samplingì— í•„ìš”)
        """
        self.data_name = data_name
        self.item_metadata = item_metadata
        self.prompt_generator = prompt_generator
        self.split = split
        self.num_negs = num_negs
        self.num_items = num_items
        
        sequential_file = f"data/{data_name}/sequential_data.txt"
        self._load_real_data(sequential_file, split)
        
        # SASRec ì¶”ì²œ ê²°ê³¼ ë¡œë“œ (use_sasrecì´ Trueì¸ ê²½ìš°ì—ë§Œ)
        self.sasrec_predictions = {}
        if self.prompt_generator.use_sasrec:
            sasrec_file = f"sasrec_results/SASRec_{data_name}_{split}_topk_prediction.json"
            if os.path.exists(sasrec_file):
                print(f"ğŸ“¦ Loading SASRec predictions from: {sasrec_file}")
                with open(sasrec_file, 'r') as f:
                    sasrec_data = json.load(f)
                    # Convert keys to int and extract only item IDs (first element of each [item_id, score] pair)
                    self.sasrec_predictions = {
                        int(k): [item[0] for item in v] 
                        for k, v in sasrec_data.items()
                    }
                print(f"âœ“ Loaded SASRec predictions for {len(self.sasrec_predictions)} users")
            else:
                print(f"âš ï¸  SASRec prediction file not found: {sasrec_file}. SASRec recommendations will not be included.")
                self.prompt_generator.use_sasrec = False
        
        # í”„ë¡¬í”„íŠ¸ ë¯¸ë¦¬ ìƒì„± (ì´ˆê¸°í™” ì‹œì )
        print(f"âœï¸  Pre-generating prompts for {len(self.user_ids)} users...")
        self.prompt_dict = {}
        for user_id in self.user_ids:
            history = self.history_dict[user_id]
            target_item_id = self.target_dict[user_id]
            
            # SASRec ì¶”ì²œ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸° (ìˆëŠ” ê²½ìš°)
            sasrec_items = self.sasrec_predictions.get(user_id, []) if self.prompt_generator.use_sasrec else None
            
            self.prompt_dict[user_id] = self.prompt_generator.generate_prompt(
                history, 
                user_id=user_id, 
                target_item_id=target_item_id,
                sasrec_items=sasrec_items,
            )

        # print sample prompts
        for user_id in [10, 20, 30]:
            print(f"User {user_id}: \n{self.prompt_dict[user_id]}")
            print("-" * 100)
        
        # Negative items ë¯¸ë¦¬ ìƒ˜í”Œë§ (ì´ˆê¸°í™” ì‹œì )
        if self.num_negs > 0:
            if self.num_items is None:
                raise ValueError("num_items must be provided when num_negs > 0")
            self._load_negative_items()
        
        print(f"âœ“ {split.upper()} Dataset loaded: {len(self.user_ids)} users")
    
    def _load_real_data(
        self,
        sequential_file: str,
        split: str,
    ):
        """ì‹¤ì œ ë°ì´í„° ë¡œë“œ"""
        all_user_ids = []
        all_history = {}
        all_targets = {}
        
        if split == "train":
            target_index = -3
        elif split == "valid":
            target_index = -2
        elif split == "test":
            target_index = -1
        else:
            raise ValueError(f"Invalid split: {split}")
        
        with open(sequential_file, "r") as f:
            for line in f:
                parts = [int(p) for p in line.strip().split()]
                user_id = parts[0]
                history = parts[1:target_index]
                target = parts[target_index]
                
                all_user_ids.append(user_id)
                all_history[user_id] = history
                all_targets[user_id] = target
        
        self.user_ids = all_user_ids
        self.history_dict = all_history
        self.target_dict = all_targets
    
    def filter_by_rank(self, csv_path: str, rank_min: Optional[int] = None, rank_max: Optional[int] = None):
        """
        í‰ê°€ ê²°ê³¼ CSVì˜ rank ë²”ìœ„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë°ì´í„°ì…‹ í•„í„°ë§
        
        Args:
            csv_path: í‰ê°€ ê²°ê³¼ CSV íŒŒì¼ ê²½ë¡œ (user_id, rank ì»¬ëŸ¼ í¬í•¨)
            rank_min: ìµœì†Œ rank (Noneì´ë©´ ì œí•œ ì—†ìŒ)
            rank_max: ìµœëŒ€ rank (Noneì´ë©´ ì œí•œ ì—†ìŒ)
        """
        # if not os.path.exists(csv_path):
        #     raise FileNotFoundError(f"CSV file not found: {csv_path}")
        
        print(f"\n{'='*80}")
        print(f"ğŸ” Filtering dataset by rank range")
        print(f"{'='*80}")
        print(f"  CSV file: {csv_path}")
        print(f"  Rank range: [{rank_min if rank_min is not None else 'None'}, {rank_max if rank_max is not None else 'None'}]")
        print(f"  Original size: {len(self.user_ids)} users")
        
        # CSV ë¡œë“œ
        df = pd.read_csv(csv_path)
        
        # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
        if 'user_id' not in df.columns or 'rank' not in df.columns:
            raise ValueError(f"CSV must contain 'user_id' and 'rank' columns. Found: {df.columns.tolist()}")
        
        # rank ë²”ìœ„ í•„í„°ë§
        mask = pd.Series([True] * len(df))
        if rank_min is not None:
            mask &= (df['rank'] >= rank_min)
        if rank_max is not None:
            mask &= (df['rank'] <= rank_max)
        
        filtered_df = df[mask]
        
        # í•„í„°ë§ëœ user_id ì„¸íŠ¸
        filtered_user_ids = set(filtered_df['user_id'].tolist())
        
        print(f"  Filtered users from CSV: {len(filtered_user_ids)} users")
        
        # ë°ì´í„°ì…‹ í•„í„°ë§
        original_count = len(self.user_ids)
        self.user_ids = [uid for uid in self.user_ids if uid in filtered_user_ids]
        
        # íˆìŠ¤í† ë¦¬ì™€ íƒ€ê²Ÿë„ í•„í„°ë§
        filtered_history = {uid: hist for uid, hist in self.history_dict.items() if uid in filtered_user_ids}
        filtered_target = {uid: tgt for uid, tgt in self.target_dict.items() if uid in filtered_user_ids}
        
        self.history_dict = filtered_history
        self.target_dict = filtered_target
        
        # í”„ë¡¬í”„íŠ¸ë„ í•„í„°ë§ (ì´ë¯¸ ìƒì„±ëœ ê²½ìš°)
        if hasattr(self, 'prompt_dict'):
            self.prompt_dict = {uid: prompt for uid, prompt in self.prompt_dict.items() if uid in filtered_user_ids}
        
        # negative itemsë„ í•„í„°ë§ (ìˆëŠ” ê²½ìš°)
        if hasattr(self, 'neg_items_dict'):
            self.neg_items_dict = {uid: items for uid, items in self.neg_items_dict.items() if uid in filtered_user_ids}
        
        print(f"  Filtered size: {len(self.user_ids)} users (removed {original_count - len(self.user_ids)} users)")
        print(f"{'='*80}\n")
    
    def _load_negative_items(self):
        """ê° ì‚¬ìš©ìë³„ë¡œ negative items ì‚¬ì „ ìƒ˜í”Œë§"""
        negative_file = Path("data") / self.data_name / "negative.txt"
        if not negative_file.exists():
            raise FileNotFoundError(f"Negative pool file not found: {negative_file}")
        
        print(f"ğŸ“¦ Loading negative pool from: {negative_file}")
        negative_pool = {}
        
        with open(negative_file, 'r') as f:
            for line in f:
                parts = line.strip().split()
                user_id = int(parts[0])
                neg_items = [int(item_id) for item_id in parts[1:]]
                negative_pool[user_id] = neg_items[:self.num_negs]

        self.neg_items_dict = negative_pool
        
    
    def __len__(self):
        return len(self.user_ids)
    
    def __getitem__(self, idx):
        user_id = self.user_ids[idx]
        history = self.history_dict[user_id]
        target = self.target_dict[user_id]
        
        # ë¯¸ë¦¬ ìƒì„±ëœ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
        prompt = self.prompt_dict[user_id]
        
        result = {
            "prompt": prompt,
            "history": history,
            "target": target,
            "user_id": user_id,
        }
        
        # Negative itemsê°€ ìˆìœ¼ë©´ ì¶”ê°€
        if self.num_negs > 0:
            result["neg_items"] = self.neg_items_dict[user_id]
        
        return result


def collate_fn(batch):
    """
    DataLoaderìš© collate function
    """
    return {
        "queries": [item["query"] for item in batch],
        "histories": [item["history"] for item in batch],
        "targets": [item["target"] for item in batch],
        "user_ids": [item["user_id"] for item in batch],
    }


def load_item_metadata(data_name: str, data_dir: str = "data") -> Dict:
    """
    ì•„ì´í…œ ë©”íƒ€ë°ì´í„° ë¡œë“œ
    
    Args:
        data_name: ë°ì´í„°ì…‹ ì´ë¦„ (e.g., "beauty")
        data_dir: ë°ì´í„° ë””ë ‰í† ë¦¬
    
    Returns:
        ì•„ì´í…œ ë©”íƒ€ë°ì´í„° ë”•ì…”ë„ˆë¦¬
    """
    # ë©”íƒ€ë°ì´í„° íŒŒì¼ ê²½ë¡œ ì‹œë„
    possible_paths = [
        f"{data_dir}/{data_name}/meta_text.json",
    ]
    
    item_metadata = {}
    
    for path in possible_paths:
        if os.path.exists(path):
            print(f"Loading item metadata from: {path}")
            
            if path.endswith('.json'):
                with open(path, 'r') as f:
                    data = json.load(f)
                    # Keyë¥¼ intë¡œ ë³€í™˜
                    item_metadata = {int(k): v for k, v in data.items()}
            elif path.endswith('.pkl'):
                with open(path, 'rb') as f:
                    item_metadata = pickle.load(f)
            
            print(f"âœ“ Loaded {len(item_metadata)} items")
            return item_metadata
    
    # ë©”íƒ€ë°ì´í„° íŒŒì¼ì„ ì°¾ì§€ ëª»í•œ ê²½ìš° ê²½ê³ 
    print(f"âš ï¸  Item metadata file not found. Using dummy metadata.")
    return {}


def create_dataloaders(
    args: argparse.Namespace,
    tokenizer: Optional = None,
    apply_chat_template: bool = True,
) -> Tuple[DataLoader, DataLoader, DataLoader, PromptGenerator, Dict]:
    """
    Train/Valid/Test DataLoader ìƒì„±
    
    Args:
        args: argparse.Namespace
    
    Returns:
        (train_dataloader, valid_dataloader, test_dataloader, prompt_generator, item_metadata)
    """
    # ì•„ì´í…œ ë©”íƒ€ë°ì´í„° ë¡œë“œ
    print(f"ğŸ“¦ Loading item metadata...")
    item_metadata = load_item_metadata(args.data_name)
    
    # num_items ê°€ì ¸ì˜¤ê¸° (argsì— ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ë©”íƒ€ë°ì´í„°ì—ì„œ ì¶”ì¶œ)
    num_items = getattr(args, 'num_items', None)
    if num_items is None and len(item_metadata) > 0:
        num_items = max(item_metadata.keys()) + 1
        print(f"  Inferred num_items from metadata: {num_items}")
    
    # num_negs ê°€ì ¸ì˜¤ê¸° (argsì— ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ 0)
    num_negs = getattr(args, 'num_negs', 0)
    
    # í”„ë¡¬í”„íŠ¸ ìƒì„±ê¸°
    print(f"âœï¸  Creating prompt generator...")
    
    # use_date íŒŒë¼ë¯¸í„° ê°€ì ¸ì˜¤ê¸° (argsì— ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ True)
    use_date = getattr(args, 'use_date', True)
    
    # prompt_type íŒŒë¼ë¯¸í„° ê°€ì ¸ì˜¤ê¸° (argsì— ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ 'recent_preference')
    prompt_type = getattr(args, 'prompt_type', 'seq_rec')
    
    # emphasize_recent_item íŒŒë¼ë¯¸í„° ê°€ì ¸ì˜¤ê¸° (argsì— ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ False)
    emphasize_recent_item = getattr(args, 'emphasize_recent_item', False)
    
    # include_target_date íŒŒë¼ë¯¸í„° ê°€ì ¸ì˜¤ê¸° (argsì— ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ False)
    include_target_date = getattr(args, 'include_target_date', False)
    
    # use_sasrec íŒŒë¼ë¯¸í„° ê°€ì ¸ì˜¤ê¸° (argsì— ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ False)
    use_sasrec = getattr(args, 'use_sasrec', False)
    
    # sasrec_top_k íŒŒë¼ë¯¸í„° ê°€ì ¸ì˜¤ê¸° (argsì— ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ 5)
    sasrec_top_k = getattr(args, 'sasrec_top_k', 5)
    
    # days_filter íŒŒë¼ë¯¸í„° ê°€ì ¸ì˜¤ê¸° (argsì— ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ None)
    days_filter = getattr(args, 'days_filter', None)
    
    # use_relative_date íŒŒë¼ë¯¸í„° ê°€ì ¸ì˜¤ê¸° (argsì— ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ False)
    use_relative_date = getattr(args, 'use_relative_date', False)
    
    prompt_generator = PromptGenerator(
        item_metadata=item_metadata,
        data_name=args.data_name,
        prompt_type=prompt_type,
        use_brand=args.use_brand,
        use_category=args.use_category,
        use_description=args.use_description,
        use_date=use_date,
        max_history_len=args.max_history_len,
        history_text_max_length=args.history_text_max_length,
        days_filter=days_filter,
        tokenizer=tokenizer,
        apply_chat_template=apply_chat_template,
        emphasize_recent_item=emphasize_recent_item,
        include_target_date=include_target_date,
        use_sasrec=use_sasrec,
        sasrec_top_k=sasrec_top_k,
        use_relative_date=use_relative_date,
    )
    
    # ë°ì´í„°ì…‹ ìƒì„±
    print(f"ğŸ“Š Creating datasets...")
    
    # if args.num_epochs > 0:
        # Train dataset
    train_dataset = RecommendationDataset(
        data_name=args.data_name,
        item_metadata=item_metadata,
        prompt_generator=prompt_generator,
        split="train",
        num_negs=num_negs,
        num_items=num_items,
    )
    
    # Train dataset í•„í„°ë§ (rank ë²”ìœ„ ê¸°ë°˜)
    filter_train_csv = getattr(args, 'filter_train_csv', None)
    if filter_train_csv is not None:
        rank_min = getattr(args, 'rank_min', None)
        rank_max = getattr(args, 'rank_max', None)
        train_dataset.filter_by_rank(filter_train_csv, rank_min, rank_max)
    
    # Valid dataset
    valid_dataset = RecommendationDataset(
        data_name=args.data_name,
        item_metadata=item_metadata,
        prompt_generator=prompt_generator,
        split="valid",
        num_negs=num_negs,
        num_items=num_items,
    )
    
    # Valid dataset í•„í„°ë§ (ì„ íƒì‚¬í•­)
    filter_valid_csv = getattr(args, 'filter_valid_csv', None)
    if filter_valid_csv is not None:
        rank_min = getattr(args, 'rank_min', None)
        rank_max = getattr(args, 'rank_max', None)
        valid_dataset.filter_by_rank(filter_valid_csv, rank_min, rank_max)
        
    train_dataloader = DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        shuffle=True,
        collate_fn=collate_fn,
    )
    
    valid_dataloader = DataLoader(
        valid_dataset,
        batch_size=args.eval_batch_size,
        shuffle=False,
        collate_fn=collate_fn,
    )
    print(f"  Train samples: {len(train_dataset)}")
    print(f"  Valid samples: {len(valid_dataset)}")

    test_dataset = RecommendationDataset(
        data_name=args.data_name,
        item_metadata=item_metadata,
        prompt_generator=prompt_generator,
        split="test",
        num_negs=num_negs,
        num_items=num_items,
    )
    
    # Test dataset í•„í„°ë§ (ì„ íƒì‚¬í•­)
    filter_test_csv = getattr(args, 'filter_test_csv', None)
    if filter_test_csv is not None:
        rank_min = getattr(args, 'rank_min', None)
        rank_max = getattr(args, 'rank_max', None)
        test_dataset.filter_by_rank(filter_test_csv, rank_min, rank_max)

    test_dataloader = DataLoader(
        test_dataset,
        batch_size=args.eval_batch_size,
        shuffle=False,
        collate_fn=collate_fn,
    )
    
    print(f"âœ“ DataLoaders created:")

    print(f"  Test samples: {len(test_dataset)}")
    if num_negs > 0:
        print(f"  Negative samples per user: {num_negs}")
    
    return train_dataset, valid_dataset, test_dataset, prompt_generator, item_metadata
    # return train_dataloader, valid_dataloader, test_dataloader, prompt_generator, item_metadata

