"""
Pre-compute proxy labels for each item based on embedding similarities
ê° ì•„ì´í…œë³„ë¡œ ê°€ì¥ ìœ ì‚¬í•œ ìƒìœ„ proxy_kê°œ ì•„ì´í…œê³¼ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ì—¬ ì €ì¥
"""

import argparse
import json
import torch
from pathlib import Path
from typing import Dict, List, Tuple


def precompute_item_similarities(
    item_embeddings: torch.Tensor,
    proxy_k: int,
    device: str = "cuda",
) -> Dict[int, List[Tuple[int, float]]]:
    """
    ëª¨ë“  ì•„ì´í…œ ê°„ ìœ ì‚¬ë„ë¥¼ ì‚¬ì „ ê³„ì‚°í•˜ì—¬ ê° ì•„ì´í…œë³„ë¡œ ê°€ì¥ ìœ ì‚¬í•œ proxy_kê°œ ì•„ì´í…œ ì €ì¥
    
    Args:
        item_embeddings: [num_items, emb_dim] ì•„ì´í…œ ì„ë² ë”©
        proxy_k: ê° ì•„ì´í…œë³„ë¡œ ì €ì¥í•  ìœ ì‚¬ ì•„ì´í…œ ê°œìˆ˜
        device: ê³„ì‚°ì— ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤
        
    Returns:
        item_proxy_labels: Dict[item_id, List[Tuple[proxy_item_id, similarity]]]
    """
    print(f"ğŸ” Computing item similarities for proxy labels (proxy_k={proxy_k})...")
    
    # ì•„ì´í…œ ì„ë² ë”© ì •ê·œí™” (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°ì„ ìœ„í•´)
    item_embeddings = item_embeddings.to(device)
    normalized_embeddings = torch.nn.functional.normalize(item_embeddings, p=2, dim=1)
    num_items = len(normalized_embeddings)
    
    print(f"  Total items: {num_items}")
    print(f"  Embedding dimension: {item_embeddings.shape[1]}")
    
    item_proxy_labels = {}
    
    # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬í•˜ì—¬ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ê°œì„ 
    batch_size = 1000
    for start_idx in range(1, num_items+1, batch_size):
        end_idx = min(start_idx + batch_size, num_items)
        
        # í˜„ì¬ ë°°ì¹˜ì˜ ì•„ì´í…œë“¤
        batch_embs = normalized_embeddings[start_idx:end_idx]  # [batch_size, emb_dim]
        
        # ì „ì²´ ì•„ì´í…œê³¼ì˜ ìœ ì‚¬ë„ ê³„ì‚°
        similarities = torch.mm(batch_embs, normalized_embeddings.T)  # [batch_size, num_items]
        
        # ê° ì•„ì´í…œì— ëŒ€í•´ ìƒìœ„ proxy_k+1ê°œ ì¶”ì¶œ (ìê¸° ìì‹  í¬í•¨)
        top_k_sims, top_k_indices = torch.topk(
            similarities, 
            k=min(proxy_k + 1, num_items), 
            dim=1
        )
        
        # ê° ì•„ì´í…œë³„ë¡œ ì €ì¥
        for i, emb_idx in enumerate(range(start_idx, end_idx)):
            # item_idëŠ” embedding indexì™€ ë™ì¼ (0-based ë˜ëŠ” 1-based ëª¨ë‘ ì§€ì›)
            item_id = emb_idx
            
            # ìê¸° ìì‹ ì„ ì œì™¸ (ë³´í†µ ê°€ì¥ ìœ ì‚¬ë„ê°€ ë†’ìŒ)
            proxy_indices = top_k_indices[i]  # [proxy_k+1]
            proxy_sims = top_k_sims[i]  # [proxy_k+1]
            
            # ìê¸° ìì‹  ì œê±° (ì„ë² ë”© ì¸ë±ìŠ¤ ê¸°ì¤€)
            mask = proxy_indices != emb_idx
            proxy_indices = proxy_indices[mask][:proxy_k]
            proxy_sims = proxy_sims[mask][:proxy_k]
            
            # ìœ ì‚¬ë„ ì •ê·œí™” (ìµœëŒ€ê°’ì´ 1.0ì´ ë˜ë„ë¡)
            if proxy_sims.max() > 0:
                normalized_sims = proxy_sims / proxy_sims.max()
            else:
                normalized_sims = proxy_sims
            
            # List[Tuple[item_id, similarity]] í˜•íƒœë¡œ ì €ì¥
            proxy_list = [
                (int(proxy_indices[j].item()), float(normalized_sims[j].item()))
                for j in range(len(proxy_indices))
            ]
            item_proxy_labels[item_id] = proxy_list
        
        if (start_idx // batch_size) % 10 == 0:
            print(f"  Processed {end_idx}/{num_items} items...")
    
    print(f"âœ“ Completed item similarity computation for {len(item_proxy_labels)} items")
    
    # ì˜ˆì‹œ ì¶œë ¥
    if len(item_proxy_labels) > 0:
        sample_item = list(item_proxy_labels.keys())[0]
        proxy_list = item_proxy_labels[sample_item]
        print(f"\n  Example: Item {sample_item} â†’ Top-{len(proxy_list)} similar items")
        print(f"           First 5 proxies:")
        for proxy_id, sim in proxy_list[:5]:
            print(f"             Item {proxy_id}: similarity={sim:.4f}")
    
    return item_proxy_labels


def main():
    parser = argparse.ArgumentParser(description="Pre-compute proxy labels for items")
    parser.add_argument("--data_name", type=str, required=True, help="Dataset name (e.g., beauty, sports, toys)")
    parser.add_argument("--emb_type", type=str, required=True, help="Embedding type (e.g., review_description, title)")
    parser.add_argument("--emb_model_name", type=str, required=True, help="Embedding model name (e.g., sentence-transformers/all-MiniLM-L6-v2)")
    parser.add_argument("--proxy_k", type=int, default=10, help="Number of proxy items per item")
    parser.add_argument("--device", type=str, default="cuda", help="Device to use (cuda or cpu)")
    parser.add_argument("--output_dir", type=str, default="data_emb", help="Output directory for proxy labels")
    
    args = parser.parse_args()
    
    # ì„ë² ë”© ëª¨ë¸ ì´ë¦„ì—ì„œ ë§ˆì§€ë§‰ ë¶€ë¶„ ì¶”ì¶œ
    emb_model_name_dir = args.emb_model_name.split("/")[-1]
    
    # ì•„ì´í…œ ì„ë² ë”© íŒŒì¼ ê²½ë¡œ
    item_embedding_file = f"data_emb/{args.data_name}_{args.emb_type}_{emb_model_name_dir}_emb.pt"
    
    print(f"ğŸ“¦ Loading item embeddings from: {item_embedding_file}")
    if not Path(item_embedding_file).exists():
        raise FileNotFoundError(f"Item embedding file not found: {item_embedding_file}")
    
    item_embeddings = torch.load(item_embedding_file, map_location=args.device)
    print(f"âœ“ Loaded embeddings for {len(item_embeddings)} items")
    
    # ì•„ì´í…œ ê°„ ìœ ì‚¬ë„ ê³„ì‚°
    item_proxy_labels = precompute_item_similarities(
        item_embeddings=item_embeddings,
        proxy_k=args.proxy_k,
        device=args.device,
    )
    
    # ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    output_file = output_dir / f"{args.data_name}_proxy_labels_k{args.proxy_k}_{args.emb_type}_{emb_model_name_dir}.json"
    
    print(f"\nğŸ’¾ Saving proxy labels to: {output_file}")
    
    # JSON í˜•íƒœë¡œ ì €ì¥
    # keyëŠ” stringìœ¼ë¡œ ë³€í™˜ (JSONì€ integer keyë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŒ)
    item_proxy_labels_str_keys = {
        str(item_id): proxy_list 
        for item_id, proxy_list in item_proxy_labels.items()
    }
    
    with open(output_file, 'w') as f:
        json.dump(item_proxy_labels_str_keys, f, indent=2)
    
    print(f"âœ“ Saved proxy labels for {len(item_proxy_labels)} items")
    
    # íŒŒì¼ í¬ê¸° ì¶œë ¥
    file_size_mb = output_file.stat().st_size / (1024 * 1024)
    print(f"  File size: {file_size_mb:.2f} MB")
    
    print("\nâœ… Pre-computation completed successfully!")
    print(f"\nTo use these proxy labels, set the following in your training script:")
    print(f"  --proxy_label_reward")
    print(f"  --proxy_k {args.proxy_k}")
    print(f"  --data_name {args.data_name}")
    print(f"  --emb_type {args.emb_type}")
    print(f"  --emb_model_name {args.emb_model_name}")


if __name__ == "__main__":
    main()
