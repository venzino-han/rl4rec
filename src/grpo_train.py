#!/usr/bin/env python3
"""
GRPO Training Script for Recommendation System
TRLì˜ GRPOTrainerë¥¼ ì‚¬ìš©í•œ ì¶”ì²œ ì‹œìŠ¤í…œ í•™ìŠµ
RetrievalServiceì™€ ì—°ë™í•˜ì—¬ NDCG ê¸°ë°˜ ë¦¬ì›Œë“œë¡œ í•™ìŠµ
"""

import os
import ray
import torch
import argparse
import json
import gc
import numpy as np
import logging as std_logging
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Optional
from tqdm import tqdm
import contextlib

from trl import GRPOTrainer, GRPOConfig
from trl.extras.profiling import profiling_decorator, profiling_context

from transformers import AutoTokenizer, AutoModelForCausalLM
from vllm import LLM, SamplingParams
from vllm.pooling_params import PoolingParams
from vllm.distributed.parallel_state import cleanup_dist_env_and_memory
from vllm.distributed.parallel_state import (
    destroy_model_parallel,
    destroy_distributed_environment,
)

from utils.reward_function import (
    RecRewardFrunction, 
    calculate_ndcg, 
    calculate_hit_rate,
    LocalEmbeddingRewardFunction,
    SimilarHistoryItemMentionReward,
    MetadataMentionReward,
)
from utils.dataset import create_dataloaders
from evaluator import RecommendationEvaluator

from accelerate import logging
from accelerate.utils import gather

import torch.distributed as dist
from numba import cuda

import wandb
import random

logger = logging.get_logger(__name__)

class GRPOTrainerRecReward(GRPOTrainer):

    @profiling_decorator
    def _calculate_rewards(self, inputs, prompts, completions, completion_ids_list):
        device = self.accelerator.device
        rewards_per_func = torch.zeros(len(prompts), len(self.reward_funcs), device=device)

        # Repeat all input columns (but "prompt", "completion", and "completion_ids") to match the num of generations

        # print(f"inputs: {inputs[0].keys()}")
        keys = [key for key in inputs[0] if key not in ["prompt", "completion", "completion_ids"]]
        reward_kwargs = {key: [example[key] for example in inputs] for key in keys}

        # print(f"reward_kwargs: {reward_kwargs}")
        # print(f"keys: {reward_kwargs.keys()}")

        # This allows for dynamic reward shaping based on training progress.
        reward_kwargs["trainer_state"] = self.state

        # Reward breakdown ìˆ˜ì§‘ì„ ìœ„í•œ ë”•ì…”ë„ˆë¦¬
        reward_breakdowns = {}

        for i, (reward_func, reward_processing_class, reward_func_name) in enumerate(
            zip(self.reward_funcs, self.reward_processing_classes, self.reward_func_names, strict=True)
        ):
            with profiling_context(self, reward_func_name):
                output_reward_func = reward_func(
                    generated_texts=completions,
                    targets=reward_kwargs["target"],
                    histories=reward_kwargs["history"],
                    user_ids=reward_kwargs["user_id"],
                    **reward_kwargs,
                )
                # Convert None values to NaN
                output_reward_func = [reward if reward is not None else torch.nan for reward in output_reward_func]

                rewards_per_func[:, i] = torch.tensor(output_reward_func, dtype=torch.float32, device=device)
                
                # Reward breakdown ì •ë³´ ìˆ˜ì§‘ (LocalEmbeddingRewardFunctionì¸ ê²½ìš°)
                if reward_func_name == "LocalEmbeddingRewardFunction":
                    breakdown = reward_func.get_reward_breakdown()
                    for key, value in breakdown.items():
                        if key not in reward_breakdowns:
                            reward_breakdowns[key] = []
                        reward_breakdowns[key].append(value)

        # If all reward functions return None for a given row, issue a detailed warning
        if torch.isnan(rewards_per_func).all(dim=1).any():
            nan_row_idx = torch.isnan(rewards_per_func).all(dim=1).nonzero(as_tuple=True)[0][0]
            row_reward_kwargs = {
                key: value[nan_row_idx] for key, value in reward_kwargs.items() if key != "trainer_state"
            }
            row_reward_kwargs["prompt"] = prompts[nan_row_idx]
            row_reward_kwargs["completion"] = completions[nan_row_idx]
            logger.warning(
                f"All reward functions returned None for the following kwargs:\n{row_reward_kwargs}\n"
                "Please ensure that at least one reward function returns a valid reward."
            )

        # Gather the reward per function: this part is crucial, because the rewards are normalized per group and the
        # completions may be distributed across processes
        rewards_per_func = gather(rewards_per_func)
        
        # --- ë³€ê²½ëœ ë¶€ë¶„: ì§ì ‘ ë¡œê¹… ëŒ€ì‹  ì¸ìŠ¤í„´ìŠ¤ ë³€ìˆ˜ì— ì €ì¥ ---
        # ë©”ì¸ í”„ë¡œì„¸ìŠ¤ì—ì„œë§Œ ê³„ì‚°í•˜ì—¬ ì €ì¥
        if reward_breakdowns and self.accelerator.is_main_process:
            self._store_reward_breakdown(reward_breakdowns)
        
        return rewards_per_func
    
    def _store_reward_breakdown(self, reward_breakdowns):
        """
        ê³„ì‚°ëœ Breakdown í†µê³„ì¹˜ë¥¼ self._stored_metricsì— ì €ì¥í•©ë‹ˆë‹¤.
        ì‹¤ì œ ë¡œê¹…ì€ log() ë©”ì„œë“œê°€ í˜¸ì¶œë  ë•Œ ìˆ˜í–‰ë©ë‹ˆë‹¤.
        """
        mode = "train" if self.model.training else "eval"
        for key, values_list in reward_breakdowns.items():
            # ì—¬ëŸ¬ reward functionì—ì„œ ìˆ˜ì§‘ëœ ê°’ë“¤ì„ í•©ì¹¨
            all_values = torch.cat(values_list) if len(values_list) > 0 else torch.tensor([])
            if len(all_values) > 0:
                self._metrics[mode][f"rewards/{key}"] = all_values.tolist()
                # self._metrics[mode][f"rewards/{key}_std"] = all_values

class GRPOTrainerWrapper:
    """
    TRL GRPOë¥¼ í™œìš©í•œ ì¶”ì²œ ì‹œìŠ¤í…œ í•™ìŠµê¸°
    """
    
    def __init__(self, args):
        self.args = args

        
        # Ray ì´ˆê¸°í™” (use_local_embeddingì´ Falseì¸ ê²½ìš°ì—ë§Œ)
        if not args.use_local_embedding:
            if not ray.is_initialized():
                print(f"ğŸ”§ Initializing Ray...")
                ray.init(address=args.ray_address, namespace=args.namespace)
                print(f"âœ“ Ray initialized")
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        print(f"ğŸ“š Loading tokenizer: {args.model_name}")
        self.tokenizer = AutoTokenizer.from_pretrained(args.model_name)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # ëª¨ë¸ ë¡œë“œ
        print(f"ğŸ¤– Loading model: {args.model_name}")
        self.model = AutoModelForCausalLM.from_pretrained(
            args.model_name,
            trust_remote_code=True,
        )
        

        
        # ë°ì´í„°ë¡œë” ìƒì„± (create_dataloaders í•¨ìˆ˜ ì‚¬ìš©)
        (
            self.train_dataset,
            self.valid_dataset,
            self.test_dataset,
            self.prompt_generator,
            self.item_metadata,
        ) = create_dataloaders(args, tokenizer=self.tokenizer)
        
        if args.num_epochs > 0:
                    
            # wandb ì´ˆê¸°í™” ë° args ì „ë‹¬
            if args.report_to == "wandb":
                print(f"ğŸ“Š Initializing Weights & Biases...")
                wandb.init(
                    project="rl4rec",
                    name=args.run_name,
                    config=vars(args),  # argsì˜ ëª¨ë“  ìš”ì†Œë¥¼ wandb configë¡œ ì „ë‹¬
                )
                print(f"âœ“ Wandb initialized with all args")

            # GRPO Config
            grpo_config = GRPOConfig(
                output_dir=args.checkpoint_dir,
                num_train_epochs=args.num_epochs,
                per_device_train_batch_size=args.batch_size,
                gradient_accumulation_steps=args.gradient_accumulation_steps,
                learning_rate=args.learning_rate,

                logging_steps=args.log_interval,
                eval_steps=args.eval_interval,
                save_steps=args.save_interval,
                save_total_limit=args.save_total_limit,
                
                max_grad_norm=args.max_grad_norm,
                seed=args.seed,
                bf16=args.bf16,
                report_to=args.report_to if args.report_to != "none" else None,
                run_name=args.run_name,
                beta=args.reference_model_kld_coef,
                
                # Loss type
                loss_type=args.loss_type,
                importance_sampling_level=args.importance_sampling_level,
                
                # GRPO specific
                num_generations=args.num_sample_generations,
                temperature=args.train_temperature,
                max_steps=args.max_steps,

                # Generation
                max_completion_length=args.max_new_tokens,
                repetition_penalty=1.0,
                top_p=0.95,

                # vLLM
                use_vllm=True,
                vllm_mode="colocate",
                vllm_gpu_memory_utilization=args.train_vllm_gpu_memory_utilization,
                vllm_max_model_length=args.max_length+args.max_new_tokens,
                vllm_enable_sleep_mode=False,
                
                vllm_importance_sampling_correction=True,
                vllm_importance_sampling_mode= "sequence_mask" if args.importance_sampling_level == "sequence" else "token_mask",
                vllm_importance_sampling_cap=3.0,

                include_for_metrics=["reward", "entropy", "grad_norm", "epoch"]
            )
            
            # GRPO Trainer
            print(f"ğŸ’° Creating reward functions")
            
            # ë³µìˆ˜ì˜ reward function ë¦¬ìŠ¤íŠ¸ ìƒì„±
            reward_funcs = []
            
            # 1. ê¸°ë³¸ reward function (embedding-based ë˜ëŠ” retrieval-based)
            print(f"  [1] Base reward: {args.reward_type}@{args.k}")
            if args.use_local_embedding:
                print(f"      Using local embedding-based reward calculation")
                base_reward_fn = LocalEmbeddingRewardFunction(
                    args=args,
                    uid_2_target=self.train_dataset.target_dict,
                )
            else:
                print(f"      Using RetrievalService-based reward calculation")
                base_reward_fn = RecRewardFrunction(
                    retrieval_service_name=args.retrieval_service_name,
                    namespace=args.namespace,
                    data_name=args.data_name,
                    reward_type=args.reward_type,
                    k=args.k,
                    normalize=args.normalize_rewards,
                    test_target=args.test_target,
                )
            reward_funcs.append(base_reward_fn)
            
            # 2. Similar History Item Mention Reward (ì˜µì…˜)
            if args.use_similar_history_reward:
                print(f"  [2] Similar History Item Mention Reward: +1.0 for mentioning similar item title (first 3 words)")
                # ì„ë² ë”© ë¡œë“œ (ìºì‹±ì„ ìœ„í•´)
                emb_model_name_dir = args.emb_model_name.split("/")[-1]
                item_embedding_file_path = f"data_emb/{args.data_name}_{args.emb_type}_{emb_model_name_dir}_emb.pt"
                item_embeddings = torch.load(item_embedding_file_path, map_location=args.device)
                
                similar_history_reward_fn = SimilarHistoryItemMentionReward(
                    data_name=args.data_name,
                    item_embeddings=item_embeddings,
                    uid_2_target=self.train_dataset.target_dict,
                    device=args.device,
                    use_position_weight=args.similar_history_position_weight,
                    position_decay=args.similar_history_position_decay,
                )
                reward_funcs.append(similar_history_reward_fn)
            
            # 3. Brand Mention Reward (ì˜µì…˜)
            if args.use_brand_reward:
                print(f"  [3] Brand Mention Reward: +0.5 for mentioning target brand")
                brand_reward_fn = BrandMentionReward(
                    data_name=args.data_name,
                    device=args.device,
                )
                reward_funcs.append(brand_reward_fn)
            
            # 4. Category Mention Reward (ì˜µì…˜)
            if args.use_category_reward:
                print(f"  [4] Category Mention Reward: +0.5 for mentioning target category")
                category_reward_fn = CategoryMentionReward(
                    data_name=args.data_name,
                    device=args.device,
                )
                reward_funcs.append(category_reward_fn)
            
            # 5. Metadata Mention Reward (ì˜µì…˜) - í†µí•© ë©”íƒ€ë°ì´í„° ë¦¬ì›Œë“œ
            if args.use_metadata_reward:
                print(f"  [5] Metadata Mention Reward: +{args.metadata_base_reward} per metadata word, "
                      f"length penalty={args.metadata_length_penalty}, "
                      f"history penalty={args.history_penalty_weight}")
                metadata_reward_fn = MetadataMentionReward(
                    data_name=args.data_name,
                    device=args.device,
                    base_reward=args.metadata_base_reward,
                    length_penalty_alpha=args.metadata_length_penalty,
                    min_length=args.metadata_min_length,
                    history_penalty_weight=args.history_penalty_weight,
                )
                reward_funcs.append(metadata_reward_fn)
            
            print(f"  Total reward functions: {len(reward_funcs)}")
            
            # reward_funcsê°€ 1ê°œë©´ ë‹¨ì¼ í•¨ìˆ˜ë¡œ, 2ê°œ ì´ìƒì´ë©´ ë¦¬ìŠ¤íŠ¸ë¡œ ì „ë‹¬
            # reward_funcs_input = reward_funcs[0] if len(reward_funcs) == 1 else reward_funcs

            print(f"ğŸ¯ Initializing GRPO Trainer...")
            self.grpo_trainer = GRPOTrainerRecReward(
                model=self.model,
                args=grpo_config,
                train_dataset=self.train_dataset,
                eval_dataset=self.valid_dataset,
                reward_funcs=reward_funcs,
                processing_class=self.tokenizer,
            )

    def evaluate_final_metrics(self, split="test"):
        """
        ìµœì¢… í‰ê°€: hit@k, ndcg@k (k=5,10,20)ë¥¼ ê³„ì‚°
        RecommendationEvaluator í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ í‰ê°€ ìˆ˜í–‰
        """
        print(f"\n{'='*80}")
        print("ğŸ§¹ Cleaning up training resources before evaluation...")
        print(f"{'='*80}")

        if split == "test":
            dataset = self.test_dataset
        elif split == "valid":
            dataset = self.valid_dataset
        else:
            raise ValueError(f"Invalid split: {split}")
        
        if hasattr(self, 'grpo_trainer'):
            del self.grpo_trainer
            self.grpo_trainer = None
            print("âœ“ GRPO Trainer cleaned")

        gc.collect()
        torch.cuda.empty_cache()
        torch.cuda.synchronize()

        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶œë ¥
        if torch.cuda.is_available():
            print("=" * 80)
            print(f"ğŸ’¾ GPU Memory after cleanup: {torch.cuda.memory_allocated() / 1024**3:.2f} GB allocated")
            print(f"ğŸ’¾ GPU Memory reserved: {torch.cuda.memory_reserved() / 1024**3:.2f} GB reserved")
            print("=" * 80)

        # Evaluator ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë° í‰ê°€ ì‹¤í–‰
        evaluator = RecommendationEvaluator(self.args, self.args.final_checkpoint_dir)
        
        try:
            results = evaluator.evaluate(dataset, split=split, save_log=True)
        finally:
            # í‰ê°€ ì™„ë£Œ í›„ ë©”ëª¨ë¦¬ ì •ë¦¬
            evaluator.cleanup()
        
        return results
    
    def train(self):
        """
        ì „ì²´ í•™ìŠµ ë£¨í”„ ì‹¤í–‰
        """
        print("=" * 80)
        print("ğŸš€ Starting GRPO Training")
        print("=" * 80)

        if self.args.num_epochs > 0:
            self.grpo_trainer.train()
        print("=" * 80)
        print("âœ“ Training completed!")        
        # ìµœì¢… í…ŒìŠ¤íŠ¸ í‰ê°€
        # test_metrics = self.evaluate_final_metrics(self.test_dataset, split="test")
        


def parse_args():
    """Command line arguments"""
    parser = argparse.ArgumentParser(
        description="GRPO Training for Recommendation System"
    )
    # basic
    parser.add_argument("--run_name", type=str, default="grpo")
    
    # Ray & Service
    parser.add_argument("--ray_address", type=str, default="auto")
    parser.add_argument("--namespace", type=str, default="rl4rec")
    parser.add_argument("--retrieval_service_name", type=str, default="RetrievalService")
    parser.add_argument("--data_name", type=str, default="beauty")
    
    # Model
    parser.add_argument("--model_name", type=str, default="google/gemma-3-1b-it")
    parser.add_argument("--device", type=str, default="cuda" if torch.cuda.is_available() else "cpu")
    parser.add_argument("--max_length", type=int, default=1024*4)
    parser.add_argument("--max_new_tokens", type=int, default=512)
    parser.add_argument("--max_emb_length", type=int, default=512)
    parser.add_argument("--temperature", type=float, default=0.01)
    parser.add_argument("--use_ref_model", action="store_true", help="Use reference model for KL penalty")
    
    # Embedding Model for Evaluation
    parser.add_argument("--emb_model_name", type=str, default="mixedbread-ai/mxbai-embed-large-v1")
    parser.add_argument("--emb_type", type=str, default="item_preference_1024_gemma-3-1b-it", help="Type of item text to embed (title, description, etc.)")
    
    # Data
    parser.add_argument("--data_dir", type=str, default="data")
    
    # Prompt Generation
    parser.add_argument("--prompt_type", type=str, default="seq_rec", help="Prompt template")
    parser.add_argument("--use_brand", action="store_true", default=True, help="Include brand in prompt")
    parser.add_argument("--use_category", action="store_true", default=True, help="Include category in prompt")
    parser.add_argument("--use_description", action="store_true", help="Include description in prompt")
    parser.add_argument("--use_features", action="store_true", help="Include features in prompt")
    parser.add_argument("--use_date", action="store_true", default=True, help="Include purchase date information in prompt")
    parser.add_argument("--use_last_item", action="store_true", default=True, help="Emphasize last item")
    parser.add_argument("--emphasize_recent_item", action="store_true",
                        help="Emphasize recent purchase item with detailed information including purchase date ('This user's most recent purchase is...' format)")
    parser.add_argument("--include_target_date", action="store_true",
                        help="Include target/label item's purchase date at the end of prompt")
    parser.add_argument("--max_history_len", type=int, default=8, help="Max history length")
    parser.add_argument("--history_text_max_length", type=int, default=100, help="Max words per history item")
    parser.add_argument("--days_filter", type=int, default=None,
                        help="Filter reviews to only include those within N days of target date")
    parser.add_argument("--test_target", action="store_true", help="Use target text for test")
    
    # SASRec Integration
    parser.add_argument("--use_sasrec", action="store_true",
                        help="Include SASRec recommendations in prompt as reference for query generation")
    parser.add_argument("--sasrec_top_k", type=int, default=5,
                        help="Number of top-K SASRec recommendations to include in prompt")
    
    # GRPO Training
    parser.add_argument("--loss_type", type=str, default="grpo")
    parser.add_argument("--learning_rate", type=float, default=1e-6)
    parser.add_argument("--batch_size", type=int, default=32)
    parser.add_argument("--eval_batch_size", type=int, default=64)
    parser.add_argument("--gradient_accumulation_steps", type=int, default=1)
    parser.add_argument("--num_sample_generations", type=int, default=8,
                        help="Number of generations per prompt for GRPO")
    parser.add_argument("--max_grad_norm", type=float, default=1.0)
    parser.add_argument("--num_epochs", type=int, default=3)
    parser.add_argument("--max_steps", type=int, default=5000)
    parser.add_argument("--bf16", action="store_true", help="Use bfloat16")
    parser.add_argument("--train_temperature", type=float, default=0.1)
    parser.add_argument("--reference_model_kld_coef", type=float, default=0.0)
    parser.add_argument("--importance_sampling_level", type=str, default="token", choices=["token", "sequence"])

    # Reward
    parser.add_argument("--reward_type", type=str, default="ndcg", choices=["ndcg", "hit", "mixed"])
    parser.add_argument("--k", type=int, default=100, help="Top-K for metrics")
    parser.add_argument("--normalize_rewards", action="store_true", help="Normalize rewards")
    parser.add_argument("--num_negs", type=int, default=0, help="Number of negative items")
    parser.add_argument("--prepend_last_item", action="store_true",)
    
    # Multiple Reward Functions
    parser.add_argument("--use_similar_history_reward", action="store_true",
                        help="Use reward for mentioning similar history item's title (first 3 words). "
                             "Reward: +1.0 for mentioning the most similar item from purchase history.")
    parser.add_argument("--similar_history_position_weight", action="store_true",
                        help="Enable position-based weighting for similar history reward. "
                             "Earlier mentions get higher rewards.")
    parser.add_argument("--similar_history_position_decay", type=float, default=0.5,
                        help="Position decay factor for similar history reward (0.0-1.0). "
                             "0.0 = no decay (position-independent), "
                             "1.0 = full decay (reward becomes 0 at text end). "
                             "Default: 0.5 (reward halves at text end)")
    parser.add_argument("--use_brand_reward", action="store_true",
                        help="Use reward for mentioning target item's brand. "
                             "Reward: +0.5 for mentioning the brand.")
    parser.add_argument("--use_category_reward", action="store_true",
                        help="Use reward for mentioning target item's category. "
                             "Reward: +0.5 for mentioning any part of the category.")
    
    # Metadata Mention Reward (í†µí•© ë©”íƒ€ë°ì´í„° ë¦¬ì›Œë“œ)
    parser.add_argument("--use_metadata_reward", action="store_true",
                        help="Use unified metadata mention reward. "
                             "Rewards mentioning metadata words (brand, category) proportionally, "
                             "with length penalty and stopword filtering using NLTK.")
    parser.add_argument("--metadata_base_reward", type=float, default=0.1,
                        help="Base reward per metadata word mentioned (default: 1.0)")
    parser.add_argument("--metadata_length_penalty", type=float, default=0.5,
                        help="Length penalty alpha for metadata reward (0.0-1.0). "
                             "Higher values penalize longer texts more (default: 0.5)")
    parser.add_argument("--metadata_min_length", type=int, default=16,
                        help="Minimum text length (in words) before length penalty applies (default: 10)")
    parser.add_argument("--history_penalty_weight", type=float, default=0.01,
                        help="Penalty weight for mentioning history metadata words not in target (default: 0.5)")

    
    # Novelty Reward (popularity-based reward)
    parser.add_argument("--novelty_reward", action="store_true",
                        help="Use novelty reward: NDCG Ã— popularity_weight "
                             "(ì¸ê¸° ì—†ëŠ” ì•„ì´í…œì„ ë†’ì€ rankë¡œ ì˜ˆì¸¡í• ìˆ˜ë¡ ë†’ì€ ë³´ìƒ)")
    parser.add_argument("--novelty_coef", type=float, default=1.0,
                        help="Novelty reward coefficient (weight for novelty reward component)")
    parser.add_argument("--novelty_target_rank", type=int, default=20,
                        help="(Deprecated, not used)")
    parser.add_argument("--novelty_mode", type=str, default="gaussian", 
                        choices=["gaussian", "uniform", "inverse"],
                        help="(Deprecated, not used)")
    parser.add_argument("--novelty_annealing", action="store_true",
                        help="Enable novelty annealing: gradually increase novelty ratio from 0 to 1 "
                             "as training progresses. Final reward = (1-ratio)*base + ratio*novelty")
    
    # Popularity Reward (long-tail item bonus)
    parser.add_argument("--popularity_coef", type=float, default=0.0,
                        help="Popularity reward coefficient (0.0 = disabled). "
                             "Rewards predicting unpopular items more.")
    
    # Target Embedding Similarity Reward
    parser.add_argument("--target_emb_reward", action="store_true",
                        help="Use target embedding similarity reward. "
                             "Rewards generated text that is similar to target item embedding.")
    parser.add_argument("--target_emb_file", type=str, default=None,
                        help="Target embedding file path. If None, uses the same embedding as emb_type.")
    parser.add_argument("--target_emb_coef", type=float, default=1.0,
                        help="Target embedding reward coefficient (weight for this reward component)")
    
    # InfoNCE Reward
    parser.add_argument("--infonce_reward", action="store_true",
                        help="Use InfoNCE (contrastive learning) reward. "
                             "Maximizes similarity with target while minimizing with negatives.")
    parser.add_argument("--infonce_coef", type=float, default=1.0,
                        help="InfoNCE reward coefficient (weight for this reward component)")
    parser.add_argument("--infonce_temperature", type=float, default=0.07,
                        help="Temperature parameter for InfoNCE (default: 0.07)")
    parser.add_argument("--infonce_emb_type", type=str, default=None,
                        help="Embedding type for InfoNCE (e.g., 'title_emb'). "
                             "If None, uses the same embedding as emb_type.")
    
    # Proxy Label Reward
    parser.add_argument("--proxy_label_reward", action="store_true",
                        help="Use proxy label reward: treats items similar to target as soft labels. "
                             "Rewards predicting target-similar items proportional to their similarity.")
    parser.add_argument("--proxy_k", type=int, default=100,
                        help="Number of proxy items (similar items) to use as soft labels")
    parser.add_argument("--proxy_label_coef", type=float, default=1.0,
                        help="Proxy label reward coefficient (weight for this reward component)")
    parser.add_argument("--proxy_label_cutoff", type=float, default=0.1,)
    parser.add_argument("--proxy_label_file", type=str, default=None,
                        help="Path to pre-computed proxy labels JSON file. "
                             "If None, automatically constructs path from data_name, emb_type, and emb_model_name. "
                             "Example: data_emb/beauty_proxy_labels_k1000_random_th0.3_item_preference_1024_gemma-3-4b-it_mxbai-embed-large-v1.json")
    
    # Local Embedding-based Reward (alternative to RetrievalService)
    parser.add_argument("--use_local_embedding", action="store_true", 
                        help="Use local embedding-based reward instead of RetrievalService")
    parser.add_argument("--emb_batch_size", type=int, default=128,
                        help="Batch size for embedding computation")
    
    # Logging & Checkpointing
    parser.add_argument("--checkpoint_dir", type=str, default="checkpoints/grpo")
    parser.add_argument("--final_checkpoint_dir", type=str, default="checkpoints/grpo/checkpoint-5000")
    parser.add_argument("--log_interval", type=int, default=100)
    parser.add_argument("--eval_interval", type=int, default=100)
    parser.add_argument("--save_interval", type=int, default=500)
    parser.add_argument("--save_total_limit", type=int, default=1)
    parser.add_argument("--report_to", type=str, default="wandb", help="Logging backend (wandb)")
    
    # Misc
    parser.add_argument("--seed", type=int, default=42)
    parser.add_argument("--debug", action="store_true")

    # vllm for evaluation
    parser.add_argument("--train_vllm_gpu_memory_utilization", type=float, default=0.45)
    parser.add_argument("--gpu_memory_utilization", type=float, default=0.95)
    parser.add_argument("--eval_emb_gpu_memory_utilization", type=float, default=0.95)
    parser.add_argument("--eval_emb_batch_size", type=int, default=512)
    parser.add_argument("--eval_max_tokens", type=int, default=512)
    parser.add_argument("--eval_emb_max_length", type=int, default=512)
    parser.add_argument("--eval_samples", type=int, default=100000)
    parser.add_argument("--dummy_generation", action="store_true", help="Use dummy generation")
    
    # Rank-based filtering for training
    parser.add_argument("--filter_train_csv", type=str, default=None,
                        help="Path to evaluation CSV file for filtering train set by rank")
    parser.add_argument("--rank_min", type=int, default=None,
                        help="Minimum rank for filtering (inclusive, None = no limit)")
    parser.add_argument("--rank_max", type=int, default=None,
                        help="Maximum rank for filtering (inclusive, None = no limit)")
    
    return parser.parse_args()


def main():
    """Main training function"""
    args = parse_args()
    args.sequential_file = f"data/{args.data_name}/sequential_data.txt"

    # fix seed
    torch.manual_seed(args.seed)
    torch.cuda.manual_seed(args.seed)
    np.random.seed(args.seed)
    random.seed(args.seed)
    
    # vLLM ë¡œê¹… ë ˆë²¨ ì¡°ì • (INFO ë©”ì‹œì§€ ì–µì œ)
    std_logging.getLogger("vllm").setLevel(std_logging.ERROR)
    
    # Trainer ì´ˆê¸°í™” ë° í•™ìŠµ
    trainer = GRPOTrainerWrapper(args)
    trainer.train()


if __name__ == "__main__":
    main()
