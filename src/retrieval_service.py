# db_server.py
import os
import ray
import torch
from vllm import LLM, PoolingParams

import sentence_transformers

@ray.remote(num_gpus=1)
class RetrievalService:
    def __init__(self, args):
        print("Loading DB & Model on GPU 1...")
        self.args = args
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # Pooling íŒŒë¼ë¯¸í„° ì„¤ì •: 512 í† í° ì´ˆê³¼ ì‹œ ì ˆì‚­
        self.pooling_params = PoolingParams(
            truncate_prompt_tokens=512,  # ì •í™•í•œ í† í° ìˆ˜ ì§€ì •
            task="embed",
            # task="token_embed",
        )

        # TODO: accelerate embedding generations by vram usage
        self.llm = LLM(
            model=self.args.emb_model_name, 
            task="embed", 
            # task="token_embed",
            # task="embedding", 
            # runner="pooling",
            enforce_eager=True,
            gpu_memory_utilization=0.8,
            trust_remote_code=True,
            max_model_len=512, 
            max_num_seqs=512,
        )

        # self.st_model = sentence_transformers.SentenceTransformer(self.args.emb_model_name, device=self.device)
        self.outputs = self._test_encode()

        
        # ì—¬ëŸ¬ ë°ì´í„°ì…‹ ì¸ë±ìŠ¤ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ê´€ë¦¬
        self.reference_indices = self._load_multiple_datasets(self.args.datasets)
        print(f"âœ“ Loaded {len(self.reference_indices)} dataset(s) on {self.device}")
        
    def _load_multiple_datasets(self, data_names=None):
        """Load multiple datasets into a dictionary"""
        indices = {}
        
        if data_names is None or len(data_names) == 0:
            # ë°ì´í„°ì…‹ì´ ì§€ì •ë˜ì§€ ì•Šì€ ê²½ìš° dummy ë°ì´í„° ìƒì„±
            name = "dummy"
            dummy_data = torch.rand(20000, 1024, device=self.device, dtype=torch.float32)
            indices[name] = dummy_data
            print(f"  Loaded dummy embedding: {dummy_data.shape}")
        else:        
            # ì‹¤ì œ ë°ì´í„°ì…‹ ë¡œë“œ
            for name in data_names:
                emb_file = f"data_emb/{name}_{self.args.emb_type}_{self.args.emb_model_name_dir}.pt"
                print(f"  Loading: {emb_file}")
                emb = torch.load(emb_file, map_location=self.device)
                indices[name] = emb / emb.norm(dim=-1, keepdim=True)
                print(f"  Loaded dataset '{name}': {emb.shape}")
        
        return indices

    def _test_encode(self):
        texts = ["This is a test sentence for embedding extraction.",
        "vLLM is a fast and easy-to-use library for LLM inference and serving.",
        "Mixedbread AI provides high-quality embedding models.",
        "Embeddings are useful for semantic search and similarity tasks.",]
        outputs = self.llm.encode(
        # # outputs = self.llm.embed(
            prompts=texts, 
            pooling_task="embed",
            # pooling_task="token_embed",
            pooling_params=self.pooling_params,  # í† í° ì ˆì‚­ íŒŒë¼ë¯¸í„° ì ìš©
            use_tqdm=True,
        )
        # print(outputs)
        return outputs

    def calculate_reward(self, texts, data_name, targets=None, neg_items=None, debug=False):
        """
        ì „ì²´ ì¸ë±ìŠ¤ ë˜ëŠ” ì§€ì •ëœ ì•„ì´í…œë“¤ì— ëŒ€í•œ ìŠ¤ì½”ì–´ ë°°ì—´ ê³„ì‚°
        
        Args:
            texts (List[str]): ì„ë² ë”©í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            data_name (str): ì‚¬ìš©í•  ë°ì´í„°ì…‹ ì´ë¦„ (í•„ìˆ˜)
            targets (List[int], optional): íƒ€ê²Ÿ ì•„ì´í…œ ID ë¦¬ìŠ¤íŠ¸ [batch_size]
            neg_items (List[List[int]], optional): ë°°ì¹˜ë³„ negative ì•„ì´í…œ ID ë¦¬ìŠ¤íŠ¸ [batch_size, num_negs]
            debug (bool): ë””ë²„ê¹… ëª¨ë“œ
        
        Returns:
            torch.Tensor: ìŠ¤ì½”ì–´ ë°°ì—´
                - targets/neg_itemsê°€ Noneì¸ ê²½ìš°: [len(texts), index_size] ì „ì²´ ì¸ë±ìŠ¤ì™€ì˜ ìœ ì‚¬ë„
                - targets/neg_itemsê°€ ì œê³µëœ ê²½ìš°: [len(texts), 1 + num_negs] target + negativesì— ëŒ€í•œ ìœ ì‚¬ë„
        """
        if data_name not in self.reference_indices:
            raise ValueError(f"Dataset '{data_name}' not found. Available: {list(self.reference_indices.keys())}")
        
        # 1. vLLM ì„ë² ë”© (Batch Processing) - ì§ì ‘ torch tensorë¡œ ë³€í™˜
        
        # texts = ["[CLS] "+text for text in texts]

        outputs = self.llm.encode(
        # # outputs = self.llm.embed(
            prompts=texts, 
            pooling_task="embed",
            # pooling_task="token_embed",
            pooling_params=self.pooling_params,  # í† í° ì ˆì‚­ íŒŒë¼ë¯¸í„° ì ìš©
            use_tqdm=debug,
        )

        
        # vLLM outputsë¥¼ ì§ì ‘ tensor stackìœ¼ë¡œ ë³€í™˜ (ì¶”ê°€ ë³€í™˜ ì—†ì´)
        embeddings_list = [torch.as_tensor(out.outputs.data, dtype=torch.float32, device=self.device)
                          for out in outputs]
        query_embeddings = torch.stack(embeddings_list)

        # query_embeddings = self.st_model.encode(texts, show_progress_bar=debug, convert_to_tensor=True)
        
        # 2. GPUì—ì„œ ìœ ì‚¬ë„ ê³„ì‚°
        reference_index = self.reference_indices[data_name]

        # cosine similarity
        query_embeddings = query_embeddings / query_embeddings.norm(dim=-1, keepdim=True)
        
        # 3. targetsì™€ neg_itemsê°€ ì œê³µë˜ì—ˆëŠ”ì§€ í™•ì¸
        if targets is not None and neg_items is not None:
            # target + negativesì— ëŒ€í•´ì„œë§Œ ìŠ¤ì½”ì–´ ê³„ì‚°
            batch_size = len(texts)
            num_negs = len(neg_items[0]) if neg_items else 0
            
            # ê²°ê³¼ í…ì„œ ì´ˆê¸°í™” [batch_size, 1 + num_negs]
            scores = torch.zeros(batch_size, 1 + num_negs, device=self.device)
            
            for i in range(batch_size):
                # ê° ìƒ˜í”Œë³„ë¡œ target + negatives ì¸ë±ì‹±
                item_indices = [targets[i]] + neg_items[i]
                item_embeddings = reference_index[item_indices]  # [1 + num_negs, emb_dim]
                
                # í•´ë‹¹ ì•„ì´í…œë“¤ê³¼ì˜ ìœ ì‚¬ë„ ê³„ì‚°
                scores[i] = torch.matmul(query_embeddings[i], item_embeddings.T)  # [1 + num_negs]
            
            return scores
        else:
            # ì „ì²´ ì¸ë±ìŠ¤ì™€ì˜ ìœ ì‚¬ë„ ê³„ì‚° (ê¸°ì¡´ ë™ì‘)
            scores = torch.matmul(query_embeddings, reference_index.T)
            return scores
    
# ì„œë²„ ì‹¤í–‰ ì½”ë“œ
if __name__ == "__main__":
    import argparse
    import time
    
    parser = argparse.ArgumentParser(description="Retrieval Service for RL4Rec")
    parser.add_argument(
        "--emb_model_name",
        type=str,
        default="mixedbread-ai/mxbai-embed-large-v1",
        help="Path or name of the embedding model"
    )
    parser.add_argument(
        "--emb_type",
        type=str,
        default="review_description",
        help="Type of embeddings to load (e.g., item, user)"
    )
    parser.add_argument(
        "--datasets",
        type=str,
        nargs="+",
        default=["default"],
        help="List of dataset names to load (e.g., --datasets dataset1 dataset2 dataset3)"
    )
    parser.add_argument(
        "--actor_name",
        type=str,
        default="RetrievalService",
        help="Name of the Ray actor"
    )
    parser.add_argument(
        "--namespace",
        type=str,
        default="ray_test",
        help="Ray namespace"
    )
    parser.add_argument(
        "--gpu_id",
        type=int,
        default=None,
        help="Specific GPU ID to use (e.g., 0, 1, 2). If not specified, uses any available GPU"
    )
    parser.add_argument(
        "--num_gpus",
        type=float,
        default=1.0,
        help="Number of GPUs to allocate (default: 1.0)"
    )
    parser.add_argument(
        "--detached",
        action="store_true",
        help="Run as detached actor (survives script termination)"
    )
    parser.add_argument(
        "--ray_address",
        type=str,
        default="auto",
        help="Ray cluster address (default: auto)"
    )
    
    args = parser.parse_args()

    args.emb_model_name_dir = args.emb_model_name.split("/")[-1]
    
    
    # Ray ì´ˆê¸°í™”
    ray.init(
        # address=args.ray_address, 
        namespace=args.namespace,
        )
    
    print(f"ğŸ“¦ Loading model: {args.emb_model_name}")
    print(f"ğŸ“Š Datasets: {args.datasets}")
    
    # Actor ì˜µì…˜ ì„¤ì •
    options = {
        "name": args.actor_name,
        "num_gpus": args.num_gpus,
    }
    
    if args.detached:
        options["lifetime"] = "detached"
        print("ğŸ”’ Running as detached actor")
    
    # Service ì‹œì‘
    service = RetrievalService.options(**options).remote(
        args=args,
    )
    
    print("âœ“ DB Server is up and running. Waiting for learners...")
    print(f"  Actor name: {args.actor_name}")
    print(f"  Namespace: {args.namespace}")
    print(f"  Available datasets: {args.datasets}")
    print("  You can now run training scripts to start training.")
    
    # Keep process alive
    while True: 
        time.sleep(10)