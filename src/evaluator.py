"""
Evaluation class for recommendation model assessment
"""
import gc
from pathlib import Path
from datetime import datetime
import numpy as np
import pandas as pd
import torch
from tqdm import tqdm
from vllm import LLM, SamplingParams
from vllm.pooling_params import PoolingParams

from utils.reward_funtion import calculate_ndcg, calculate_hit_rate


class RecommendationEvaluator:
    """
    ì¶”ì²œ ëª¨ë¸ í‰ê°€ë¥¼ ìœ„í•œ í´ë˜ìŠ¤
    vLLMì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ìƒì„± ë° ì„ë² ë”© ê¸°ë°˜ ê²€ìƒ‰ ìˆ˜í–‰
    """
    
    def __init__(self, args, checkpoint_dir):
        """
        Args:
            args: í•™ìŠµ ì„¤ì • íŒŒë¼ë¯¸í„°
            checkpoint_dir: í‰ê°€í•  ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ
        """
        self.args = args
        self.checkpoint_dir = checkpoint_dir
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # vLLM ëª¨ë¸ë“¤ (lazy loading)
        self.llm = None
        self.emb_llm = None
        self.item_embeddings = None
        
    def _load_generation_model(self):
        """í…ìŠ¤íŠ¸ ìƒì„±ì„ ìœ„í•œ vLLM ëª¨ë¸ ë¡œë“œ"""
        if self.llm is None:
            print("ğŸ¤– Loading model with vLLM for generation...")
            self.sampling_params = SamplingParams(
                # n=1,
                temperature=0.01,
                max_tokens=self.args.max_new_tokens,
                repetition_penalty=1.1,
                # stop=["<|eot_id|>", "<|reserved_special_token_0|>", "<eos>"],
            )
        
            self.llm = LLM(
                model=self.checkpoint_dir,
                tensor_parallel_size=1,
                dtype=torch.bfloat16,
                # dtype=torch.bfloat16 if self.args.bf16 else torch.float32,
                gpu_memory_utilization=self.args.gpu_memory_utilization,
                max_model_len=self.args.max_length,
                max_num_seqs=self.args.eval_batch_size,
            )
    
    def _load_embedding_model(self):
        """ì„ë² ë”© ê³„ì‚°ì„ ìœ„í•œ vLLM ëª¨ë¸ ë¡œë“œ"""
        if self.emb_llm is None:
            print("ğŸ” Loading embedding model for retrieval...")
            emb_model_name = getattr(self.args, 'emb_model_name', 'mixedbread-ai/mxbai-embed-large-v1')
            
            # Pooling íŒŒë¼ë¯¸í„° ì„¤ì •
            self.pooling_params = PoolingParams(
                truncate_prompt_tokens=self.args.eval_emb_max_length,
                task="embed",
            )
            
            self.emb_llm = LLM(
                model=emb_model_name,
                task="embed",
                enforce_eager=True,
                gpu_memory_utilization=self.args.gpu_memory_utilization,
                trust_remote_code=True,
                max_model_len=self.args.eval_emb_max_length,
                max_num_seqs=self.args.eval_emb_batch_size,
            )
    
    def _load_item_embeddings(self):
        """ì•„ì´í…œ ì„ë² ë”© ë¡œë“œ"""
        if self.item_embeddings is None:
            print("ğŸ“¦ Loading item embeddings...")
            emb_model_name = getattr(self.args, 'emb_model_name', 'mixedbread-ai/mxbai-embed-large-v1')
            emb_model_name_dir = emb_model_name.split('/')[-1]
            emb_type = getattr(self.args, 'emb_type', 'title')
            
            emb_file = f"data_emb/{self.args.data_name}_{emb_type}_{emb_model_name_dir}.pt"
            self.item_embeddings = torch.load(emb_file, map_location=self.device)
            self.item_embeddings = self.item_embeddings / self.item_embeddings.norm(dim=-1, keepdim=True)
            print(f"âœ“ Loaded item embeddings: {self.item_embeddings.shape}")
    
    def generate_all_texts(self, prompts):
        """
        ëª¨ë“  í”„ë¡¬í”„íŠ¸ì— ëŒ€í•´ í…ìŠ¤íŠ¸ ìƒì„±
        
        Args:
            prompts: ìƒì„±í•  í”„ë¡¬í”„íŠ¸ ë¦¬ìŠ¤íŠ¸
            batch_size: ë°°ì¹˜ í¬ê¸° (Noneì´ë©´ args.eval_batch_size ì‚¬ìš©)
        
        Returns:
            generated_texts: ìƒì„±ëœ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        """
        self._load_generation_model()
        # ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„° ì„¤ì •
        print(f"ğŸš€ Generating responses for {len(prompts)} prompts...")
        outputs = self.llm.generate(prompts, self.sampling_params)
        generated_texts = [output.outputs[0].text for output in outputs]
        return generated_texts
    
    def compute_embeddings(self, texts):
        """
        í…ìŠ¤íŠ¸ë“¤ì— ëŒ€í•œ ì„ë² ë”©ì„ í•œë²ˆì— ê³„ì‚°
        
        Args:
            texts: ì„ë² ë”©í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            batch_size: ë°°ì¹˜ í¬ê¸° (Noneì´ë©´ args.eval_emb_batch_size ì‚¬ìš©)
        
        Returns:
            query_embeddings: ì •ê·œí™”ëœ ì¿¼ë¦¬ ì„ë² ë”© [num_texts, emb_dim]
        """
        self._load_embedding_model()        
        print(f"ğŸ” Computing embeddings for {len(texts)} texts...")
        # add cls token
        texts = [f"[CLS] {text}" for text in texts]

        # Embedding ê³„ì‚°
        emb_outputs = self.emb_llm.encode(
            prompts=texts,
            pooling_task="embed",
            pooling_params=self.pooling_params,
            use_tqdm=True,
        )
        # Query embeddings ì¶”ì¶œ
        embeddings_list = [
            torch.as_tensor(out.outputs.data, dtype=torch.float32, device=self.device)
            for out in emb_outputs
        ]
        query_embeddings = torch.stack(embeddings_list)
        return query_embeddings
    
    def compute_retrieval_metrics(self, query_embeddings, targets, histories, ks=[5, 10, 20]):
        """
        ê²€ìƒ‰ ë©”íŠ¸ë¦­ ê³„ì‚° (ë°°ì¹˜ë³„ ì²˜ë¦¬)
        
        Args:
            query_embeddings: ì¿¼ë¦¬ ì„ë² ë”© [num_queries, emb_dim]
            targets: íƒ€ê²Ÿ ì•„ì´í…œ ID ë¦¬ìŠ¤íŠ¸
            histories: íˆìŠ¤í† ë¦¬ ì•„ì´í…œ ID ë¦¬ìŠ¤íŠ¸ì˜ ë¦¬ìŠ¤íŠ¸
            ks: í‰ê°€í•  k ê°’ë“¤
        
        Returns:
            metrics: ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬
            rank_info: ê° ìƒ˜í”Œì˜ rank ë° score ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        self._load_item_embeddings()
        
        print(f"ğŸ“Š Computing retrieval metrics...")
        
        # ë©”íŠ¸ë¦­ ì €ì¥ìš© ë”•ì…”ë„ˆë¦¬ ì´ˆê¸°í™”
        metrics = {f'ndcg@{k}': [] for k in ks}
        metrics.update({f'hit@{k}': [] for k in ks})
        
        # Rank ì •ë³´ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸
        rank_info = []
        
        # ë°°ì¹˜ í¬ê¸° ì„¤ì •
        batch_size = self.args.eval_batch_size
        num_queries = query_embeddings.shape[0]
        num_batches = (num_queries + batch_size - 1) // batch_size
        
        print(f"Processing {num_queries} queries in {num_batches} batches...")
        
        for batch_idx in tqdm(range(num_batches), desc="Computing metrics"):
            start_idx = batch_idx * batch_size
            end_idx = min(start_idx + batch_size, num_queries)
            
            # ë°°ì¹˜ ë°ì´í„° ì¶”ì¶œ
            batch_query_emb = query_embeddings[start_idx:end_idx]
            batch_targets = targets[start_idx:end_idx]
            batch_histories = histories[start_idx:end_idx]
            
            # ì „ì²´ ì•„ì´í…œê³¼ì˜ ìœ ì‚¬ë„ ê³„ì‚°
            batch_scores = torch.matmul(batch_query_emb, self.item_embeddings.T)  # [batch_size, num_items]
            
            # ê³¼ê±° êµ¬ë§¤ ì•„ì´í…œ ì œì™¸
            for i in range(len(batch_targets)):
                history_indices = [idx for idx in batch_histories[i] if idx != batch_targets[i]]
                if history_indices:
                    batch_scores[i, history_indices] = -float('inf')
            
            # ê° ìƒ˜í”Œì˜ rank ê³„ì‚°
            for i in range(len(batch_targets)):
                target_item = batch_targets[i]
                target_score = batch_scores[i, target_item].item()
                
                # íƒ€ê²Ÿ ì•„ì´í…œë³´ë‹¤ ë†’ì€ ì ìˆ˜ë¥¼ ê°€ì§„ ì•„ì´í…œì˜ ê°œìˆ˜ = rank - 1
                rank = (batch_scores[i] > target_score).sum().item() + 1
                
                rank_info.append({
                    'target_item': target_item,
                    'target_score': target_score,
                    'rank': rank,
                })
            
            # ê° kì— ëŒ€í•´ ë©”íŠ¸ë¦­ ê³„ì‚°
            for k in ks:
                # NDCG ê³„ì‚°
                ndcg_scores = calculate_ndcg(
                    batch_scores,
                    batch_targets,
                    batch_histories,
                    k=k,
                    use_negatives_only=False
                )
                metrics[f'ndcg@{k}'].extend(ndcg_scores.cpu().tolist())
                
                # Hit ê³„ì‚°
                hit_scores = calculate_hit_rate(
                    batch_scores,
                    batch_targets,
                    batch_histories,
                    k=k,
                    use_negatives_only=False
                )
                metrics[f'hit@{k}'].extend(hit_scores.cpu().tolist())
        
        return metrics, rank_info
    
    def evaluate(self, dataset, split="test", save_log=True):
        """
        ì „ì²´ í‰ê°€ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        
        Args:
            dataset: í‰ê°€í•  ë°ì´í„°ì…‹
            split: ë°ì´í„°ì…‹ split ì´ë¦„ ("test", "val" ë“±)
            save_log: ë¡œê·¸ íŒŒì¼ ì €ì¥ ì—¬ë¶€
        
        Returns:
            results: í‰ê°€ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        print(f"\n{'='*80}")
        print(f"ğŸ“Š Final Evaluation on {split.upper()} Set")
        print(f"{'='*80}")
        
        # 1. ë°ì´í„° ìˆ˜ì§‘
        print("ğŸ“ Collecting data...")
        all_prompts = []
        all_targets = []
        all_histories = []
        
        for i in range(len(dataset)):
            sample = dataset[i]
            all_prompts.append(sample["prompt"])
            all_targets.append(sample["target"])
            all_histories.append(sample["history"])

        if len(all_prompts) > self.args.eval_samples:
            all_prompts = all_prompts[:self.args.eval_samples]
            all_targets = all_targets[:self.args.eval_samples]
            all_histories = all_histories[:self.args.eval_samples]
        
        # 2. í…ìŠ¤íŠ¸ ìƒì„± (ëª¨ë“  í”„ë¡¬í”„íŠ¸ì— ëŒ€í•´)
        generated_texts = self.generate_all_texts(all_prompts)

        # ìƒì„± ëª¨ë¸ ì¦‰ì‹œ ì •ë¦¬ (ë©”ëª¨ë¦¬ ì ˆì•½)
        print("\nğŸ§¹ Cleaning up generation model...")
        if self.llm is not None:
            try:
                if hasattr(self.llm, 'llm_engine'):
                    del self.llm.llm_engine
            except:
                pass
            del self.llm
            self.llm = None
        torch.cuda.empty_cache()
        gc.collect()
        
        if torch.cuda.is_available():
            print(f"ğŸ’¾ GPU Memory after generation: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")
        
        # 3. ì„ë² ë”© ê³„ì‚° (ëª¨ë“  ìƒì„±ëœ í…ìŠ¤íŠ¸ì— ëŒ€í•´)
        query_embeddings = self.compute_embeddings(generated_texts)
        
        # ì„ë² ë”© ëª¨ë¸ë„ ì •ë¦¬ (ë©”íŠ¸ë¦­ ê³„ì‚° ì „)
        print("\nğŸ§¹ Cleaning up embedding model...")
        if self.emb_llm is not None:
            try:
                if hasattr(self.emb_llm, 'llm_engine'):
                    del self.emb_llm.llm_engine
            except:
                pass
            del self.emb_llm
            self.emb_llm = None
        torch.cuda.empty_cache()
        gc.collect()
        
        if torch.cuda.is_available():
            print(f"ğŸ’¾ GPU Memory after embedding: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")
        
        # 4. ë©”íŠ¸ë¦­ ê³„ì‚°
        metrics, rank_info = self.compute_retrieval_metrics(
            query_embeddings, 
            all_targets, 
            all_histories,
            ks=[5, 10, 20]
        )
        
        # 5. ìƒ˜í”Œ ì¶œë ¥ ì €ì¥ (ì²˜ìŒ 3ê°œ)
        sample_outputs = []
        num_samples = min(3, len(all_prompts))
        for i in range(num_samples):
            sample_outputs.append({
                'prompt': all_prompts[i],
                'generated': generated_texts[i],
                'target': all_targets[i],
                'history': all_histories[i]
            })
        
        # 6. ê²°ê³¼ ì¶œë ¥
        self._print_sample_outputs(sample_outputs)
        results = self._print_metrics(metrics, split)
        
        # 7. ë¡œê·¸ íŒŒì¼ ì €ì¥
        if save_log:
            self._save_log_file(results, metrics, sample_outputs, split)
            # CSV íŒŒì¼ ì €ì¥
            self._save_csv_file(
                all_prompts, 
                generated_texts, 
                all_targets, 
                all_histories,
                rank_info, 
                split
            )
        
        return results
    
    def _print_sample_outputs(self, sample_outputs):
        """ìƒ˜í”Œ í”„ë¡¬í”„íŠ¸ì™€ ìƒì„± ê²°ê³¼ ì¶œë ¥"""
        print(f"\n{'='*80}")
        print(f"ğŸ“ Sample Prompts and Generated Texts")
        print(f"{'='*80}")
        
        for idx, sample in enumerate(sample_outputs, 1):
            print(f"\n[Sample {idx}]")
            print(f"{'â”€'*80}")
            print(f"Target Item ID: {sample['target']}")
            print(f"History Items: {sample['history']}")
            print(f"\n[Prompt]")
            # í”„ë¡¬í”„íŠ¸ê°€ ê¸¸ë©´ ì• 300ìë§Œ ì¶œë ¥
            prompt_preview = sample['prompt'][:300] + "..." if len(sample['prompt']) > 300 else sample['prompt']
            print(prompt_preview)
            print(f"\n[Generated Text]")
            print(sample['generated'])
            print(f"{'â”€'*80}")
    
    def _print_metrics(self, metrics, split):
        """ë©”íŠ¸ë¦­ ì¶œë ¥ ë° í‰ê·  ê³„ì‚°"""
        print(f"\n{'='*80}")
        print(f"ğŸ“ˆ Final Evaluation Results ({split.upper()})")
        print(f"{'='*80}")
        
        results = {}
        for metric_name in ['hit@5', 'hit@10', 'hit@20', 'ndcg@5', 'ndcg@10', 'ndcg@20']:
            mean_val = np.mean(metrics[metric_name])
            results[metric_name] = float(mean_val)
            print(f"  {metric_name.upper()}: {mean_val:.4f}")
        
        print(f"{'='*80}\n")
        
        return results
    
    def _save_log_file(self, results, metrics, sample_outputs, split):
        """í‰ê°€ ê²°ê³¼ë¥¼ ë¡œê·¸ íŒŒì¼ë¡œ ì €ì¥"""
        results_dir = Path("results")
        results_dir.mkdir(exist_ok=True)
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_filename = f"{self.args.run_name}_{split}_eval.log"
        log_file = results_dir / log_filename
        
        with open(log_file, 'a') as f:
            # í—¤ë”
            f.write("="*80 + "\n")
            f.write(f"Evaluation Results - {split.upper()}\n")
            f.write(f"Run Name: {self.args.run_name}\n")
            f.write(f"Dataset: {self.args.data_name}\n")
            f.write(f"Model: {self.args.model_name}\n")
            f.write(f"Timestamp: {timestamp}\n")
            f.write("="*80 + "\n\n")
            
            # ë©”íŠ¸ë¦­ ê²°ê³¼
            f.write("EVALUATION METRICS\n")
            f.write("-"*80 + "\n")
            for metric_name, value in results.items():
                f.write(f"  {metric_name.upper()}: {value:.4f}\n")
            f.write("-"*80 + "\n\n")
            
            # ìƒ˜í”Œ ì¶œë ¥
            f.write("SAMPLE PROMPTS AND GENERATED TEXTS\n")
            f.write("="*80 + "\n")
            for idx, sample in enumerate(sample_outputs, 1):
                f.write(f"\n[Sample {idx}]\n")
                f.write("-"*80 + "\n")
                f.write(f"Target Item ID: {sample['target']}\n")
                f.write(f"History Items: {sample['history']}\n")
                f.write(f"\n[Prompt]\n")
                f.write(sample['prompt'] + "\n")
                f.write(f"\n[Generated Text]\n")
                f.write(sample['generated'] + "\n")
                f.write("-"*80 + "\n")
        
        print(f"ğŸ’¾ Evaluation results saved to: {log_file}")
    
    def _save_csv_file(self, prompts, generated_texts, targets, histories, rank_info, split):
        """í‰ê°€ ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥"""
        results_dir = Path("results")
        results_dir.mkdir(exist_ok=True)
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        csv_filename = f"{self.args.run_name}_{split}_eval_{timestamp}.csv"
        csv_file = results_dir / csv_filename
        
        # DataFrame ìƒì„±
        data = []
        for i in range(len(prompts)):
            data.append({
                'sample_id': i,
                'prompt': prompts[i],
                'generated_text': generated_texts[i],
                'target_item': targets[i],
                'history_items': str(histories[i]),  # ë¦¬ìŠ¤íŠ¸ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
                'target_score': rank_info[i]['target_score'],
                'rank': rank_info[i]['rank'],
                'hit@5': 1 if rank_info[i]['rank'] <= 5 else 0,
                'hit@10': 1 if rank_info[i]['rank'] <= 10 else 0,
                'hit@20': 1 if rank_info[i]['rank'] <= 20 else 0,
            })
        
        df = pd.DataFrame(data)
        
        # CSV ì €ì¥
        df.to_csv(csv_file, index=False, encoding='utf-8-sig')
        
        print(f"ğŸ’¾ Detailed results saved to CSV: {csv_file}")
        
        # ê°„ë‹¨í•œ í†µê³„ ì¶œë ¥
        print(f"\nğŸ“ˆ CSV Statistics:")
        print(f"  Total samples: {len(df)}")
        print(f"  Mean rank: {df['rank'].mean():.2f}")
        print(f"  Median rank: {df['rank'].median():.0f}")
        print(f"  Hit@5 rate: {df['hit@5'].mean():.4f}")
        print(f"  Hit@10 rate: {df['hit@10'].mean():.4f}")
        print(f"  Hit@20 rate: {df['hit@20'].mean():.4f}")
    
    def cleanup(self):
        """ë©”ëª¨ë¦¬ ì •ë¦¬"""
        print("\nğŸ§¹ Cleaning up evaluator resources...")
        
        # vLLM ìƒì„± ëª¨ë¸ ì •ë¦¬
        if self.llm is not None:
            try:
                # vLLMì˜ ê²½ìš° llm_engineì„ ëª…ì‹œì ìœ¼ë¡œ ì •ë¦¬
                if hasattr(self.llm, 'llm_engine'):
                    del self.llm.llm_engine
            except:
                pass
            del self.llm
            self.llm = None
            print("  âœ“ Generation model cleaned up")
        
        # vLLM ì„ë² ë”© ëª¨ë¸ ì •ë¦¬
        if self.emb_llm is not None:
            try:
                if hasattr(self.emb_llm, 'llm_engine'):
                    del self.emb_llm.llm_engine
            except:
                pass
            del self.emb_llm
            self.emb_llm = None
            print("  âœ“ Embedding model cleaned up")
        
        # ì•„ì´í…œ ì„ë² ë”© ì •ë¦¬
        if self.item_embeddings is not None:
            self.item_embeddings = self.item_embeddings.cpu()
            del self.item_embeddings
            self.item_embeddings = None
            print("  âœ“ Item embeddings cleaned up")
        
        # GPU ë©”ëª¨ë¦¬ ê°•ì œ í•´ì œ
        torch.cuda.empty_cache()
        gc.collect()
        
        if torch.cuda.is_available():
            torch.cuda.synchronize()
            print(f"  ğŸ’¾ GPU Memory after evaluator cleanup: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")
        
        print("âœ“ Evaluator cleanup complete")

