"""
Evaluation class for recommendation model assessment
"""
import gc
import json
from pathlib import Path
from datetime import datetime
import numpy as np
import pandas as pd
import torch
from tqdm import tqdm
from vllm import LLM, SamplingParams
from vllm.pooling_params import PoolingParams
from sentence_transformers import SentenceTransformer

from utils.reward_function import calculate_ndcg, calculate_hit_rate, extract_query_from_tags


def get_last_item_text(dataset, item_metadata, use_brand=True, use_category=True):
    """
    ê° ì‚¬ìš©ìì˜ ë§ˆì§€ë§‰ êµ¬ë§¤ ì•„ì´í…œ ì •ë³´ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
    
    Args:
        dataset: RecommendationDataset ì¸ìŠ¤í„´ìŠ¤
        item_metadata: ì•„ì´í…œ ë©”íƒ€ë°ì´í„°
        use_brand: ë¸Œëœë“œ í¬í•¨ ì—¬ë¶€
        use_category: ì¹´í…Œê³ ë¦¬ í¬í•¨ ì—¬ë¶€
    
    Returns:
        last_item_texts: ê° ìƒ˜í”Œì˜ ë§ˆì§€ë§‰ ì•„ì´í…œ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
    """
    last_item_texts = []
    
    for i in range(len(dataset)):
        data = dataset[i]
        history = data.get('history', [])
        
        if len(history) > 0:
            last_item_id = history[-1]  # ë§ˆì§€ë§‰ ì•„ì´í…œ
            item_info = item_metadata.get(last_item_id, {})
            
            text_parts = []
            title = item_info.get('title', '')
            # limit title length to 64 words
            title = " ".join(title.split()[:64])
            text_parts.append(f"Last Item: {title}")
            
            if use_brand:
                brand = item_info.get('brand', '')
                if brand:
                    text_parts.append(f"Brand: {brand}")
            
            if use_category:
                category = item_info.get('category', '')
                if category:
                    text_parts.append(f"Category: {category}")
            
            last_item_text = "\n".join(text_parts)
        else:
            last_item_text = ""
        
        last_item_texts.append(last_item_text)
    
    return last_item_texts


class RecommendationEvaluator:
    """
    ì¶”ì²œ ëª¨ë¸ í‰ê°€ë¥¼ ìœ„í•œ í´ë˜ìŠ¤
    vLLMì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ìƒì„± ë° ì„ë² ë”© ê¸°ë°˜ ê²€ìƒ‰ ìˆ˜í–‰
    """
    
    def __init__(self, args, checkpoint_dir, item_metadata=None):
        """
        Args:
            args: í•™ìŠµ ì„¤ì • íŒŒë¼ë¯¸í„°
            checkpoint_dir: í‰ê°€í•  ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ
            item_metadata: ì•„ì´í…œ ë©”íƒ€ë°ì´í„° (prepend_last_item ì‚¬ìš© ì‹œ í•„ìš”)
        """
        self.args = args
        self.checkpoint_dir = checkpoint_dir
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.item_metadata = item_metadata
        
        # vLLM ëª¨ë¸ë“¤ (lazy loading)
        self.llm = None
        self.emb_llm = None
        self.item_embeddings = None
        self.use_sentence_transformers = False  # SentenceTransformer ì‚¬ìš© ì—¬ë¶€
        
        # ì•„ì´í…œ ì¸ê¸°ë„ ë° Novelty ê³„ì‚°ìš©
        self.item_popularity = None
        self.item_novelty = None
        self.hot_items = None  # ìƒìœ„ 20% ì¸ê¸° ì•„ì´í…œ ì„¸íŠ¸
        self.cold_items = None  # ë‚˜ë¨¸ì§€ 80% ì•„ì´í…œ ì„¸íŠ¸

        self._load_item_embeddings()
        self._compute_item_popularity()
        # Cold/Hot êµ¬ë¶„
        
        
    def _load_generation_model(self):
        """í…ìŠ¤íŠ¸ ìƒì„±ì„ ìœ„í•œ vLLM ëª¨ë¸ ë¡œë“œ"""
        if hasattr(self.args, 'zeroshot_evaluation') and self.args.zeroshot_evaluation:
            print("ğŸ¤– Loading model with vLLM for zeroshot evaluation...")
            self.sampling_params = SamplingParams(
                temperature=0.01,
                max_tokens=self.args.eval_max_tokens,
                repetition_penalty=1.1,
            )
            self.llm = LLM(
                model=self.args.model_name,
                tensor_parallel_size=1,
                dtype=torch.bfloat16,
                gpu_memory_utilization=self.args.gpu_memory_utilization,
                max_model_len=self.args.max_length,
                max_num_seqs=self.args.eval_batch_size,
            )

        if self.llm is None:
            print("ğŸ¤– Loading model with vLLM for generation...")
            self.sampling_params = SamplingParams(
                # n=1,
                temperature=0.01,
                max_tokens=self.args.eval_max_tokens,
                repetition_penalty=1.1,
                # stop=["<|eot_id|>", "<|reserved_special_token_0|>", "<eos>"],
            )
        
            self.llm = LLM(
                model=self.checkpoint_dir,
                tensor_parallel_size=1,
                dtype=torch.bfloat16,
                # dtype=torch.bfloat16 if self.args.bf16 else torch.float32,
                gpu_memory_utilization=self.args.gpu_memory_utilization,
                max_model_len=self.args.max_length,
                max_num_seqs=self.args.eval_batch_size,
            )
    
    def _load_embedding_model(self):
        """ì„ë² ë”© ê³„ì‚°ì„ ìœ„í•œ vLLM ë˜ëŠ” SentenceTransformer ëª¨ë¸ ë¡œë“œ"""
        if self.emb_llm is None:
            print("ğŸ” Loading embedding model for retrieval...")
            emb_model_name = getattr(self.args, 'emb_model_name', 'mixedbread-ai/mxbai-embed-large-v1')
            use_sentence_transformers = getattr(self.args, 'use_sentence_transformers', False)
            
            # SentenceTransformer ì‚¬ìš© ì¡°ê±´ ì²´í¬
            if use_sentence_transformers:
                print("ğŸ” Using SentenceTransformer for embedding...")
                self.emb_llm = SentenceTransformer(emb_model_name)
                self.use_sentence_transformers = True
            else:
                print("ğŸ” Using vLLM for embedding...")
                # Pooling íŒŒë¼ë¯¸í„° ì„¤ì •
                self.pooling_params = PoolingParams(
                    truncate_prompt_tokens=self.args.eval_emb_max_length,
                    task="embed",
                )
                
                self.emb_llm = LLM(
                    model=emb_model_name,
                    # task="embed",
                    runner="pooling",
                    enforce_eager=True,
                    gpu_memory_utilization=self.args.gpu_memory_utilization,
                    trust_remote_code=True,
                    max_model_len=self.args.eval_emb_max_length,
                    max_num_seqs=self.args.eval_emb_batch_size,
                )
                self.use_sentence_transformers = False
    
    def _load_item_embeddings(self):
        """ì•„ì´í…œ ì„ë² ë”© ë¡œë“œ"""
        if self.item_embeddings is None:
            print("ğŸ“¦ Loading item embeddings...")
            emb_model_name = getattr(self.args, 'emb_model_name', 'mixedbread-ai/mxbai-embed-large-v1')
            emb_model_name_dir = emb_model_name.split('/')[-1]
            emb_type = getattr(self.args, 'emb_type', 'title')
            
            try:
                emb_file = f"data_emb/{self.args.data_name}_{emb_type}_{emb_model_name_dir}.pt"
                self.item_embeddings = torch.load(emb_file, map_location=self.device)
            except:
                emb_file = f"data_emb/{self.args.data_name}_{emb_type}_{emb_model_name_dir}_emb.pt"
                self.item_embeddings = torch.load(emb_file, map_location=self.device)
            self.item_embeddings = self.item_embeddings / self.item_embeddings.norm(dim=-1, keepdim=True)
            print(f"âœ“ Loaded item embeddings: {self.item_embeddings.shape}")
    
    def _compute_item_popularity(self):
        """
        ì•„ì´í…œ ì¸ê¸°ë„ ë° Novelty ê³„ì‚° (ìºì‹± ì§€ì›)
        Hot/Cold ì•„ì´í…œ êµ¬ë¶„: ìƒìœ„ 20% ì¸ê¸°ë„ = Hot, ë‚˜ë¨¸ì§€ 80% = Cold
        
        train setì˜ sequential_data.txt íŒŒì¼ì„ ì½ì–´ì„œ ê³„ì‚°
        
        3ê°œì˜ íŒŒì¼ë¡œ ë¶„ë¦¬ ì €ì¥:
        - item_popularity.npy: numpy ë°°ì—´
        - item_novelty.npy: numpy ë°°ì—´
        - item_cold_hot.json: {"cold_items": [...], "hot_items": [...]}
        """
        if self.item_popularity is None:
            # ìºì‹œ íŒŒì¼ ê²½ë¡œ
            cache_dir = Path(f"./data/{self.args.data_name}")
            popularity_file = cache_dir / "item_popularity.npy"
            novelty_file = cache_dir / "item_novelty.npy"
            cold_hot_file = cache_dir / "item_cold_hot.json"
            
            # ìºì‹œ íŒŒì¼ì´ ëª¨ë‘ ì¡´ì¬í•˜ë©´ ë¡œë“œ
            if popularity_file.exists() and novelty_file.exists() and cold_hot_file.exists():
                print(f"ğŸ“¦ Loading cached item data from {cache_dir}...")
                try:
                    # Popularity ë¡œë“œ (.npy íŒŒì¼)
                    self.item_popularity = np.load(popularity_file)
                    
                    # Novelty ë¡œë“œ (.npy íŒŒì¼)
                    self.item_novelty = np.load(novelty_file)
                    
                    # Cold/Hot ë¡œë“œ (JSON íŒŒì¼)
                    with open(cold_hot_file, 'r') as f:
                        cold_hot_data = json.load(f)
                    
                    # Cold/Hot itemsë¥¼ setìœ¼ë¡œ ë³€í™˜
                    self.cold_items = set(cold_hot_data['cold_items'])
                    self.hot_items = set(cold_hot_data['hot_items'])
                    
                    max_pop = self.item_popularity.max()
                    print(f"âœ“ Item data loaded from cache. Max popularity: {max_pop:.4f}, Mean novelty: {self.item_novelty.mean():.4f}")
                    print(f"âœ“ Hot items: {len(self.hot_items)}, Cold items: {len(self.cold_items)}")
                    return
                except Exception as e:
                    print(f"âš ï¸  Failed to load cache: {e}. Computing from scratch...")
            
            # ìºì‹œê°€ ì—†ìœ¼ë©´ ê³„ì‚°
            print("ğŸ“Š Computing item popularity and novelty from sequential_data.txt...")
            
            # ì „ì²´ ì•„ì´í…œ ìˆ˜
            num_items = self.item_embeddings.shape[0]
            
            # sequential_data.txt íŒŒì¼ ì½ê¸°
            sequential_file = Path(f"./data/{self.args.data_name}/sequential_data.txt")
            # ê° ì•„ì´í…œì˜ ì¶œí˜„ íšŸìˆ˜ ê³„ì‚°
            item_counts = np.zeros(num_items + 1, dtype=np.int32)  # 1-indexed
            
            with open(sequential_file, 'r') as f:
                for line in f:
                    parts = line.strip().split()
                    # ì²« ë²ˆì§¸ëŠ” user_id, ë‚˜ë¨¸ì§€ëŠ” ì•„ì´í…œ ì‹œí€€ìŠ¤ (history + target)
                    items = [int(x) for x in parts[1:-2]]
                    
                    # ëª¨ë“  ì•„ì´í…œì˜ ì¶œí˜„ íšŸìˆ˜ ì¹´ìš´íŠ¸
                    for item_id in items:
                        item_counts[item_id] += 1
            
            print(f"âœ“ Loaded {sequential_file}, total items processed")
            
            # ì¸ê¸°ë„ ì €ì¥
            self.item_popularity = (item_counts+1) / item_counts.sum()
            max_count = item_counts.max()
            self.item_novelty = -np.log(self.item_popularity)
            
            # Hot/Cold ì•„ì´í…œ êµ¬ë¶„: ìƒìœ„ 20% ì¸ê¸°ë„ë¥¼ ê¸°ì¤€ìœ¼ë¡œ
            # ì¸ê¸°ë„ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ 20% threshold ê³„ì‚°
            sorted_counts = np.sort(item_counts)[::-1]  # ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
            threshold_idx = int(len(sorted_counts) * 0.2)
            threshold = sorted_counts[threshold_idx] if threshold_idx < len(sorted_counts) else 0
            
            # Hot items: ì¸ê¸°ë„ê°€ threshold ì´ìƒì¸ ì•„ì´í…œ (1-indexed)
            # Cold items: ì¸ê¸°ë„ê°€ threshold ë¯¸ë§Œì¸ ì•„ì´í…œ (1-indexed)
            hot_mask = (item_counts >= threshold) & (np.arange(len(item_counts)) > 0)
            cold_mask = (item_counts < threshold) & (np.arange(len(item_counts)) > 0)
            
            self.hot_items = set(np.where(hot_mask)[0].tolist())
            self.cold_items = set(np.where(cold_mask)[0].tolist())
            
            print(f"âœ“ Item popularity computed. Max count: {max_count}, Mean novelty: {self.item_novelty.mean():.4f}")
            print(f"âœ“ Hot items (top 20%): {len(self.hot_items)}, Cold items (80%): {len(self.cold_items)}, Threshold: {threshold}")
            
            # ìºì‹œ íŒŒì¼ì— ì €ì¥ (3ê°œì˜ íŒŒì¼ë¡œ ë¶„ë¦¬)
            try:
                # ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
                cache_dir.mkdir(parents=True, exist_ok=True)
                
                # 1. Popularity ì €ì¥ (.npy íŒŒì¼)
                np.save(popularity_file, self.item_popularity)
                print(f"ğŸ’¾ Item popularity cached to {popularity_file}")
                
                # 2. Novelty ì €ì¥ (.npy íŒŒì¼)
                np.save(novelty_file, self.item_novelty)
                print(f"ğŸ’¾ Item novelty cached to {novelty_file}")
                
                # 3. Cold/Hot ì €ì¥ (JSON íŒŒì¼)
                cold_hot_data = {
                    'cold_items': list(self.cold_items),
                    'hot_items': list(self.hot_items),
                    'threshold': int(threshold),
                    'num_cold': len(self.cold_items),
                    'num_hot': len(self.hot_items),
                }
                with open(cold_hot_file, 'w') as f:
                    json.dump(cold_hot_data, f)
                print(f"ğŸ’¾ Cold/Hot items cached to {cold_hot_file}")
                
            except Exception as e:
                print(f"âš ï¸  Failed to save cache: {e}")
    
    def _identify_cold_warm_items(self, targets):
        """
        Cold/Hot íƒ€ê²Ÿ ì•„ì´í…œ êµ¬ë¶„
        ì „ì²´ train ë°ì´í„° ê¸°ë°˜ ì¸ê¸°ë„ë¡œ ìƒìœ„ 20% = Hot, ë‚˜ë¨¸ì§€ 80% = Cold
        
        Args:
            targets: íƒ€ê²Ÿ ì•„ì´í…œ ID ë¦¬ìŠ¤íŠ¸
        
        Returns:
            cold_indices: Cold íƒ€ê²Ÿì˜ ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸
            hot_indices: Hot íƒ€ê²Ÿì˜ ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸
        """
        if self.hot_items is None or self.cold_items is None:
            raise ValueError("Hot/Cold items not computed. Call _compute_item_popularity first.")
        
        cold_indices = []
        hot_indices = []
        
        for i, target in enumerate(targets):
            # íƒ€ê²Ÿ ì•„ì´í…œì´ hot_items ì„¸íŠ¸ì— ìˆìœ¼ë©´ hot, ì—†ìœ¼ë©´ cold
            if target in self.hot_items:
                hot_indices.append(i)
            else:
                cold_indices.append(i)
        
        return cold_indices, hot_indices
    
    def generate_all_texts(self, prompts):
        """
        ëª¨ë“  í”„ë¡¬í”„íŠ¸ì— ëŒ€í•´ í…ìŠ¤íŠ¸ ìƒì„±
        
        Args:
            prompts: ìƒì„±í•  í”„ë¡¬í”„íŠ¸ ë¦¬ìŠ¤íŠ¸
            batch_size: ë°°ì¹˜ í¬ê¸° (Noneì´ë©´ args.eval_batch_size ì‚¬ìš©)
        
        Returns:
            generated_texts: ìƒì„±ëœ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        """
        self._load_generation_model()
        # ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„° ì„¤ì •
        print(f"ğŸš€ Generating responses for {len(prompts)} prompts...")
        outputs = self.llm.generate(prompts, self.sampling_params)
        generated_texts = [output.outputs[0].text for output in outputs]
        return generated_texts
    
    def compute_embeddings(self, texts):
        """
        í…ìŠ¤íŠ¸ë“¤ì— ëŒ€í•œ ì„ë² ë”©ì„ í•œë²ˆì— ê³„ì‚°
        <query> íƒœê·¸ê°€ ìˆëŠ” ê²½ìš° íƒœê·¸ ë‚´ë¶€ì˜ í…ìŠ¤íŠ¸ë§Œ ì‚¬ìš©
        
        Args:
            texts: ì„ë² ë”©í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            batch_size: ë°°ì¹˜ í¬ê¸° (Noneì´ë©´ args.eval_emb_batch_size ì‚¬ìš©)
        
        Returns:
            query_embeddings: ì •ê·œí™”ëœ ì¿¼ë¦¬ ì„ë² ë”© [num_texts, emb_dim]
        """
        self._load_embedding_model()
        
        # <query> íƒœê·¸ê°€ ìˆìœ¼ë©´ ì¶”ì¶œ, ì—†ìœ¼ë©´ ì›ë³¸ ì‚¬ìš©
        processed_texts = [extract_query_from_tags(text, tag="query") for text in texts]
        
        print(f"ğŸ” Computing embeddings for {len(processed_texts)} texts...")
        
        if self.use_sentence_transformers:
            # SentenceTransformer ì‚¬ìš©
            print("ğŸ” Using SentenceTransformer encode...")
            query_embeddings = self.emb_llm.encode(
                processed_texts, 
                batch_size=self.args.eval_emb_batch_size, 
                show_progress_bar=True, 
                convert_to_tensor=True
            )

        else:
            # vLLM ì‚¬ìš©
            # add cls token
            processed_texts_with_cls = [f"[CLS] {text}" for text in processed_texts]

            # Embedding ê³„ì‚°
            emb_outputs = self.emb_llm.encode(
                prompts=processed_texts_with_cls,
                pooling_task="embed",
                pooling_params=self.pooling_params,
                use_tqdm=True,
            )
            # Query embeddings ì¶”ì¶œ
            embeddings_list = [
                torch.as_tensor(out.outputs.data, dtype=torch.float32, device=self.device)
                for out in emb_outputs
            ]
            query_embeddings = torch.stack(embeddings_list)
        
        return query_embeddings
    
    def compute_retrieval_metrics(self, query_embeddings, targets, histories, ks=[5, 10, 20]):
        """
        ê²€ìƒ‰ ë©”íŠ¸ë¦­ ê³„ì‚° (ë°°ì¹˜ë³„ ì²˜ë¦¬)
        Cold/Hot êµ¬ë¶„ ë° Novelty ë©”íŠ¸ë¦­ í¬í•¨
        - Cold: ì¸ê¸°ë„ í•˜ìœ„ 80% ì•„ì´í…œ
        - Hot: ì¸ê¸°ë„ ìƒìœ„ 20% ì•„ì´í…œ
        
        Args:
            query_embeddings: ì¿¼ë¦¬ ì„ë² ë”© [num_queries, emb_dim]
            targets: íƒ€ê²Ÿ ì•„ì´í…œ ID ë¦¬ìŠ¤íŠ¸
            histories: íˆìŠ¤í† ë¦¬ ì•„ì´í…œ ID ë¦¬ìŠ¤íŠ¸ì˜ ë¦¬ìŠ¤íŠ¸
            ks: í‰ê°€í•  k ê°’ë“¤
        
        Returns:
            metrics: ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬
            rank_info: ê° ìƒ˜í”Œì˜ rank ë° score ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        cold_indices, hot_indices = self._identify_cold_warm_items(targets)
        print(f"  Cold targets: {len(cold_indices)}, Hot targets: {len(hot_indices)}")
        self.cold_indices = cold_indices
        self.hot_indices = hot_indices
        print(f"ğŸ“Š Computing retrieval metrics...")
        
        # ë©”íŠ¸ë¦­ ì €ì¥ìš© ë”•ì…”ë„ˆë¦¬ ì´ˆê¸°í™”
        metrics = {f'ndcg@{k}': [] for k in ks}
        metrics.update({f'hit@{k}': [] for k in ks})
        metrics.update({f'cold_ndcg@{k}': [] for k in ks})
        metrics.update({f'cold_hit@{k}': [] for k in ks})
        metrics.update({f'hot_ndcg@{k}': [] for k in ks})
        metrics.update({f'hot_hit@{k}': [] for k in ks})
        metrics.update({f'novelty@{k}': [] for k in ks})
        metrics.update({f'novelty_hit@{k}': [] for k in ks})
        
        # Rank ì •ë³´ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸
        rank_info = []
        
        # ë°°ì¹˜ í¬ê¸° ì„¤ì •
        batch_size = self.args.eval_batch_size
        num_queries = query_embeddings.shape[0]
        num_batches = (num_queries + batch_size - 1) // batch_size
        
        print(f"Processing {num_queries} queries in {num_batches} batches...")
        
        for batch_idx in tqdm(range(num_batches), desc="Computing metrics"):
            start_idx = batch_idx * batch_size
            end_idx = min(start_idx + batch_size, num_queries)
            
            # ë°°ì¹˜ ë°ì´í„° ì¶”ì¶œ
            batch_query_emb = query_embeddings[start_idx:end_idx]
            batch_targets = targets[start_idx:end_idx]
            batch_histories = histories[start_idx:end_idx]
            
            # ì „ì²´ ì•„ì´í…œê³¼ì˜ ìœ ì‚¬ë„ ê³„ì‚°
            batch_scores = torch.matmul(batch_query_emb, self.item_embeddings.T)  # [batch_size, num_items]
            
            # ê³¼ê±° êµ¬ë§¤ ì•„ì´í…œ ì œì™¸
            for i in range(len(batch_targets)):
                history_indices = [idx for idx in batch_histories[i] if idx != batch_targets[i]]
                if history_indices:
                    batch_scores[i, history_indices] = -float('inf')
            
            # ê° ìƒ˜í”Œì˜ rank ê³„ì‚° ë° Novelty ë©”íŠ¸ë¦­
            for i in range(len(batch_targets)):
                global_idx = start_idx + i
                target_item = batch_targets[i]
                target_score = batch_scores[i, target_item].item()
                
                # íƒ€ê²Ÿ ì•„ì´í…œë³´ë‹¤ ë†’ì€ ì ìˆ˜ë¥¼ ê°€ì§„ ì•„ì´í…œì˜ ê°œìˆ˜ = rank - 1
                rank = (batch_scores[i] > target_score).sum().item() + 1
                
                # Cold/Warm êµ¬ë¶„
                is_cold = global_idx in self.cold_indices
                
                rank_info.append({
                    'target_item': target_item,
                    'target_score': target_score,
                    'rank': rank,
                    'is_cold': is_cold,
                    'target_novelty': float(self.item_novelty[target_item]),
                })
            
            # ê° kì— ëŒ€í•´ ë©”íŠ¸ë¦­ ê³„ì‚°
            for k in ks:
                # NDCG ê³„ì‚°
                ndcg_scores = calculate_ndcg(
                    batch_scores,
                    batch_targets,
                    batch_histories,
                    k=k,
                    use_negatives_only=False
                )
                metrics[f'ndcg@{k}'].extend(ndcg_scores.cpu().tolist())
                
                # Hit ê³„ì‚°
                hit_scores = calculate_hit_rate(
                    batch_scores,
                    batch_targets,
                    batch_histories,
                    k=k,
                    use_negatives_only=False
                )
                metrics[f'hit@{k}'].extend(hit_scores.cpu().tolist())
                
                # Cold/Hot ë³„ ë©”íŠ¸ë¦­ ê³„ì‚°
                for i in range(len(batch_targets)):
                    global_idx = start_idx + i
                    is_cold = global_idx in self.cold_indices
                    
                    if is_cold:
                        metrics[f'cold_ndcg@{k}'].append(ndcg_scores[i].item())
                        metrics[f'cold_hit@{k}'].append(hit_scores[i].item())
                    else:
                        metrics[f'hot_ndcg@{k}'].append(ndcg_scores[i].item())
                        metrics[f'hot_hit@{k}'].append(hit_scores[i].item())
                
                # Novelty ë©”íŠ¸ë¦­ ê³„ì‚°
                for i in range(len(batch_targets)):
                    global_idx = start_idx + i
                    target_item = batch_targets[i]
                    
                    # Top-k ì•„ì´í…œ ì¶”ì¶œ
                    top_k_items = torch.topk(batch_scores[i], k).indices.cpu().numpy()
                    
                    # Top-k ì•„ì´í…œë“¤ì˜ í‰ê·  Novelty
                    avg_novelty = np.mean([self.item_novelty[item] for item in top_k_items])
                    metrics[f'novelty@{k}'].append(avg_novelty)
                    
                    # Hit@kì— íƒ€ê²Ÿ ì•„ì´í…œì˜ Noveltyë¥¼ ì ìš©
                    hit_value = hit_scores[i].item()
                    target_novelty = self.item_novelty[target_item]
                    novelty_hit = hit_value * target_novelty
                    metrics[f'novelty_hit@{k}'].append(novelty_hit)
        
        return metrics, rank_info
    
    def evaluate(self, dataset, split="test", save_log=True, pre_generated_texts=None):
        """
        ì „ì²´ í‰ê°€ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        
        Args:
            dataset: í‰ê°€í•  ë°ì´í„°ì…‹
            split: ë°ì´í„°ì…‹ split ì´ë¦„ ("test", "val" ë“±)
            save_log: ë¡œê·¸ íŒŒì¼ ì €ì¥ ì—¬ë¶€
            pre_generated_texts: ë¯¸ë¦¬ ìƒì„±ëœ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ (ì„ íƒì‚¬í•­)
        
        Returns:
            results: í‰ê°€ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        
        Note:
            - args.prepend_last_itemì´ Trueì´ê³  item_metadataê°€ ì œê³µëœ ê²½ìš°,
              ë§ˆì§€ë§‰ êµ¬ë§¤ ì•„ì´í…œ ì •ë³´ë¥¼ ìƒì„±ëœ í…ìŠ¤íŠ¸ ì•ì— ì¶”ê°€í•©ë‹ˆë‹¤.
        """
        print(f"\n{'='*80}")
        print(f"ğŸ“Š Final Evaluation on {split.upper()} Set")
        print(f"{'='*80}")
        
        # 1. ë°ì´í„° ìˆ˜ì§‘
        print("ğŸ“ Collecting data...")
        all_prompts = []
        all_targets = []
        all_histories = []
        all_user_ids = []
        
        for i in range(len(dataset)):
            sample = dataset[i]
            all_prompts.append(sample["prompt"])
            all_targets.append(sample["target"])
            all_histories.append(sample["history"])
            all_user_ids.append(sample["user_id"])

        if len(all_prompts) > self.args.eval_samples:
            all_prompts = all_prompts[:self.args.eval_samples]
            all_targets = all_targets[:self.args.eval_samples]
            all_histories = all_histories[:self.args.eval_samples]
            all_user_ids = all_user_ids[:self.args.eval_samples]
        
        # 2. í…ìŠ¤íŠ¸ ìƒì„± (ëª¨ë“  í”„ë¡¬í”„íŠ¸ì— ëŒ€í•´)
        if pre_generated_texts is not None:
            print("ğŸ“„ Using pre-generated texts from CSV...")
            generated_texts = pre_generated_texts[:len(all_prompts)]
            print(f"  Loaded {len(generated_texts)} pre-generated texts")
        elif hasattr(self.args, "dummy_generation") and self.args.dummy_generation:
            generated_texts = all_prompts
        else:
            generated_texts = self.generate_all_texts(all_prompts)

        # ìƒì„± ëª¨ë¸ ì¦‰ì‹œ ì •ë¦¬ (ë©”ëª¨ë¦¬ ì ˆì•½)
        if pre_generated_texts is None:  # ìƒì„± ëª¨ë¸ì„ ì‚¬ìš©í•œ ê²½ìš°ì—ë§Œ ì •ë¦¬
            print("\nğŸ§¹ Cleaning up generation model...")
            if self.llm is not None:
                try:
                    if hasattr(self.llm, 'llm_engine'):
                        del self.llm.llm_engine
                except:
                    pass
                del self.llm
                self.llm = None
            torch.cuda.empty_cache()
            gc.collect()
            
            if torch.cuda.is_available():
                print(f"ğŸ’¾ GPU Memory after generation: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")
        
        # 2.5. ë§ˆì§€ë§‰ ì•„ì´í…œ prepend ì˜µì…˜ ì²˜ë¦¬
        if hasattr(self.args, 'prepend_last_item') and self.args.prepend_last_item:
            if self.item_metadata is None:
                print("âš ï¸  Warning: prepend_last_item is enabled but item_metadata is not provided. Skipping prepending.")
            else:
                print("ğŸ“ Prepending last purchased item to generated texts...")
                last_item_texts = get_last_item_text(
                    dataset, 
                    self.item_metadata,
                    use_brand=getattr(self.args, 'use_brand', True),
                    use_category=getattr(self.args, 'use_category', True)
                )
                
                # ë§ˆì§€ë§‰ ì•„ì´í…œ í…ìŠ¤íŠ¸ë¥¼ generated text ì•ì— ì¶”ê°€
                modified_texts = []
                for last_item_text, generated_text in zip(last_item_texts, generated_texts):
                    if last_item_text:
                        modified_text = f"{last_item_text}\n\n{generated_text}"
                    else:
                        modified_text = generated_text
                    modified_texts.append(modified_text)
                
                generated_texts = modified_texts
                print(f"âœ“ Prepended last item to {len(generated_texts)} texts")
                
                # ìƒ˜í”Œ ì¶œë ¥
                print("\n" + "="*80)
                print("ğŸ“ Sample Modified Text (with last item prepended):")
                print("="*80)
                if len(generated_texts) > 0:
                    print(generated_texts[0][:500] + "..." if len(generated_texts[0]) > 500 else generated_texts[0])
                    print("="*80)
        
        # 3. ì„ë² ë”© ê³„ì‚° (ëª¨ë“  ìƒì„±ëœ í…ìŠ¤íŠ¸ì— ëŒ€í•´)
        query_embeddings = self.compute_embeddings(generated_texts)
        
        # ì„ë² ë”© ëª¨ë¸ë„ ì •ë¦¬ (ë©”íŠ¸ë¦­ ê³„ì‚° ì „)
        print("\nğŸ§¹ Cleaning up embedding model...")
        if self.emb_llm is not None:
            try:
                if hasattr(self.emb_llm, 'llm_engine'):
                    del self.emb_llm.llm_engine
            except:
                pass
            del self.emb_llm
            self.emb_llm = None
        torch.cuda.empty_cache()
        gc.collect()
        
        if torch.cuda.is_available():
            print(f"ğŸ’¾ GPU Memory after embedding: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")
        
        # 4. ë©”íŠ¸ë¦­ ê³„ì‚°
        metrics, rank_info = self.compute_retrieval_metrics(
            query_embeddings, 
            all_targets, 
            all_histories,
            ks=[5, 10, 20]
        )
        
        # 5. ìƒ˜í”Œ ì¶œë ¥ ì €ì¥ (ì²˜ìŒ 3ê°œ)
        sample_outputs = []
        num_samples = min(3, len(all_prompts))
        for i in range(num_samples):
            sample_outputs.append({
                'prompt': all_prompts[i],
                'generated': generated_texts[i],
                'target': all_targets[i],
                'history': all_histories[i]
            })
        
        # 6. ê²°ê³¼ ì¶œë ¥
        self._print_sample_outputs(sample_outputs)
        results = self._print_metrics(metrics, split)
        
        # 7. ë¡œê·¸ íŒŒì¼ ì €ì¥
        if save_log:
            self._save_log_file(results, metrics, sample_outputs, split)
            # CSV íŒŒì¼ ì €ì¥
            self._save_csv_file(
                all_prompts, 
                generated_texts, 
                all_targets, 
                all_histories,
                all_user_ids,
                rank_info, 
                split
            )
            # ë§ˆìŠ¤í„° ë¡œê·¸ íŒŒì¼ ì €ì¥
            self._save_master_log(results, split)
        
        return results
    
    def _print_sample_outputs(self, sample_outputs):
        """ìƒ˜í”Œ í”„ë¡¬í”„íŠ¸ì™€ ìƒì„± ê²°ê³¼ ì¶œë ¥"""
        print(f"\n{'='*80}")
        print(f"ğŸ“ Sample Prompts and Generated Texts")
        print(f"{'='*80}")
        
        for idx, sample in enumerate(sample_outputs, 1):
            print(f"\n[Sample {idx}]")
            print(f"{'â”€'*80}")
            print(f"Target Item ID: {sample['target']}")
            print(f"History Items: {sample['history']}")
            print(f"\n[Prompt]")
            # í”„ë¡¬í”„íŠ¸ê°€ ê¸¸ë©´ ì• 300ìë§Œ ì¶œë ¥
            prompt_preview = sample['prompt'][:300] + "..." if len(sample['prompt']) > 300 else sample['prompt']
            print(prompt_preview)
            print(f"\n[Generated Text]")
            print(sample['generated'])
            print(f"{'â”€'*80}")
    
    def _print_metrics(self, metrics, split):
        """ë©”íŠ¸ë¦­ ì¶œë ¥ ë° í‰ê·  ê³„ì‚°"""
        print(f"\n{'='*80}")
        print(f"ğŸ“ˆ Final Evaluation Results ({split.upper()})")
        print(f"{'='*80}")
        
        results = {}
        
        # ê¸°ë³¸ ë©”íŠ¸ë¦­
        print("\n[Overall Metrics]")
        for metric_name in ['hit@5', 'hit@10', 'hit@20', 'ndcg@5', 'ndcg@10', 'ndcg@20']:
            if metric_name in metrics and len(metrics[metric_name]) > 0:
                mean_val = np.mean(metrics[metric_name])
                results[metric_name] = float(mean_val)
                print(f"  {metric_name.upper()}: {mean_val:.4f}")
        
        # Cold ë©”íŠ¸ë¦­
        print("\n[Cold Items Metrics (80%)]")
        for metric_name in ['cold_hit@5', 'cold_hit@10', 'cold_hit@20', 'cold_ndcg@5', 'cold_ndcg@10', 'cold_ndcg@20']:
            if metric_name in metrics and len(metrics[metric_name]) > 0:
                mean_val = np.mean(metrics[metric_name])
                results[metric_name] = float(mean_val)
                print(f"  {metric_name.upper()}: {mean_val:.4f}")
            else:
                results[metric_name] = 0.0
                print(f"  {metric_name.upper()}: N/A (no cold items)")
        
        # Hot ë©”íŠ¸ë¦­
        print("\n[Hot Items Metrics (Top 20%)]")
        for metric_name in ['hot_hit@5', 'hot_hit@10', 'hot_hit@20', 'hot_ndcg@5', 'hot_ndcg@10', 'hot_ndcg@20']:
            if metric_name in metrics and len(metrics[metric_name]) > 0:
                mean_val = np.mean(metrics[metric_name])
                results[metric_name] = float(mean_val)
                print(f"  {metric_name.upper()}: {mean_val:.4f}")
            else:
                results[metric_name] = 0.0
                print(f"  {metric_name.upper()}: N/A (no hot items)")
        
        # Novelty ë©”íŠ¸ë¦­
        print("\n[Novelty Metrics]")
        for metric_name in ['novelty@5', 'novelty@10', 'novelty@20', 'novelty_hit@5', 'novelty_hit@10', 'novelty_hit@20']:
            if metric_name in metrics and len(metrics[metric_name]) > 0:
                mean_val = np.mean(metrics[metric_name])
                results[metric_name] = float(mean_val)
                print(f"  {metric_name.upper()}: {mean_val:.4f}")
        
        print(f"{'='*80}\n")
        
        return results
    
    def _save_log_file(self, results, metrics, sample_outputs, split):
        """í‰ê°€ ê²°ê³¼ë¥¼ ë¡œê·¸ íŒŒì¼ë¡œ ì €ì¥"""
        results_dir = Path("results")
        results_dir.mkdir(exist_ok=True)
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_filename = f"{self.args.run_name}_{split}_eval.log"
        log_file = results_dir / log_filename
        
        with open(log_file, 'a') as f:
            # í—¤ë”
            f.write("="*80 + "\n")
            f.write(f"Evaluation Results - {split.upper()}\n")
            f.write(f"Run Name: {self.args.run_name}\n")
            f.write(f"Dataset: {self.args.data_name}\n")
            f.write(f"Model: {self.args.model_name}\n")
            f.write(f"Timestamp: {timestamp}\n")
            f.write("="*80 + "\n\n")
            
            # ë©”íŠ¸ë¦­ ê²°ê³¼
            f.write("EVALUATION METRICS\n")
            f.write("-"*80 + "\n")
            
            # Overall Metrics
            f.write("\n[Overall Metrics]\n")
            for metric_name in ['hit@5', 'hit@10', 'hit@20', 'ndcg@5', 'ndcg@10', 'ndcg@20']:
                if metric_name in results:
                    f.write(f"  {metric_name.upper()}: {results[metric_name]:.4f}\n")
            
            # Cold Metrics
            f.write("\n[Cold Items Metrics (80%)]\n")
            for metric_name in ['cold_hit@5', 'cold_hit@10', 'cold_hit@20', 'cold_ndcg@5', 'cold_ndcg@10', 'cold_ndcg@20']:
                if metric_name in results:
                    f.write(f"  {metric_name.upper()}: {results[metric_name]:.4f}\n")
            
            # Hot Metrics
            f.write("\n[Hot Items Metrics (Top 20%)]\n")
            for metric_name in ['hot_hit@5', 'hot_hit@10', 'hot_hit@20', 'hot_ndcg@5', 'hot_ndcg@10', 'hot_ndcg@20']:
                if metric_name in results:
                    f.write(f"  {metric_name.upper()}: {results[metric_name]:.4f}\n")
            
            # Novelty Metrics
            f.write("\n[Novelty Metrics]\n")
            for metric_name in ['novelty@5', 'novelty@10', 'novelty@20', 'novelty_hit@5', 'novelty_hit@10', 'novelty_hit@20']:
                if metric_name in results:
                    f.write(f"  {metric_name.upper()}: {results[metric_name]:.4f}\n")
            
            f.write("-"*80 + "\n\n")
            
            # ìƒ˜í”Œ ì¶œë ¥
            f.write("SAMPLE PROMPTS AND GENERATED TEXTS\n")
            f.write("="*80 + "\n")
            for idx, sample in enumerate(sample_outputs, 1):
                f.write(f"\n[Sample {idx}]\n")
                f.write("-"*80 + "\n")
                f.write(f"Target Item ID: {sample['target']}\n")
                f.write(f"History Items: {sample['history']}\n")
                f.write(f"\n[Prompt]\n")
                f.write(sample['prompt'] + "\n")
                f.write(f"\n[Generated Text]\n")
                f.write(sample['generated'] + "\n")
                f.write("-"*80 + "\n")
        
        print(f"ğŸ’¾ Evaluation results saved to: {log_file}")
    
    def _save_master_log(self, results, split):
        """
        ë‹¨ì¼ ë§ˆìŠ¤í„° ë¡œê·¸ íŒŒì¼ì— ëª¨ë“  ì„±ëŠ¥ ì§€í‘œ ê¸°ë¡
        
        Args:
            results: ë©”íŠ¸ë¦­ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
            split: ë°ì´í„°ì…‹ split ì´ë¦„
        """
        results_dir = Path("results")
        results_dir.mkdir(exist_ok=True)
        
        master_log_file = results_dir / f"0_{self.args.data_name}_master.log"
        
        with open(master_log_file, 'a') as f:
            f.write("="*120 + "\n")
            f.write(f"Master Evaluation Log - {self.args.run_name}\n")
            f.write(f"Timestamp: {datetime.now().strftime('%Y%m%d_%H%M%S')}\n")
            f.write(f"Dataset: {self.args.data_name}\n")
            f.write(f"Model: {self.args.model_name}\n")
            f.write("="*120 + "\n\n")
            
            # ì»¬ëŸ¼ í—¤ë” (Overall)
            f.write("[Overall Metrics]\n")
            f.write("Split\tH@5\tN@5\tH@10\tN@10\tH@20\tN@20\n")
            f.write("-"*70 + "\n")
                        
            f.write(f"{split}\t"
                   f"{results.get('hit@5', 0.0):.4f}\t"
                   f"{results.get('ndcg@5', 0.0):.4f}\t"
                   f"{results.get('hit@10', 0.0):.4f}\t"
                   f"{results.get('ndcg@10', 0.0):.4f}\t"
                   f"{results.get('hit@20', 0.0):.4f}\t"
                   f"{results.get('ndcg@20', 0.0):.4f}\n")
            
            # Cold Metrics (80%)
            f.write("\n[Cold Items Metrics (80%)]\n")
            f.write("Split\tH@5\tN@5\tH@10\tN@10\tH@20\tN@20\n")
            f.write("-"*70 + "\n")
        
            f.write(f"{split}\t"
                   f"{results.get('cold_hit@5', 0.0):.4f}\t"
                   f"{results.get('cold_ndcg@5', 0.0):.4f}\t"
                   f"{results.get('cold_hit@10', 0.0):.4f}\t"
                   f"{results.get('cold_ndcg@10', 0.0):.4f}\t"
                   f"{results.get('cold_hit@20', 0.0):.4f}\t"
                   f"{results.get('cold_ndcg@20', 0.0):.4f}\n")
            
            # Hot Metrics (Top 20%)
            f.write("\n[Hot Items Metrics (Top 20%)]\n")
            f.write("Split\tH@5\tN@5\tH@10\tN@10\tH@20\tN@20\n")
            f.write("-"*70 + "\n")
            
            f.write(f"{split}\t"
                   f"{results.get('hot_hit@5', 0.0):.4f}\t"
                   f"{results.get('hot_ndcg@5', 0.0):.4f}\t"
                   f"{results.get('hot_hit@10', 0.0):.4f}\t"
                   f"{results.get('hot_ndcg@10', 0.0):.4f}\t"
                   f"{results.get('hot_hit@20', 0.0):.4f}\t"
                   f"{results.get('hot_ndcg@20', 0.0):.4f}\n")
            
            # Novelty Metrics
            f.write("\n[Novelty Metrics]\n")
            f.write("Split\tNov@5\tNH@5\tNov@10\tNH@10\tNov@20\tNH@20\n")
            f.write("-"*70 + "\n")
            
            f.write(f"{split}\t"
                   f"{results.get('novelty@5', 0.0):.4f}\t"
                   f"{results.get('novelty_hit@5', 0.0):.4f}\t"
                   f"{results.get('novelty@10', 0.0):.4f}\t"
                   f"{results.get('novelty_hit@10', 0.0):.4f}\t"
                   f"{results.get('novelty@20', 0.0):.4f}\t"
                   f"{results.get('novelty_hit@20', 0.0):.4f}\n")

            f.write("-"*120 + "\n\n")
        
        print(f"ğŸ“Š Master log updated: {master_log_file}")
    
    def _save_csv_file(self, prompts, generated_texts, targets, histories, user_ids, rank_info, split):
        """í‰ê°€ ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥"""
        results_dir = Path("results")
        results_dir.mkdir(exist_ok=True)
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        csv_filename = f"{self.args.run_name}_{split}_eval_{timestamp}.csv"
        csv_file = results_dir / csv_filename
        
        # DataFrame ìƒì„±
        data = []
        for i in range(len(prompts)):
            rank = rank_info[i]['rank']
            is_cold = rank_info[i]['is_cold']
            target_novelty = rank_info[i]['target_novelty']
            
            data.append({
                'sample_id': i,
                'user_id': user_ids[i],
                'target_item_id': targets[i],
                'prompt': prompts[i],
                'generated_text': generated_texts[i],
                'history_items': str(histories[i]),  # ë¦¬ìŠ¤íŠ¸ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
                'target_score': rank_info[i]['target_score'],
                'rank': rank,
                'is_cold': is_cold,
                'target_novelty': target_novelty,
                'hit@5': 1 if rank <= 5 else 0,
                'hit@10': 1 if rank <= 10 else 0,
                'hit@20': 1 if rank <= 20 else 0,
                'novelty_hit@5': target_novelty if rank <= 5 else 0,
                'novelty_hit@10': target_novelty if rank <= 10 else 0,
                'novelty_hit@20': target_novelty if rank <= 20 else 0,
            })
        
        df = pd.DataFrame(data)
        
        # CSV ì €ì¥
        df.to_csv(csv_file, index=False, encoding='utf-8-sig')
        
        print(f"ğŸ’¾ Detailed results saved to CSV: {csv_file}")
        
        # ê°„ë‹¨í•œ í†µê³„ ì¶œë ¥
        print(f"\nğŸ“ˆ CSV Statistics:")
        print(f"  Total samples: {len(df)}")
        print(f"  Cold items: {df['is_cold'].sum()}, Warm items: {(~df['is_cold']).sum()}")
        print(f"  Mean rank: {df['rank'].mean():.2f}")
        print(f"  Median rank: {df['rank'].median():.0f}")
        print(f"  Hit@5 rate: {df['hit@5'].mean():.4f}")
        print(f"  Hit@10 rate: {df['hit@10'].mean():.4f}")
        print(f"  Hit@20 rate: {df['hit@20'].mean():.4f}")
        print(f"  Mean target novelty: {df['target_novelty'].mean():.4f}")
    
    def cleanup(self):
        """ë©”ëª¨ë¦¬ ì •ë¦¬"""
        print("\nğŸ§¹ Cleaning up evaluator resources...")
        
        # vLLM ìƒì„± ëª¨ë¸ ì •ë¦¬
        if self.llm is not None:
            try:
                # vLLMì˜ ê²½ìš° llm_engineì„ ëª…ì‹œì ìœ¼ë¡œ ì •ë¦¬
                if hasattr(self.llm, 'llm_engine'):
                    del self.llm.llm_engine
            except:
                pass
            del self.llm
            self.llm = None
            print("  âœ“ Generation model cleaned up")
        
        # ì„ë² ë”© ëª¨ë¸ ì •ë¦¬ (vLLM ë˜ëŠ” SentenceTransformer)
        if self.emb_llm is not None:
            if not self.use_sentence_transformers:
                # vLLMì˜ ê²½ìš°
                try:
                    if hasattr(self.emb_llm, 'llm_engine'):
                        del self.emb_llm.llm_engine
                except:
                    pass
            del self.emb_llm
            self.emb_llm = None
            print("  âœ“ Embedding model cleaned up")
        
        # ì•„ì´í…œ ì„ë² ë”© ì •ë¦¬
        if self.item_embeddings is not None:
            self.item_embeddings = self.item_embeddings.cpu()
            del self.item_embeddings
            self.item_embeddings = None
            print("  âœ“ Item embeddings cleaned up")
        
        # ì•„ì´í…œ ì¸ê¸°ë„ ë° Novelty ì •ë¦¬
        if self.item_popularity is not None:
            del self.item_popularity
            self.item_popularity = None
        
        if self.item_novelty is not None:
            del self.item_novelty
            self.item_novelty = None
        
        if self.hot_items is not None:
            del self.hot_items
            self.hot_items = None
        
        if self.cold_items is not None:
            del self.cold_items
            self.cold_items = None
        
        if any([self.item_popularity is None, self.item_novelty is None, 
                self.hot_items is None, self.cold_items is None]):
            print("  âœ“ Item popularity, novelty, and hot/cold sets cleaned up")
        
        # GPU ë©”ëª¨ë¦¬ ê°•ì œ í•´ì œ
        torch.cuda.empty_cache()
        gc.collect()
        
        if torch.cuda.is_available():
            torch.cuda.synchronize()
            print(f"  ğŸ’¾ GPU Memory after evaluator cleanup: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")
        
        print("âœ“ Evaluator cleanup complete")

