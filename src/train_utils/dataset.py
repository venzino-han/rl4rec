"""
Dataset utilities for RL4Rec
ì¶”ì²œ ì‹œìŠ¤í…œ í•™ìŠµì„ ìœ„í•œ ë°ì´í„°ì…‹ ë° ë°ì´í„°ë¡œë” ìœ í‹¸ë¦¬í‹°
"""

import os
import json
import pickle
import numpy as np
import argparse
from pathlib import Path
from typing import List, Dict, Optional, Tuple

import torch
from torch.utils.data import Dataset, DataLoader


class PromptGenerator:
    """
    ì‚¬ìš©ì ì‹œí€€ìŠ¤ë¡œë¶€í„° í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” í´ë˜ìŠ¤
    """
    
    def __init__(
        self,
        item_metadata: Dict,
        use_brand: bool = True,
        use_category: bool = True,
        use_description: bool = False,
        use_features: bool = False,
        use_last_item: bool = True,
        max_history_len: int = 5,
        history_text_max_length: int = 100,
    ):
        """
        Args:
            item_metadata: ì•„ì´í…œ ë©”íƒ€ë°ì´í„° ë”•ì…”ë„ˆë¦¬
            use_brand: ë¸Œëœë“œ ì •ë³´ í¬í•¨ ì—¬ë¶€
            use_category: ì¹´í…Œê³ ë¦¬ ì •ë³´ í¬í•¨ ì—¬ë¶€
            use_description: ì„¤ëª… ì •ë³´ í¬í•¨ ì—¬ë¶€
            use_features: íŠ¹ì§• ì •ë³´ í¬í•¨ ì—¬ë¶€
            use_last_item: ë§ˆì§€ë§‰ ì•„ì´í…œ ê°•ì¡° ì—¬ë¶€
            max_history_len: ìµœëŒ€ íˆìŠ¤í† ë¦¬ ê¸¸ì´
            history_text_max_length: íˆìŠ¤í† ë¦¬ í…ìŠ¤íŠ¸ ìµœëŒ€ ë‹¨ì–´ ìˆ˜
        """
        self.item_metadata = item_metadata
        self.use_brand = use_brand
        self.use_category = use_category
        self.use_description = use_description
        self.use_features = use_features
        self.use_last_item = use_last_item
        self.max_history_len = max_history_len
        self.history_text_max_length = history_text_max_length
    
    def generate_prompt(self, item_ids: List[int]) -> str:
        """
        ì‚¬ìš©ì ì‹œí€€ìŠ¤ë¡œë¶€í„° í”„ë¡¬í”„íŠ¸ ìƒì„±
        
        Args:
            item_ids: ì•„ì´í…œ ID ë¦¬ìŠ¤íŠ¸
        
        Returns:
            ìƒì„±ëœ í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´
        """
        # íˆìŠ¤í† ë¦¬ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        history_text_list = []
        
        # ê° ì•„ì´í…œ ì²˜ë¦¬
        for item_id in item_ids:
            item_data = self.item_metadata.get(item_id)
            if item_data is None:
                # ë©”íƒ€ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° ìŠ¤í‚µ
                continue
            
            item_title = item_data.get('title', 'Unknown Item')
            item_brand = item_data.get('brand', 'Unknown Brand')
            item_categories = item_data.get('category', 'Unknown Category')
            item_description = item_data.get('description', '')
            item_features = item_data.get('features', '')
            
            # ê¸°ë³¸ íˆìŠ¤í† ë¦¬ í¬ë§·
            item_history_text = f"**Title:** `{item_title}`"
            
            if self.use_brand:
                item_history_text += f"\n**Brand:** {item_brand}"
            
            if self.use_category:
                item_history_text += f"\n**Categories:** {item_categories}"
            
            if self.use_description and item_description:
                item_description = item_description.replace("\n", " ")
                if len(item_description.split()) > self.history_text_max_length:
                    item_description = " ".join(
                        item_description.split()[:self.history_text_max_length]
                    ) + "..."
                item_history_text += f"\n**Description:** {item_description}"
            
            if self.use_features and item_features:
                if len(item_features.split("\n")) > 10:
                    item_features = "\n".join(item_features.split("\n")[:10])
                item_features = item_features.replace("\n-", ",").replace("- ", "")
                item_history_text += f"\n**Features:**\n{item_features}"
            
            history_text_list.append(item_history_text)
        
        # íˆìŠ¤í† ë¦¬ ê¸¸ì´ ì œì•½ ì ìš©
        if len(history_text_list) > self.max_history_len:
            history_text_list = history_text_list[-self.max_history_len:]
        
        # ìµœì¢… íˆìŠ¤í† ë¦¬ í…ìŠ¤íŠ¸ êµ¬ì„±
        history_text = "\n---\n".join(
            f"{i+1}. {history}" for i, history in enumerate(history_text_list)
        )
        
        # ë§ˆì§€ë§‰ ì•„ì´í…œ ê°•ì¡°
        if self.use_last_item and len(item_ids) > 0:
            last_item = self.item_metadata.get(item_ids[-1], {})
            last_item_title = last_item.get('title', 'Unknown Item')
            history_text += f"\n\n`{last_item_title}` is the most recently purchased item."
        
        # ìµœì¢… í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = (
            f"# User Purchase History\n\n"
            f"{history_text}\n\n"
            f"# Task\n"
            f"Based on this user's purchase history, describe user's preference:\n"
        )
        
        return prompt


class RecommendationDataset(Dataset):
    """
    ì¶”ì²œ ì‹œìŠ¤í…œìš© ë°ì´í„°ì…‹
    ì‚¬ìš©ì íˆìŠ¤í† ë¦¬, íƒ€ê²Ÿ ì•„ì´í…œ, í”„ë¡¬í”„íŠ¸ë¥¼ í¬í•¨
    """
    
    def __init__(
        self,
        data_name: str,
        item_metadata: Dict,
        prompt_generator: PromptGenerator,
        split: str = "train",
    ):
        """
        Args:
            sequential_file: ì‹œí€€ì…œ ë°ì´í„° íŒŒì¼ ê²½ë¡œ (user_id history target)
            item_metadata: ì•„ì´í…œ ë©”íƒ€ë°ì´í„° ë”•ì…”ë„ˆë¦¬
            prompt_generator: í”„ë¡¬í”„íŠ¸ ìƒì„±ê¸°
            split: ë°ì´í„° ë¶„í•  ("train", "valid", "test")
        """
        self.item_metadata = item_metadata
        self.prompt_generator = prompt_generator
        self.split = split
        sequential_file = f"data/{data_name}/sequential_data.txt"
        self._load_real_data(sequential_file, split)
        
        print(f"âœ“ {split.upper()} Dataset loaded: {len(self.user_ids)} users")
    
    def _load_real_data(
        self,
        sequential_file: str,
        split: str,
    ):
        """ì‹¤ì œ ë°ì´í„° ë¡œë“œ"""
        all_user_ids = []
        all_history = {}
        all_targets = {}
        
        if split == "train":
            target_index = -3
        elif split == "valid":
            target_index = -2
        elif split == "test":
            target_index = -1
        else:
            raise ValueError(f"Invalid split: {split}")
        
        with open(sequential_file, "r") as f:
            for line in f:
                parts = [int(p) for p in line.strip().split()]
                user_id = parts[0]
                history = parts[1:target_index]
                target = parts[target_index]
                
                all_user_ids.append(user_id)
                all_history[user_id] = history
                all_targets[user_id] = target
        
        self.user_ids = all_user_ids
        self.history_dict = all_history
        self.target_dict = all_targets
    
    def __len__(self):
        return len(self.user_ids)
    
    def __getitem__(self, idx):
        user_id = self.user_ids[idx]
        history = self.history_dict[user_id]
        target = self.target_dict[user_id]
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = self.prompt_generator.generate_prompt(history)
        
        return {
            "prompt": prompt,
            "history": history,
            "target": target,
            "user_id": user_id,
        }


def collate_fn(batch):
    """
    DataLoaderìš© collate function
    """
    return {
        "queries": [item["query"] for item in batch],
        "histories": [item["history"] for item in batch],
        "targets": [item["target"] for item in batch],
        "user_ids": [item["user_id"] for item in batch],
    }


def load_item_metadata(dataset_name: str, data_dir: str = "data") -> Dict:
    """
    ì•„ì´í…œ ë©”íƒ€ë°ì´í„° ë¡œë“œ
    
    Args:
        dataset_name: ë°ì´í„°ì…‹ ì´ë¦„ (e.g., "beauty")
        data_dir: ë°ì´í„° ë””ë ‰í† ë¦¬
    
    Returns:
        ì•„ì´í…œ ë©”íƒ€ë°ì´í„° ë”•ì…”ë„ˆë¦¬
    """
    # ë©”íƒ€ë°ì´í„° íŒŒì¼ ê²½ë¡œ ì‹œë„
    possible_paths = [
        f"{data_dir}/{dataset_name}/meta_text.json",
    ]
    
    item_metadata = {}
    
    for path in possible_paths:
        if os.path.exists(path):
            print(f"Loading item metadata from: {path}")
            
            if path.endswith('.json'):
                with open(path, 'r') as f:
                    data = json.load(f)
                    # Keyë¥¼ intë¡œ ë³€í™˜
                    item_metadata = {int(k): v for k, v in data.items()}
            elif path.endswith('.pkl'):
                with open(path, 'rb') as f:
                    item_metadata = pickle.load(f)
            
            print(f"âœ“ Loaded {len(item_metadata)} items")
            return item_metadata
    
    # ë©”íƒ€ë°ì´í„° íŒŒì¼ì„ ì°¾ì§€ ëª»í•œ ê²½ìš° ê²½ê³ 
    print(f"âš ï¸  Item metadata file not found. Using dummy metadata.")
    return {}


def create_dataloaders(
    args: argparse.Namespace,
) -> Tuple[DataLoader, DataLoader, DataLoader, PromptGenerator, Dict]:
    """
    Train/Valid/Test DataLoader ìƒì„±
    
    Args:
        args: argparse.Namespace
    
    Returns:
        (train_dataloader, valid_dataloader, test_dataloader, prompt_generator, item_metadata)
    """
    # ì•„ì´í…œ ë©”íƒ€ë°ì´í„° ë¡œë“œ
    print(f"ğŸ“¦ Loading item metadata...")
    item_metadata = load_item_metadata(args.dataset_name)
    
    # í”„ë¡¬í”„íŠ¸ ìƒì„±ê¸°
    print(f"âœï¸  Creating prompt generator...")
    prompt_generator = PromptGenerator(
        item_metadata=item_metadata,
        use_brand=args.use_brand,
        use_category=args.use_category,
        use_description=args.use_description,
        max_history_len=args.max_history_len,
        history_text_max_length=args.history_text_max_length,
    )
    
    # ë°ì´í„°ì…‹ ìƒì„±
    print(f"ğŸ“Š Creating datasets...")
    
    # Train dataset
    train_dataset = RecommendationDataset(
        data_name=args.dataset_name,
        item_metadata=item_metadata,
        prompt_generator=prompt_generator,
        split="train",
    )
    
    # Valid dataset
    valid_dataset = RecommendationDataset(
        data_name=args.dataset_name,
        item_metadata=item_metadata,
        prompt_generator=prompt_generator,
        split="valid",
    )
    
    # Test dataset
    test_dataset = RecommendationDataset(
        data_name=args.dataset_name,
        item_metadata=item_metadata,
        prompt_generator=prompt_generator,
        split="test",
    )
    
    # DataLoaders
    print(f"ğŸ”„ Creating dataloaders...")
    
    train_dataloader = DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        shuffle=True,
        collate_fn=collate_fn,
    )
    
    valid_dataloader = DataLoader(
        valid_dataset,
        batch_size=args.eval_batch_size,
        shuffle=False,
        collate_fn=collate_fn,
    )
    
    test_dataloader = DataLoader(
        test_dataset,
        batch_size=args.eval_batch_size,
        shuffle=False,
        collate_fn=collate_fn,
    )
    
    print(f"âœ“ DataLoaders created:")
    print(f"  Train samples: {len(train_dataset)}")
    print(f"  Valid samples: {len(valid_dataset)}")
    print(f"  Test samples: {len(test_dataset)}")
    
    return train_dataset, valid_dataset, test_dataset, prompt_generator, item_metadata
    # return train_dataloader, valid_dataloader, test_dataloader, prompt_generator, item_metadata

