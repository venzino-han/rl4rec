# RL4Rec Training Guide

ê°•í™”í•™ìŠµ ê¸°ë°˜ ì¶”ì²œ ì‹œìŠ¤í…œ í•™ìŠµ ê°€ì´ë“œ

## ğŸ“‹ ëª©ì°¨

1. [ì‹œìŠ¤í…œ êµ¬ì¡°](#ì‹œìŠ¤í…œ-êµ¬ì¡°)
2. [ì„¤ì¹˜ ë° ì„¤ì •](#ì„¤ì¹˜-ë°-ì„¤ì •)
3. [ì‚¬ìš© ë°©ë²•](#ì‚¬ìš©-ë°©ë²•)
4. [íŒŒì¼ ì„¤ëª…](#íŒŒì¼-ì„¤ëª…)
5. [ì»¤ìŠ¤í„°ë§ˆì´ì§•](#ì»¤ìŠ¤í„°ë§ˆì´ì§•)

---

## ğŸ—ï¸ ì‹œìŠ¤í…œ êµ¬ì¡°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  RL Trainer     â”‚         â”‚ Retrieval Serviceâ”‚
â”‚  (GPU 0)        â”‚â—„â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  (GPU 1)         â”‚
â”‚                 â”‚  Ray    â”‚                  â”‚
â”‚ - Policy Model  â”‚         â”‚ - vLLM Embedding â”‚
â”‚ - Generate Text â”‚         â”‚ - Item Embeddingsâ”‚
â”‚ - Compute Loss  â”‚         â”‚ - Score Compute  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ì£¼ìš” ì»´í¬ë„ŒíŠ¸

1. **RetrievalService** (`src/retrieval_service.py`)
   - vLLMì„ ì‚¬ìš©í•œ í…ìŠ¤íŠ¸ ì„ë² ë”©
   - ì‚¬ì „ ê³„ì‚°ëœ ì•„ì´í…œ ì„ë² ë”©ê³¼ì˜ ìœ ì‚¬ë„ ê³„ì‚°
   - Ray Actorë¡œ ì‹¤í–‰ë˜ì–´ ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ì™€ í†µì‹ 

2. **RecPolicy** (`src/rec_model.py`)
   - ì‚¬ìš©ì íˆìŠ¤í† ë¦¬ ê¸°ë°˜ í…ìŠ¤íŠ¸ ìƒì„± Policy Model
   - LLM(GPT-2 ë“±) ê¸°ë°˜ ìƒì„± ëª¨ë¸
   - Policy Gradientë¡œ í•™ìŠµ

3. **RLTrainer** (`src/rl_train.py`)
   - ì „ì²´ í•™ìŠµ ë£¨í”„ ê´€ë¦¬
   - Policy Modelê³¼ RetrievalService ì—°ë™
   - ì²´í¬í¬ì¸íŠ¸ ë° ë¡œê¹… ê´€ë¦¬

---

## ğŸ”§ ì„¤ì¹˜ ë° ì„¤ì •

### 1. í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜

```bash
pip install torch transformers vllm ray
```

### 2. ë°ì´í„° ì¤€ë¹„

ì•„ì´í…œ ì„ë² ë”© íŒŒì¼ì„ `data_emb/` ë””ë ‰í† ë¦¬ì— ì¤€ë¹„:

```
data_emb/
â”œâ”€â”€ beauty_review_description_mxbai-embed-large-v1.pt
â”œâ”€â”€ sports_review_description_mxbai-embed-large-v1.pt
â”œâ”€â”€ toys_review_description_mxbai-embed-large-v1.pt
â””â”€â”€ yelp_review_description_mxbai-embed-large-v1.pt
```

ê° íŒŒì¼ì€ `torch.Tensor` í˜•íƒœë¡œ ì €ì¥ëœ ì•„ì´í…œ ì„ë² ë”©:
- Shape: `[num_items, embedding_dim]`
- dtype: `torch.float32`

### 3. Ray í´ëŸ¬ìŠ¤í„° ì´ˆê¸°í™” (ì„ íƒì‚¬í•­)

ë‹¨ì¼ ë¨¸ì‹ :
```bash
ray start --head --num-gpus=2
```

ë©€í‹° ë…¸ë“œ:
```bash
# Head ë…¸ë“œ
ray start --head --port=6379

# Worker ë…¸ë“œ
ray start --address='HEAD_IP:6379'
```

---

## ğŸš€ ì‚¬ìš© ë°©ë²•

### Step 1: RetrievalService ì‹œì‘

í„°ë¯¸ë„ 1ì—ì„œ ì‹¤í–‰:

```bash
# GPU 0ì—ì„œ ì‹¤í–‰
./runs/run_retrieval.sh

# ë˜ëŠ” GPU 1ì—ì„œ ì‹¤í–‰
CUDA_VISIBLE_DEVICES=1 python3 src/retrieval_service.py \
    --emb_model_name "mixedbread-ai/mxbai-embed-large-v1" \
    --emb_type "review_description" \
    --datasets beauty \
    --actor_name "RetrievalService" \
    --namespace "rl4rec" \
    --num_gpus 1.0
```

ì¶œë ¥ ì˜ˆì‹œ:
```
âœ“ Loaded 1 dataset(s) on cuda
  Loaded dataset 'beauty': torch.Size([20000, 1024])
âœ“ DB Server is up and running. Waiting for learners...
```

### Step 2: RL Training ì‹œì‘

í„°ë¯¸ë„ 2ì—ì„œ ì‹¤í–‰:

```bash
# ê¸°ë³¸ í•™ìŠµ
./runs/run_rl_train.sh

# ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (Dummy ëª¨ë“œ)
./runs/run_rl_train_dummy.sh

# Yelp ë°ì´í„°ì…‹
./runs/run_rl_train_yelp.sh
```

ì¶œë ¥ ì˜ˆì‹œ:
```
================================================================================
ğŸš€ Starting RL Training
================================================================================
Step      0 | Loss: 0.5234 | Reward: 0.6543 (max: 0.8912, min: 0.4321)
Step     10 | Loss: 0.4876 | Reward: 0.6721 (max: 0.9034, min: 0.4567)
Step     20 | Loss: 0.4532 | Reward: 0.6892 (max: 0.9123, min: 0.4789)
...
âœ“ Checkpoint saved: checkpoints/beauty_rl/checkpoint_step_100.pt
```

### Step 3: ì²´í¬í¬ì¸íŠ¸ í™•ì¸

```bash
ls -lh checkpoints/beauty_rl/
```

ì¶œë ¥:
```
checkpoint_step_100.pt
checkpoint_step_200.pt
checkpoint_final.pt
training_log.jsonl
```

---

## ğŸ“ íŒŒì¼ ì„¤ëª…

### ì½”ì–´ íŒŒì¼

#### `src/rec_model.py`

Policy Model ì •ì˜:

- **RecPolicy**: LLM ê¸°ë°˜ Policy Model
  - `build_prompts()`: ì‚¬ìš©ì íˆìŠ¤í† ë¦¬ â†’ í”„ë¡¬í”„íŠ¸ ë³€í™˜
  - `generate()`: í”„ë¡¬í”„íŠ¸ â†’ ì¶”ì²œ í…ìŠ¤íŠ¸ ìƒì„±
  - `compute_loss()`: Policy Gradient Loss ê³„ì‚°
  - `update()`: ëª¨ë¸ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸

- **DummyRecPolicy**: í…ŒìŠ¤íŠ¸ìš© ê°„ë‹¨í•œ ëª¨ë¸

ì‚¬ìš© ì˜ˆì‹œ:
```python
from rec_model import RecPolicy

policy = RecPolicy(model_name="gpt2", device="cuda")

# í”„ë¡¬í”„íŠ¸ ìƒì„±
user_histories = [["Product A", "Product B"], ["Product C"]]
prompts = policy.build_prompts(user_histories)

# í…ìŠ¤íŠ¸ ìƒì„±
texts = policy.generate(prompts)

# ì—…ë°ì´íŠ¸
rewards = torch.tensor([0.8, 0.6])
stats = policy.update(prompts, texts, rewards)
```

#### `src/rl_train.py`

í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸:

- **RLTrainer**: ì „ì²´ í•™ìŠµ í”„ë¡œì„¸ìŠ¤ ê´€ë¦¬
  - `train_step()`: í•œ ìŠ¤í… í•™ìŠµ
  - `compute_rewards()`: RetrievalService í†µí•´ ë³´ìƒ ê³„ì‚°
  - `train()`: ì „ì²´ í•™ìŠµ ë£¨í”„

ì§ì ‘ ì‹¤í–‰:
```bash
python3 src/rl_train.py \
    --dataset_name beauty \
    --num_steps 1000 \
    --batch_size 16 \
    --learning_rate 1e-5
```

#### `src/retrieval_service.py`

ì„ë² ë”© ë° ìŠ¤ì½”ì–´ ê³„ì‚° ì„œë¹„ìŠ¤:

- **RetrievalService**: Ray Actor
  - `calculate_reward(texts, dataset_name)`: ì „ì²´ ìŠ¤ì½”ì–´ ë°°ì—´ ë°˜í™˜
  - Shape: `[len(texts), num_items]`

### ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

| ìŠ¤í¬ë¦½íŠ¸ | ì„¤ëª… |
|---------|------|
| `runs/run_retrieval.sh` | RetrievalService ì‹œì‘ (Beauty ë°ì´í„°ì…‹) |
| `runs/run_rl_train.sh` | RL í•™ìŠµ ì‹œì‘ (Beauty, 1000 steps) |
| `runs/run_rl_train_dummy.sh` | ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš© Dummy ëª¨ë“œ |
| `runs/run_rl_train_yelp.sh` | Yelp ë°ì´í„°ì…‹ í•™ìŠµ |

---

## ğŸ¨ ì»¤ìŠ¤í„°ë§ˆì´ì§•

### 1. ìƒˆë¡œìš´ ë°ì´í„°ì…‹ ì¶”ê°€

```bash
# 1. ì•„ì´í…œ ì„ë² ë”© ìƒì„± ë° ì €ì¥
python3 scripts/generate_embeddings.py \
    --dataset toys \
    --emb_model "mixedbread-ai/mxbai-embed-large-v1"

# 2. RetrievalService ì‹œì‘
python3 src/retrieval_service.py \
    --datasets toys \
    --emb_type item

# 3. í•™ìŠµ ì‹œì‘
python3 src/rl_train.py --dataset_name toys
```

### 2. ë‹¤ë¥¸ Policy Model ì‚¬ìš©

```python
# src/rl_train.py ì‹¤í–‰ ì‹œ
python3 src/rl_train.py \
    --policy_model "meta-llama/Llama-2-7b-hf" \
    --learning_rate 1e-6
```

### 3. ë³´ìƒ í•¨ìˆ˜ ë³€ê²½

`src/rl_train.py`ì˜ `compute_rewards()` ë©”ì„œë“œ ìˆ˜ì •:

```python
def compute_rewards(self, generated_texts, dataset_name):
    scores = ray.get(
        self.retrieval_service.calculate_reward.remote(
            generated_texts, dataset_name
        )
    )
    
    # ì»¤ìŠ¤í…€ ë³´ìƒ ê³„ì‚°
    # ì˜ˆ: Top-5 í‰ê·  + Diversity penalty
    top_5 = torch.topk(scores, k=5, dim=1).values
    avg_score = top_5.mean(dim=1)
    diversity = top_5.std(dim=1)
    rewards = avg_score + 0.1 * diversity
    
    return rewards
```

### 4. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë³€ê²½

`src/rec_model.py`ì˜ `build_prompts()` ë©”ì„œë“œ ìˆ˜ì •:

```python
def build_prompts(self, user_histories):
    prompts = []
    for history in user_histories:
        prompt = f"""
        User Profile:
        - Recent purchases: {', '.join(history[-5:])}
        - Preferences: High-quality products
        
        Generate a compelling description for the next recommended product:
        """
        prompts.append(prompt)
    return prompts
```

---

## ğŸ“Š ë¡œê·¸ ë¶„ì„

í•™ìŠµ ë¡œê·¸ëŠ” JSONL í˜•ì‹ìœ¼ë¡œ ì €ì¥:

```python
import json
import pandas as pd

# ë¡œê·¸ ë¡œë“œ
logs = []
with open("checkpoints/beauty_rl/training_log.jsonl") as f:
    for line in f:
        logs.append(json.loads(line))

df = pd.DataFrame(logs)

# ë¶„ì„
print(df[["step", "loss", "avg_reward"]].describe())

# ì‹œê°í™”
import matplotlib.pyplot as plt
plt.plot(df["step"], df["avg_reward"])
plt.xlabel("Step")
plt.ylabel("Average Reward")
plt.show()
```

---

## ğŸ› íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### RetrievalServiceë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ

```
âŒ Error: RetrievalService not found!
```

**í•´ê²°**: í„°ë¯¸ë„ 1ì—ì„œ RetrievalServiceê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸

```bash
ray list actors --namespace rl4rec
```

### GPU ë©”ëª¨ë¦¬ ë¶€ì¡±

```
CUDA out of memory
```

**í•´ê²°**:
1. Batch size ê°ì†Œ: `--batch_size 8`
2. GPU memory utilization ì¡°ì •: `gpu_memory_utilization=0.6`
3. ëª¨ë¸ í¬ê¸° ê°ì†Œ: `--policy_model gpt2` (ëŒ€ì‹  gpt2-medium ì‚¬ìš© ì•ˆ í•¨)

### í•™ìŠµì´ ë„ˆë¬´ ëŠë¦¼

**í•´ê²°**:
1. Dummy ëª¨ë“œë¡œ í…ŒìŠ¤íŠ¸: `--use_dummy`
2. ë” ì‘ì€ ëª¨ë¸ ì‚¬ìš©
3. GPU ê°œìˆ˜ í™•ì¸: `ray status`

---

## ğŸ“š ì°¸ê³  ìë£Œ

- [Ray Documentation](https://docs.ray.io/)
- [vLLM Documentation](https://docs.vllm.ai/)
- [Transformers Documentation](https://huggingface.co/docs/transformers/)

---

## ğŸ“ TODO

- [ ] ì‹¤ì œ ì‚¬ìš©ì ë°ì´í„° ë¡œë” êµ¬í˜„
- [ ] Wandb/TensorBoard ë¡œê¹… ì¶”ê°€
- [ ] Multi-task learning ì§€ì›
- [ ] Distributed training ì§€ì›
- [ ] A/B Testing í”„ë ˆì„ì›Œí¬

---

**Happy Training! ğŸš€**

