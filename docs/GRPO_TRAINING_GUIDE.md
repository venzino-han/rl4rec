# GRPO ê¸°ë°˜ RL4Rec í•™ìŠµ ê°€ì´ë“œ

## ê°œìš”

TRL (Transformer Reinforcement Learning) ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ **GRPO (Group Relative Policy Optimization)**ë¥¼ í™œìš©í•˜ì—¬ NDCG ê¸°ë°˜ ë¦¬ì›Œë“œë¡œ ì¶”ì²œ ì‹œìŠ¤í…œì„ í•™ìŠµí•˜ëŠ” ê°€ì´ë“œì…ë‹ˆë‹¤.

## GRPOë€?

GRPO (Group Relative Policy Optimization)ëŠ” PPOì˜ ê°œì„ ëœ ë³€í˜•ìœ¼ë¡œ, ë‹¤ìŒê³¼ ê°™ì€ íŠ¹ì§•ì´ ìˆìŠµë‹ˆë‹¤:

- **Group-based Normalization**: ê°™ì€ í”„ë¡¬í”„íŠ¸ì—ì„œ ìƒì„±ëœ ì—¬ëŸ¬ ìƒ˜í”Œë“¤ì˜ ìƒëŒ€ì  í’ˆì§ˆì„ ë¹„êµ
- **Sample Efficiency**: ê° í”„ë¡¬í”„íŠ¸ë‹¹ ì—¬ëŸ¬ ìƒ˜í”Œì„ ìƒì„±í•˜ì—¬ í•™ìŠµ íš¨ìœ¨ í–¥ìƒ
- **Stable Training**: ê·¸ë£¹ ë‚´ ì •ê·œí™”ë¡œ ì•ˆì •ì ì¸ í•™ìŠµ

### PPO vs GRPO

| íŠ¹ì§• | PPO | GRPO |
|------|-----|------|
| ìƒ˜í”Œë§ | í”„ë¡¬í”„íŠ¸ë‹¹ 1ê°œ | í”„ë¡¬í”„íŠ¸ë‹¹ Nê°œ |
| ì •ê·œí™” | ë°°ì¹˜ ì „ì²´ | ê·¸ë£¹ ë‚´ë¶€ |
| í•™ìŠµ ì•ˆì •ì„± | ë³´í†µ | ë†’ìŒ |
| ìƒ˜í”Œ íš¨ìœ¨ì„± | ë³´í†µ | ë†’ìŒ |

## ì£¼ìš” íŒŒì¼

```
src/
â”œâ”€â”€ grpo_train.py           # GRPO í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ (ë©”ì¸)
â”œâ”€â”€ train_utils/
â”‚   â””â”€â”€ reward_funtion.py   # NDCG ë¦¬ì›Œë“œ í•¨ìˆ˜
â””â”€â”€ rec_model.py            # vLLM í†µí•© ëª¨ë¸

runs/
â”œâ”€â”€ run_grpo_train.sh       # ì „ì²´ í•™ìŠµ ì‹¤í–‰
â””â”€â”€ run_grpo_test.sh        # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰

docs/
â””â”€â”€ GRPO_TRAINING_GUIDE.md  # ì´ ë¬¸ì„œ
```

## ì‚¬ìš© ë°©ë²•

### 1. ì‚¬ì „ ì¤€ë¹„

#### Retrieval Service ì‹œì‘
```bash
./runs/run_retrieval.sh
```

#### Ray í´ëŸ¬ìŠ¤í„° í™•ì¸
```bash
ray status
```

### 2. ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (Dummy ë°ì´í„°)

í•™ìŠµ íŒŒì´í”„ë¼ì¸ì„ ë¹ ë¥´ê²Œ í…ŒìŠ¤íŠ¸:

```bash
./runs/run_grpo_test.sh
```

**í…ŒìŠ¤íŠ¸ ì„¤ì •:**
- Dummy ë°ì´í„° 100ê°œ
- 1 ì—í¬í¬, 50 ìŠ¤í…
- ë°°ì¹˜ í¬ê¸° 4, ìƒ˜í”Œ 2ê°œ

### 3. ì „ì²´ í•™ìŠµ ì‹¤í–‰

ì‹¤ì œ ë°ì´í„°ë¡œ í•™ìŠµ:

```bash
./runs/run_grpo_train.sh
```

**í•™ìŠµ ì„¤ì •:**
- Beauty ë°ì´í„°ì…‹
- NDCG@10 ë¦¬ì›Œë“œ
- 3 ì—í¬í¬, 10,000 ìŠ¤í…
- ë°°ì¹˜ í¬ê¸° 4, ìƒ˜í”Œ 4ê°œ

### 4. ì»¤ìŠ¤í…€ ì„¤ì •

```bash
python src/grpo_train.py \
    --policy_model "gpt2" \
    --dataset_name "beauty" \
    --reward_type "ndcg" \
    --k 10 \
    --batch_size 4 \
    --num_sample_generations 4 \
    --learning_rate 1e-5 \
    --num_epochs 3 \
    --checkpoint_dir "checkpoints/my_experiment" \
    --normalize_rewards
```

## ì£¼ìš” íŒŒë¼ë¯¸í„°

### ëª¨ë¸ ì„¤ì •

```bash
--policy_model "gpt2"              # HuggingFace ëª¨ë¸ ì´ë¦„
--device "cuda"                    # ë””ë°”ì´ìŠ¤ (cuda/cpu)
--max_length 128                   # ì…ë ¥ ìµœëŒ€ ê¸¸ì´
--max_new_tokens 64                # ìƒì„± í† í° ìˆ˜
--temperature 1.0                  # ìƒ˜í”Œë§ ì˜¨ë„
--use_ref_model                    # Reference ëª¨ë¸ ì‚¬ìš© (KL penalty)
```

### GRPO í•™ìŠµ ì„¤ì •

```bash
--batch_size 4                     # ë°°ì¹˜ í¬ê¸°
--num_sample_generations 4         # í”„ë¡¬í”„íŠ¸ë‹¹ ìƒì„± ìƒ˜í”Œ ìˆ˜ (ì¤‘ìš”!)
--gradient_accumulation_steps 4    # Gradient accumulation
--learning_rate 1e-5               # í•™ìŠµë¥ 
--max_grad_norm 1.0                # Gradient clipping
--num_epochs 3                     # ì—í¬í¬ ìˆ˜
--max_steps 10000                  # ìµœëŒ€ ìŠ¤í… ìˆ˜
```

**ì¤‘ìš”:** `num_sample_generations`ëŠ” GRPOì˜ í•µì‹¬ íŒŒë¼ë¯¸í„°ì…ë‹ˆë‹¤:
- ê°’ì´ í´ìˆ˜ë¡: ë” ë§ì€ ìƒ˜í”Œ ë¹„êµ, í•™ìŠµ ì•ˆì •ì„± ì¦ê°€, ê³„ì‚° ë¹„ìš© ì¦ê°€
- ê¶Œì¥ ê°’: 2~8

### ë¦¬ì›Œë“œ ì„¤ì •

```bash
--reward_type "ndcg"               # ë¦¬ì›Œë“œ íƒ€ì… (ndcg/hit/mrr/mixed)
--k 10                             # Top-K ê°’
--normalize_rewards                # ë¦¬ì›Œë“œ ì •ê·œí™”
```

### ë°ì´í„° ì„¤ì •

```bash
--prompt_file "path/to/prompts.json"
--sequential_file "path/to/sequential.txt"
--use_dummy                        # Dummy ë°ì´í„° ì‚¬ìš© (í…ŒìŠ¤íŠ¸ìš©)
--dummy_size 1000                  # Dummy ë°ì´í„° í¬ê¸°
```

## í•™ìŠµ í”„ë¡œì„¸ìŠ¤

### 1. ë°ì´í„° ë¡œë“œ

```python
dataset = RecommendationDataset(
    prompt_file="prompts.json",
    sequential_file="sequential.txt",
    tokenizer=tokenizer,
    use_dummy=False,  # ì‹¤ì œ ë°ì´í„° ì‚¬ìš©
)
```

ë°ì´í„° í˜•ì‹:
- **prompts.json**: `{user_id: prompt_text, ...}`
- **sequential.txt**: `user_id item1 item2 ... itemN target`

### 2. GRPO í•™ìŠµ ìŠ¤í…

```
For each batch:
    1. í”„ë¡¬í”„íŠ¸ í† í¬ë‚˜ì´ì¦ˆ
    2. ê° í”„ë¡¬í”„íŠ¸ë‹¹ Nê°œ ìƒ˜í”Œ ìƒì„± (num_sample_generations)
    3. ê° ìƒ˜í”Œì˜ NDCG ë¦¬ì›Œë“œ ê³„ì‚°
    4. ê·¸ë£¹ ë‚´ ìƒëŒ€ì  í’ˆì§ˆ ë¹„êµ
    5. GRPO ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ëª¨ë¸ ì—…ë°ì´íŠ¸
```

### 3. ë¦¬ì›Œë“œ ê³„ì‚°

```python
# ê° ìƒ˜í”Œì˜ ë¦¬ì›Œë“œ ê³„ì‚°
for each generation:
    # RetrievalServiceë¥¼ í†µí•´ ìœ ì‚¬ë„ ì ìˆ˜ ê³„ì‚°
    scores = retrieval_service.calculate(generation)
    
    # NDCG ê³„ì‚°
    ndcg = calculate_ndcg(
        scores=scores,
        target=target_item,
        history=history_items,
        k=10
    )
    
    rewards.append(ndcg)
```

## ì•„í‚¤í…ì²˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      GRPO Training Loop                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  1. Load Batch (Prompts)  â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  2. Generate N samples per prompt     â”‚
              â”‚     (num_sample_generations=4)        â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  3. Calculate Rewards (NDCG)          â”‚
              â”‚     - Ray RetrievalService            â”‚
              â”‚     - Similarity scores               â”‚
              â”‚     - NDCG@K calculation              â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  4. Group Normalization               â”‚
              â”‚     - Compare samples in same group   â”‚
              â”‚     - Relative quality scoring        â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  5. GRPO Update                       â”‚
              â”‚     - Policy gradient                 â”‚
              â”‚     - KL penalty (optional)           â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## í‰ê°€ ë©”íŠ¸ë¦­

### í•™ìŠµ ì¤‘ ë©”íŠ¸ë¦­

```
Step    100 | Epoch 1 Batch   10 | Reward: 0.3245 Â± 0.1234 | Loss: 2.3456
```

- **Reward**: í‰ê·  NDCG@K
- **Â± (std)**: ë¦¬ì›Œë“œ í‘œì¤€í¸ì°¨ (ë‚®ì„ìˆ˜ë¡ ì•ˆì •ì )
- **Loss**: GRPO ì†ì‹¤

### Epoch ìš”ì•½

```
ğŸ“Š Epoch 1 Summary:
  Average Reward: 0.3456
ğŸ† Best model saved: checkpoints/grpo/checkpoint_best (reward: 0.3456)
```

## ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬

### ìë™ ì €ì¥

```
checkpoints/grpo/
â”œâ”€â”€ checkpoint_step_500/     # ì£¼ê¸°ì  ì €ì¥
â”œâ”€â”€ checkpoint_step_1000/
â”œâ”€â”€ checkpoint_best/         # ìµœê³  ì„±ëŠ¥ ëª¨ë¸
â””â”€â”€ checkpoint_final/        # ìµœì¢… ëª¨ë¸
```

### ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained("checkpoints/grpo/checkpoint_best")
tokenizer = AutoTokenizer.from_pretrained("checkpoints/grpo/checkpoint_best")
```

## ê³ ê¸‰ ê¸°ëŠ¥

### 1. Reference Model ì‚¬ìš©

KL Divergence penaltyë¥¼ ìœ„í•œ reference ëª¨ë¸:

```bash
python src/grpo_train.py \
    --use_ref_model \
    --policy_model "gpt2" \
    ...
```

Reference ëª¨ë¸ì€ ì´ˆê¸° ëª¨ë¸ì˜ ë³µì‚¬ë³¸ìœ¼ë¡œ, í•™ìŠµ ì¤‘ ê³ ì •ë©ë‹ˆë‹¤.

### 2. Mixed Precision Training

```bash
# BFloat16 (ê¶Œì¥, A100/H100)
python src/grpo_train.py --bf16 ...

# Float16 (V100/RTX)
python src/grpo_train.py --fp16 ...
```

### 3. W&B ë¡œê¹…

```bash
python src/grpo_train.py \
    --report_to "wandb" \
    --run_name "grpo_ndcg10_experiment" \
    ...
```

### 4. ì»¤ìŠ¤í…€ ë¦¬ì›Œë“œ í•¨ìˆ˜

`train_utils/reward_funtion.py`ë¥¼ ìˆ˜ì •í•˜ì—¬ ìƒˆë¡œìš´ ë¦¬ì›Œë“œ í•¨ìˆ˜ ì¶”ê°€:

```python
class CustomRewardFunction(RLRewardFunction):
    def __call__(self, generated_texts, target_items, history_items, **kwargs):
        # ê¸°ë³¸ NDCG
        ndcg = calculate_ndcg(...)
        
        # ì¶”ê°€ ë©”íŠ¸ë¦­ (ì˜ˆ: diversity)
        diversity = calculate_diversity(generated_texts)
        
        # ê²°í•©
        rewards = 0.8 * ndcg + 0.2 * diversity
        return rewards
```

## íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### RetrievalService ì—°ê²° ì‹¤íŒ¨

```
âŒ Error: RetrievalService not found!
```

**í•´ê²°:**
```bash
./runs/run_retrieval.sh
ray status  # í™•ì¸
```

### CUDA OOM ì—ëŸ¬

**í•´ê²° 1**: ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
```bash
--batch_size 2 \
--num_sample_generations 2 \
--gradient_accumulation_steps 8
```

**í•´ê²° 2**: Gradient checkpointing (TODO)

### ëŠë¦° ìƒì„± ì†ë„

**ì›ì¸**: `num_sample_generations`ê°€ ë„ˆë¬´ í¼

**í•´ê²°**: ìƒ˜í”Œ ìˆ˜ ì¤„ì´ê¸°
```bash
--num_sample_generations 2  # 4ì—ì„œ 2ë¡œ ê°ì†Œ
```

### ë¶ˆì•ˆì •í•œ í•™ìŠµ (Loss ë°œì‚°)

**í•´ê²° 1**: í•™ìŠµë¥  ê°ì†Œ
```bash
--learning_rate 5e-6  # 1e-5ì—ì„œ ê°ì†Œ
```

**í•´ê²° 2**: Gradient clipping ê°•í™”
```bash
--max_grad_norm 0.5  # 1.0ì—ì„œ ê°ì†Œ
```

**í•´ê²° 3**: ë¦¬ì›Œë“œ ì •ê·œí™”
```bash
--normalize_rewards
```

## ì„±ëŠ¥ ìµœì í™” íŒ

### 1. íš¨ìœ¨ì ì¸ ë°°ì¹˜ ì„¤ì •

```bash
# GPU ë©”ëª¨ë¦¬ë¥¼ ìµœëŒ€í•œ í™œìš©
batch_size Ã— num_sample_generations Ã— gradient_accumulation_steps = íš¨ê³¼ì  ë°°ì¹˜ í¬ê¸°

# ì˜ˆ: íš¨ê³¼ì  ë°°ì¹˜ í¬ê¸° 64
--batch_size 4 \
--num_sample_generations 4 \
--gradient_accumulation_steps 4
```

### 2. ë°ì´í„° ë¡œë”© ìµœì í™”

- ë°ì´í„°ì…‹ì„ ë©”ëª¨ë¦¬ì— ìºì‹±
- DataLoaderì˜ `num_workers` ì¡°ì • (TODO)

### 3. ìƒì„± ì†ë„ ìµœì í™”

- vLLM ì‚¬ìš© (rec_model.pyì— ì´ë¯¸ êµ¬í˜„ë¨)
- Batched generation

## ì‹¤í—˜ ì˜ˆì œ

### Baseline ì‹¤í—˜

```bash
python src/grpo_train.py \
    --policy_model "gpt2" \
    --reward_type "ndcg" \
    --k 10 \
    --batch_size 4 \
    --num_sample_generations 4 \
    --learning_rate 1e-5 \
    --run_name "baseline_ndcg10"
```

### Hit@K ë¦¬ì›Œë“œ ì‹¤í—˜

```bash
python src/grpo_train.py \
    --reward_type "hit" \
    --k 10 \
    --run_name "hit10_experiment"
```

### Mixed ë¦¬ì›Œë“œ ì‹¤í—˜

```bash
python src/grpo_train.py \
    --reward_type "mixed" \
    --k 10 \
    --run_name "mixed_ndcg_hit"
```

## FAQ

**Q: GRPOì™€ PPOì˜ ì°¨ì´ëŠ”?**
A: GRPOëŠ” ê° í”„ë¡¬í”„íŠ¸ë‹¹ ì—¬ëŸ¬ ìƒ˜í”Œì„ ìƒì„±í•˜ì—¬ ê·¸ë£¹ ë‚´ì—ì„œ ìƒëŒ€ì  í’ˆì§ˆì„ ë¹„êµí•©ë‹ˆë‹¤. ì´ëŠ” í•™ìŠµì„ ë” ì•ˆì •ì ì´ê³  íš¨ìœ¨ì ìœ¼ë¡œ ë§Œë“­ë‹ˆë‹¤.

**Q: num_sample_generationsë¥¼ ëª‡ìœ¼ë¡œ ì„¤ì •í•´ì•¼ í•˜ë‚˜ìš”?**
A: 2~8 ì‚¬ì´ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤. ë†’ì„ìˆ˜ë¡ ì•ˆì •ì ì´ì§€ë§Œ ê³„ì‚° ë¹„ìš©ì´ ì¦ê°€í•©ë‹ˆë‹¤.

**Q: Dummy ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸í•˜ëŠ” ì´ìœ ëŠ”?**
A: í•™ìŠµ íŒŒì´í”„ë¼ì¸ì„ ë¹ ë¥´ê²Œ ê²€ì¦í•˜ê³  ë²„ê·¸ë¥¼ ì°¾ê¸° ìœ„í•¨ì…ë‹ˆë‹¤.

**Q: Reference modelì´ í•„ìš”í•œê°€ìš”?**
A: ì„ íƒì‚¬í•­ì…ë‹ˆë‹¤. KL penaltyë¥¼ ì›í•˜ë©´ ì‚¬ìš©í•˜ì„¸ìš”.

## ì°¸ê³  ìë£Œ

- [TRL ê³µì‹ ë¬¸ì„œ](https://huggingface.co/docs/trl)
- [GRPO ë…¼ë¬¸](https://arxiv.org/abs/...)
- [vLLM ë¬¸ì„œ](https://docs.vllm.ai/)
- [Ray ë¬¸ì„œ](https://docs.ray.io/)



