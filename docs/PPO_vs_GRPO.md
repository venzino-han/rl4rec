# PPO vs GRPO ë¹„êµ

## ê°œìš”

RL4Rec í”„ë¡œì íŠ¸ì—ì„œ ì œê³µí•˜ëŠ” ë‘ ê°€ì§€ ê°•í™”í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì˜ ì°¨ì´ì ê³¼ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ë¹„êµí•©ë‹ˆë‹¤.

## ì•Œê³ ë¦¬ì¦˜ ë¹„êµ

| íŠ¹ì§• | PPO | GRPO |
|------|-----|------|
| **ìƒ˜í”Œë§** | í”„ë¡¬í”„íŠ¸ë‹¹ 1ê°œ ìƒì„± | í”„ë¡¬í”„íŠ¸ë‹¹ Nê°œ ìƒì„± |
| **ë¦¬ì›Œë“œ ì •ê·œí™”** | ì „ì²´ ë°°ì¹˜ ê¸°ì¤€ | ê·¸ë£¹(ê°™ì€ í”„ë¡¬í”„íŠ¸) ë‚´ |
| **í•™ìŠµ ì•ˆì •ì„±** | ë³´í†µ | ë†’ìŒ |
| **ìƒ˜í”Œ íš¨ìœ¨ì„±** | ë‚®ìŒ | ë†’ìŒ |
| **ê³„ì‚° ë¹„ìš©** | ë‚®ìŒ | ë†’ìŒ (Në°°) |
| **êµ¬í˜„ ë³µì¡ë„** | ë³´í†µ | ë‚®ìŒ (TRL ì œê³µ) |
| **ì¶”ì²œ ì‚¬ìš© ì¼€ì´ìŠ¤** | íƒìƒ‰ì´ ì¤‘ìš”í•œ ê²½ìš° | ì•ˆì •ì  í•™ìŠµì´ ì¤‘ìš”í•œ ê²½ìš° |

## íŒŒì¼ êµ¬ì¡°

```
src/
â”œâ”€â”€ train_with_trl.py     # PPO Trainer
â””â”€â”€ grpo_train.py          # GRPO Trainer (ê¶Œì¥)

runs/
â”œâ”€â”€ run_trl_train.sh       # PPO ì‹¤í–‰
â”œâ”€â”€ run_grpo_train.sh      # GRPO ì‹¤í–‰
â””â”€â”€ run_grpo_test.sh       # GRPO ë¹ ë¥¸ í…ŒìŠ¤íŠ¸

docs/
â”œâ”€â”€ TRL_TRAINING_GUIDE.md  # PPO ê°€ì´ë“œ
â””â”€â”€ GRPO_TRAINING_GUIDE.md # GRPO ê°€ì´ë“œ (ì´ ë¬¸ì„œ)
```

## ìƒì„¸ ë¹„êµ

### 1. ìƒ˜í”Œë§ ì „ëµ

#### PPO
```python
# ê° í”„ë¡¬í”„íŠ¸ì—ì„œ 1ê°œë§Œ ìƒì„±
for prompt in prompts:
    generation = model.generate(prompt)  # 1ê°œ
    reward = calculate_reward(generation)
    update(reward)
```

**ì¥ì :**
- ë¹ ë¥¸ ìƒì„± (í”„ë¡¬í”„íŠ¸ë‹¹ 1ê°œ)
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì 

**ë‹¨ì :**
- ë†’ì€ ë¶„ì‚° (variance)
- ë¶ˆì•ˆì •í•œ í•™ìŠµ

#### GRPO
```python
# ê° í”„ë¡¬í”„íŠ¸ì—ì„œ Nê°œ ìƒì„±
for prompt in prompts:
    generations = model.generate(prompt, num_samples=N)  # Nê°œ
    rewards = [calculate_reward(g) for g in generations]
    # ê·¸ë£¹ ë‚´ ë¹„êµë¥¼ í†µí•œ ì—…ë°ì´íŠ¸
    update_with_group_comparison(rewards)
```

**ì¥ì :**
- ë‚®ì€ ë¶„ì‚° (ê°™ì€ í”„ë¡¬í”„íŠ¸ì˜ ì—¬ëŸ¬ ìƒ˜í”Œ ë¹„êµ)
- ì•ˆì •ì ì¸ í•™ìŠµ
- ë†’ì€ ìƒ˜í”Œ íš¨ìœ¨ì„±

**ë‹¨ì :**
- ëŠë¦° ìƒì„± (Në°°)
- ë” ë§ì€ ë©”ëª¨ë¦¬ í•„ìš”

### 2. ë¦¬ì›Œë“œ ì •ê·œí™”

#### PPO: ì „ì²´ ë°°ì¹˜ ì •ê·œí™”
```python
# ë°°ì¹˜ ì „ì²´ì˜ í‰ê· /í‘œì¤€í¸ì°¨ë¡œ ì •ê·œí™”
rewards = (rewards - rewards.mean()) / rewards.std()
```

**ë¬¸ì œì :**
- ë°°ì¹˜ êµ¬ì„±ì— ë”°ë¼ í•™ìŠµì´ ì˜í–¥ì„ ë°›ìŒ
- ë‹¤ë¥¸ í”„ë¡¬í”„íŠ¸ë“¤ì˜ ë¦¬ì›Œë“œê°€ ì„œë¡œ ì˜í–¥

#### GRPO: ê·¸ë£¹ ë‚´ ì •ê·œí™”
```python
# ê°™ì€ í”„ë¡¬í”„íŠ¸ì—ì„œ ìƒì„±ëœ ìƒ˜í”Œë“¤ë¼ë¦¬ë§Œ ë¹„êµ
for group in groups:  # ê°™ì€ í”„ë¡¬í”„íŠ¸ì˜ Nê°œ ìƒ˜í”Œ
    group_rewards = rewards[group]
    normalized = (group_rewards - group_rewards.mean()) / group_rewards.std()
```

**ì¥ì :**
- í”„ë¡¬í”„íŠ¸ë³„ ë…ë¦½ì  ë¹„êµ
- ë” ì•ˆì •ì ì¸ í•™ìŠµ ì‹ í˜¸

### 3. í•™ìŠµ íš¨ìœ¨ì„±

#### íš¨ê³¼ì  ë°°ì¹˜ í¬ê¸°

**PPO:**
```
íš¨ê³¼ì  ë°°ì¹˜ = batch_size Ã— gradient_accumulation_steps
ì˜ˆ: 4 Ã— 4 = 16
```

**GRPO:**
```
íš¨ê³¼ì  ë°°ì¹˜ = batch_size Ã— num_sample_generations Ã— gradient_accumulation_steps
ì˜ˆ: 4 Ã— 4 Ã— 4 = 64
```

GRPOëŠ” ê°™ì€ ì„¤ì •ì—ì„œ `num_sample_generations`ë°° ë” ë§ì€ ìƒ˜í”Œì„ í™œìš©í•©ë‹ˆë‹¤.

### 4. ê³„ì‚° ë¹„ìš©

#### GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰

**PPO:**
```
Memory âˆ batch_size Ã— max_length
```

**GRPO:**
```
Memory âˆ batch_size Ã— num_sample_generations Ã— max_length
```

GRPOëŠ” ì•½ `num_sample_generations`ë°° ë” ë§ì€ ë©”ëª¨ë¦¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

#### í•™ìŠµ ì‹œê°„

| ì„¤ì • | PPO | GRPO |
|------|-----|------|
| Batch size | 16 | 4 |
| Num samples | 1 | 4 |
| ìŠ¤í…ë‹¹ ì‹œê°„ | 1x | ~2-3x |
| ìˆ˜ë ´ê¹Œì§€ ìŠ¤í… | 10,000 | 3,000-5,000 |
| **ì´ í•™ìŠµ ì‹œê°„** | **1x** | **0.6-1.5x** |

GRPOëŠ” ìŠ¤í…ë‹¹ ëŠë¦¬ì§€ë§Œ ë” ë¹¨ë¦¬ ìˆ˜ë ´í•˜ì—¬ ì´ í•™ìŠµ ì‹œê°„ì´ ë¹„ìŠ·í•˜ê±°ë‚˜ ë” ì§§ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤

### PPOë¥¼ ì„ íƒí•´ì•¼ í•˜ëŠ” ê²½ìš°

âœ… GPU ë©”ëª¨ë¦¬ê°€ ì œí•œì ì¸ ê²½ìš°
âœ… ë¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘ì´ í•„ìš”í•œ ê²½ìš°
âœ… íƒìƒ‰(exploration)ì´ ì¤‘ìš”í•œ ê²½ìš°
âœ… ì˜¨ë¼ì¸ í•™ìŠµ(online learning)

**ì˜ˆì œ:**
```bash
./runs/run_trl_train.sh
```

### GRPOë¥¼ ì„ íƒí•´ì•¼ í•˜ëŠ” ê²½ìš° (ê¶Œì¥)

âœ… ì•ˆì •ì ì¸ í•™ìŠµì´ ì¤‘ìš”í•œ ê²½ìš°
âœ… ìƒ˜í”Œ íš¨ìœ¨ì„±ì´ ì¤‘ìš”í•œ ê²½ìš°
âœ… ì¶©ë¶„í•œ GPU ë©”ëª¨ë¦¬ê°€ ìˆëŠ” ê²½ìš°
âœ… í”„ë¡œë•ì…˜ í™˜ê²½

**ì˜ˆì œ:**
```bash
./runs/run_grpo_train.sh
```

## ì‹¤í—˜ ê²°ê³¼ ë¹„êµ

### í…ŒìŠ¤íŠ¸ ì¡°ê±´

```
ëª¨ë¸: GPT-2
ë°ì´í„°ì…‹: Beauty
ë¦¬ì›Œë“œ: NDCG@10
ì´ ìƒ˜í”Œ ìˆ˜: ë™ì¼ (~10,000)
```

### ì˜ˆìƒ ê²°ê³¼

| ë©”íŠ¸ë¦­ | PPO | GRPO |
|--------|-----|------|
| NDCG@10 | 0.32 | 0.35 |
| í•™ìŠµ ì•ˆì •ì„± | â­â­â­ | â­â­â­â­â­ |
| ìˆ˜ë ´ ì†ë„ | ëŠë¦¼ | ë¹ ë¦„ |
| ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ | ë‚®ìŒ | ë†’ìŒ |
| ì´ í•™ìŠµ ì‹œê°„ | ê¸°ì¤€ | 0.6-1.5x |

## ì‹¤í–‰ ì˜ˆì œ

### PPO í•™ìŠµ

```bash
# ê¸°ë³¸ PPO í•™ìŠµ
python src/train_with_trl.py \
    --model_name "gpt2" \
    --reward_type "ndcg" \
    --k 10 \
    --batch_size 16 \
    --mini_batch_size 4 \
    --learning_rate 1e-5 \
    --num_epochs 3

# ê²°ê³¼: checkpoints/trl_ppo/
```

### GRPO í•™ìŠµ

```bash
# ê¸°ë³¸ GRPO í•™ìŠµ
python src/grpo_train.py \
    --policy_model "gpt2" \
    --reward_type "ndcg" \
    --k 10 \
    --batch_size 4 \
    --num_sample_generations 4 \
    --learning_rate 1e-5 \
    --num_epochs 3

# ê²°ê³¼: checkpoints/grpo/
```

### ë™ì¼í•œ íš¨ê³¼ì  ë°°ì¹˜ í¬ê¸°ë¡œ ë¹„êµ

```bash
# PPO: íš¨ê³¼ì  ë°°ì¹˜ = 16
python src/train_with_trl.py \
    --batch_size 16 \
    --mini_batch_size 4

# GRPO: íš¨ê³¼ì  ë°°ì¹˜ = 4 Ã— 4 = 16
python src/grpo_train.py \
    --batch_size 4 \
    --num_sample_generations 4
```

## í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ê°€ì´ë“œ

### PPO íŠœë‹

1. **Learning rate**: 1e-6 ~ 1e-4
2. **Batch size**: 8 ~ 32
3. **Mini batch size**: 2 ~ 8
4. **PPO epochs**: 2 ~ 6

```bash
python src/train_with_trl.py \
    --learning_rate 5e-5 \
    --batch_size 16 \
    --mini_batch_size 4 \
    --ppo_epochs 4
```

### GRPO íŠœë‹

1. **Learning rate**: 5e-6 ~ 5e-5
2. **Batch size**: 2 ~ 8
3. **Num sample generations**: 2 ~ 8
4. **Gradient accumulation**: 2 ~ 8

```bash
python src/grpo_train.py \
    --learning_rate 1e-5 \
    --batch_size 4 \
    --num_sample_generations 4 \
    --gradient_accumulation_steps 4
```

## ë””ë²„ê¹… íŒ

### PPO ë¬¸ì œ

**ì¦ìƒ**: Loss ë°œì‚°
```bash
# í•´ê²°: Learning rate ê°ì†Œ, KL penalty ì¦ê°€
--learning_rate 5e-6
```

**ì¦ìƒ**: ë¦¬ì›Œë“œê°€ ì¦ê°€í•˜ì§€ ì•ŠìŒ
```bash
# í•´ê²°: Batch size ì¦ê°€
--batch_size 32
```

### GRPO ë¬¸ì œ

**ì¦ìƒ**: CUDA OOM
```bash
# í•´ê²°: num_sample_generations ê°ì†Œ
--num_sample_generations 2
```

**ì¦ìƒ**: í•™ìŠµì´ ë„ˆë¬´ ëŠë¦¼
```bash
# í•´ê²°: Batch size ì¦ê°€, num_sample_generations ê°ì†Œ
--batch_size 8 --num_sample_generations 2
```

## ê¶Œì¥ ì‚¬í•­

### ğŸ† ì¼ë°˜ì ì¸ ê²½ìš°: GRPO ì‚¬ìš©

```bash
./runs/run_grpo_train.sh
```

**ì´ìœ :**
- ë” ì•ˆì •ì ì¸ í•™ìŠµ
- ë” ë†’ì€ ìµœì¢… ì„±ëŠ¥
- TRLì—ì„œ ì˜ ì§€ì›ë¨

### ğŸš€ ë¹ ë¥¸ ì‹¤í—˜: PPO ì‚¬ìš©

```bash
./runs/run_trl_train.sh --batch_size 8
```

**ì´ìœ :**
- ë¹ ë¥¸ ë°˜ë³µ ì‹¤í—˜
- ë‚®ì€ ë©”ëª¨ë¦¬ ìš”êµ¬ì‚¬í•­

### ğŸ§ª í”„ë¡œí† íƒ€ì´í•‘: Dummy ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸

```bash
./runs/run_grpo_test.sh
```

**ì´ìœ :**
- ë¹ ë¥¸ íŒŒì´í”„ë¼ì¸ ê²€ì¦
- ë²„ê·¸ ì°¾ê¸°

## ì¶”ê°€ ìë£Œ

- [PPO ë…¼ë¬¸](https://arxiv.org/abs/1707.06347)
- [GRPO ìƒì„¸ ê°€ì´ë“œ](./GRPO_TRAINING_GUIDE.md)
- [PPO ìƒì„¸ ê°€ì´ë“œ](./TRL_TRAINING_GUIDE.md)
- [TRL ê³µì‹ ë¬¸ì„œ](https://huggingface.co/docs/trl)

## ê²°ë¡ 

**ëŒ€ë¶€ë¶„ì˜ ê²½ìš° GRPOë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.** ë” ì•ˆì •ì ì´ê³  íš¨ìœ¨ì ì¸ í•™ìŠµì´ ê°€ëŠ¥í•˜ë©°, TRLì—ì„œ ì˜ ì§€ì›ë©ë‹ˆë‹¤.

GPU ë©”ëª¨ë¦¬ê°€ ì œí•œì ì´ê±°ë‚˜ ë¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘ì´ í•„ìš”í•œ ê²½ìš°ì—ë§Œ PPOë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.



